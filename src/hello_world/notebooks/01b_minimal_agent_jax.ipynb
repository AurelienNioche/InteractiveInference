{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdd1b7-3adc-44a0-8a12-a630fd7e4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pip\n",
    "!pip install --user --upgrade tqdm\n",
    "!pip install --user --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "#!pip install \"jax[cpu]===0.3.14\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697909d-50c3-4969-8607-36b4d8ad0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1ee8d-b13f-4bbe-b668-5abb497d22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cc8d3-2bea-4aef-99e1-ec17c5b9e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax_key = jax.random.PRNGKey(42)\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX device type: {jax.devices()[0].device_kind}\")\n",
    "#jax.default_device(jax.devices(\"gpu\")[1])\n",
    "\n",
    "import minimal_agent as ma_np\n",
    "import minimal_agent_jax as ma_jax\n",
    "import minimal_environment as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56543ea0-6698-48e4-994f-3d51550c0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_safelog(x):\n",
    "  return jnp.log( jnp.maximum(x, 1e-16 ))\n",
    "\n",
    "def jax_step_naive_fun(num_actions, o, q, πs, p_o, p_t, log_p_c):\n",
    "    \"\"\" Deterministic policy selection with pragmatic value and state information gain.\"\"\"\n",
    "    # update belief from new observation\n",
    "    joint = q * p_o[:,o]\n",
    "    q = joint / joint.sum()\n",
    "    # policy rollout: \n",
    "    step = lambda q, a: (q @ p_t[:,a,:], q @ p_t[:,a,:])\n",
    "    qs_π = lambda q, π: jax.lax.scan(step, init=q, xs=π)[1]\n",
    "    qs_πs = jax.vmap(qs_π, in_axes=(None, 0), out_axes=(0))\n",
    "    q_ss = qs_πs(q, πs)\n",
    "    # pragmatic value\n",
    "    pragmatic = (q_ss @ log_p_c).sum(axis=1)\n",
    "    # state info gain\n",
    "    p_oo = q_ss @ p_o # prior\n",
    "    joint = q_ss[...,None] * p_o\n",
    "    q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "    d_oo = (q_oo * (jax_safelog( q_oo ) - jax_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "    info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "    # action selection\n",
    "    π = πs[jnp.argmax(pragmatic + info_gain)]\n",
    "    # propagate belief through time\n",
    "    q = q @ p_t[:,π[0],:]\n",
    "    return q, π\n",
    "\n",
    "jit_step_naive_fun = jax.jit(jax_step_naive_fun, static_argnums=(0,))\n",
    "\n",
    "def jax_step_batched_fun(o, q, πs, p_o, p_t, log_p_c):\n",
    "    \"\"\" Deterministic policy selection with pragmatic value and state information gain.\"\"\"\n",
    "    # update belief from new observation\n",
    "    joint = q * p_o[:,o]\n",
    "    q = joint / joint.sum()\n",
    "    \n",
    "    # policy rollout: \n",
    "    # coalesced access to transition dynamics speeds up vectorised rollouts across policies\n",
    "    def step(q, a):\n",
    "        q = a @ (q @ p_t)\n",
    "        return q, q\n",
    "    \n",
    "    qs_π = lambda q, π: jax.lax.scan(step, init=q, xs=π)[1]\n",
    "    qs_πs = jax.vmap(qs_π, in_axes=(None, 0), out_axes=(0)) # carry, output\n",
    "    \n",
    "    # batching policies helps avoid GPU out of memory errors\n",
    "    def batch_nefe(q, πs):\n",
    "        q_ss = qs_πs(q, πs)\n",
    "        pragmatic = (q_ss @ log_p_c).sum(axis=1)\n",
    "        # state info gain\n",
    "        p_oo = q_ss @ p_o # prior\n",
    "        joint = q_ss[...,None] * p_o\n",
    "        q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "        d_oo = (q_oo * (jax_safelog( q_oo ) - jax_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "        info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "        nefe = pragmatic + info_gain\n",
    "        return q, nefe\n",
    "    \n",
    "    nefe = jax.lax.scan(batch_nefe, init=q, xs=πs)[1] # scan over batches\n",
    "    # action selection\n",
    "    π = πs.reshape( (-1,) + πs.shape[2:] )[jnp.argmax(nefe)] # perform policy selection by squeezing first two dimensions\n",
    "    π = jnp.argmax(π, axis=-1)\n",
    "    # propagate belief through time\n",
    "    q = q @ p_t[π[0],:,:]\n",
    "    return q, π\n",
    "\n",
    "jit_step_batched_fun = jax.jit(jax_step_batched_fun)\n",
    "\n",
    "class MinimalAgentJax:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, \n",
    "                 num_batches=1):\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.num_batches = num_batches\n",
    "        \n",
    "        self.p_t = jnp.asarray(env.p_s1_given_s_a.swapaxes(0,1))\n",
    "        self.p_o = jnp.asarray(env.p_o_given_s)\n",
    "        print(f'Enumerating {self.env.a_N**k:,} candidate policies of length {k}')\n",
    "        #batched policies with one-hot actions\n",
    "        πs = np.stack(np.meshgrid(*[np.arange(self.env.a_N) for _ in range(k)])).T.reshape(self.num_batches, -1, k)\n",
    "        self.πs = jax.nn.one_hot(jax.device_put(πs), self.env.a_N, dtype=int)\n",
    "        \n",
    "        # slow naive implementation\n",
    "        #self.πs = jnp.asarray([x for x in itertools.product( range(self.env.a_N), repeat=self.k )])\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] \\\n",
    "                 + 1/(self.env.s_N*5) * np.ones(self.env.s_N)\n",
    "        self.q_star = q_star / q_star.sum()\n",
    "        self.log_p_c = jnp.log( self.q_star )\n",
    "        # initialize state prior as uniform\n",
    "        self.q = jnp.asarray(np.ones(self.env.s_N) / self.env.s_N )\n",
    "    \n",
    "    def step(self, o, use_jit=True):\n",
    "        params = {\n",
    "            'o': o,\n",
    "            'q': self.q,\n",
    "            'p_o': self.p_o,\n",
    "            'p_t': self.p_t,\n",
    "            'log_p_c': self.log_p_c,\n",
    "            'πs': self.πs,\n",
    "        }\n",
    "        self.q, π = jit_step_batched_fun(**params) if use_jit else jax_step_batched_fun(**params)\n",
    "        return π[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efef31-b54f-4fab-bdf8-06d7dcbfef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(me)\n",
    "importlib.reload(ma_np)\n",
    "importlib.reload(ma_jax)\n",
    "\n",
    "# numpy: 5 seconds per iteration with 32 states and 16 step time horizon\n",
    "# jax: 3 iterations per second -> 15x faster\n",
    "s_food = 0\n",
    "s_0 = 5\n",
    "target_state = 28\n",
    "N = 32 # number states\n",
    "k = 10 # planning horizon; run time increases exponentially with planning horizon\n",
    "T = 128 # number of simulation steps\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=s_food, # location of the food source\n",
    "                    s_0=s_0, \n",
    "                    o_decay=0.15) # starting location \n",
    "\n",
    "agent_np = ma_np.MinimalAgent(env=env, target_state=target_state, k=k, use_info_gain=True, use_pragmatic_value=True, select_max_π=True)\n",
    "\n",
    "agent_jax = ma_jax.MinimalAgentJax(env=env, \n",
    "                            target_state=target_state, \n",
    "                            k=k)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent_np.reset() # initialize belief state and target state distribution\n",
    "agent_jax.reset() # initialize belief state and target state distribution\n",
    "\n",
    "o = int(0)\n",
    "agent_jax.step(o=o)\n",
    "\n",
    "%timeit agent_np.step(int(o))\n",
    "%timeit agent_jax.step(o=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a779d-667b-4b9c-a76e-f2e548e9c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ma_jax)\n",
    "s_food = 0\n",
    "s_0 = 6\n",
    "target_state = 26\n",
    "N = 32 # number states\n",
    "k = 14 # planning horizon; run time increases exponentially with planning horizon\n",
    "T = 128 # number of simulation steps\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=s_food, # location of the food source\n",
    "                    s_0=s_0, \n",
    "                    o_decay=0.1) # starting location \n",
    "\n",
    "agent_jax = ma_jax.MinimalAgentJax(env=env, target_state=target_state, k=k, num_batches=2**2)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "#agent_np.reset() # initialize belief state and target state distribution\n",
    "agent_jax.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "qq = [agent_jax.q]\n",
    "aa = []\n",
    "    \n",
    "for i in tqdm( range(T) ):\n",
    "    π = agent_jax.step(int(o))\n",
    "    a =  np.copy(π)\n",
    "    o = env.step(a)\n",
    "    ss.append(env.s_t)\n",
    "    qq.append(jnp.copy(agent_jax.q))\n",
    "    aa.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da915e-349d-49ff-a801-db4a944d763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "plot_steps = 128\n",
    "aa = np.array(aa)[:plot_steps-1]\n",
    "ss = np.array(ss)[:plot_steps]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "plt.imshow(jnp.array(qq)[:plot_steps].T, label='belief', aspect='auto')\n",
    "t = np.arange(len(aa))\n",
    "i_left = t[aa==0]\n",
    "i_right = t[aa==1]\n",
    "plt.scatter(i_left, ss[:-1][i_left], s=8**2, c='white', marker=CARETUP)\n",
    "plt.scatter(i_right, ss[:-1][i_right], s=8**2, c='white', marker=CARETDOWN)\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352502dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.p_o_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe8af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
