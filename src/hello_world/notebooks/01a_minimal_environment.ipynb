{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad67ad10-5d62-4df7-a6fd-7a2a62e7ea5a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206e212-0334-4e61-8ed6-80c9b23c9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b53e87-9197-456a-9716-5343a20bf23d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b69ee9-591a-4a3d-b883-a971b59a71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import minimal_environment as me\n",
    "importlib.reload(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd34b0-801b-454d-88d0-927f4b91817d",
   "metadata": {},
   "source": [
    "# Environments\n",
    "\n",
    "Environments are defined as discrete time Partially Observable Markov Decision Processes ([POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)s) witout reward function:\n",
    "\n",
    "- a set of states $s \\in \\mathcal{S}$\n",
    "- a set of observations $o \\in \\mathcal{\\Omega}$\n",
    "- a set of actions $a \\in \\mathcal{A}$\n",
    "- a set of conditional transition probabilities $\\mathcal{\\tau}: p(s'|s, a)$\n",
    "- a set of emission/ observation probabilities $\\mathcal{O}: p(o|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0e300-3d76-4597-808e-511f02fcfca5",
   "metadata": {},
   "source": [
    "## MinimalEnv environment\n",
    "\n",
    "The `MinimalEnv` environment has a discrete set of states and generates discrete (binary) outputs: food `True` or no food `False`.\n",
    "\n",
    "### Emission probability \n",
    "The emission probability distributions $p(o|s)$ are defined as a conditional probability table `p[s,o]`, where each row $i$ defines the probability of observing `False` (column 0) and `True` (column 1) in state $s_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7604a-6361-4798-b258-30d017598236",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_decay=0.4) # decay of observing food away from source \n",
    "\n",
    "p_o_given_s = env.p_o_given_s #precomputed result of env.emission_probability()\n",
    "print(p_o_given_s)\n",
    "print('shape', p_o_given_s.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "o_food = 1\n",
    "ax.bar(range(env.p_o_given_s[:,1].shape[0]), env.p_o_given_s[:,o_food])\n",
    "ax.bar([env.s_food], env.p_o_given_s[env.s_food,o_food], color='red', label='food source')\n",
    "ax.set_xlabel('state')\n",
    "ax.set_ylabel('$p(o=True|s)$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91af27-3aff-4385-b3ae-b7b173819d1d",
   "metadata": {},
   "source": [
    "### Transition dynamics\n",
    "The transition dynamics $p(s'|s, a)$ are defined as a conditional probability table `p[s,a,s']`. The subarray `p[0,0,:]`, for example, defines the probability of transitioning from state $0$ to any successor state, given that action `0` (move left) was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a451a-af6a-44c7-9b9c-5ea24336de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s1_given_s_a = env.p_s1_given_s_a # precomputed result of env.transition_dynamics()\n",
    "print('shape', p_s1_given_s_a.shape)\n",
    "p_s1_given_s_a[0,0,:] # left: 0, right: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b2ed0-4a2a-46ac-a88d-1f52021bcf4a",
   "metadata": {},
   "source": [
    "### Random Agent Behavior\n",
    "\n",
    "Unlike a POMDP, the environment itself does not define a goal, motivation, or purpose for an agent that interacts with it. The envirment is indifferent about how agents interact with it. There is therefore no value (good, bad, high or low performance) associated with any individual sequence of behavior. \n",
    "\n",
    "The code below simulates the interaction between the minimal environment and an agent that behaves randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c348-28e7-4724-b779-0d647328e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "ss, os = [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "os.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  os.append(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c570a-728b-4de6-8d91-d23da96958a5",
   "metadata": {},
   "source": [
    "We inspect the sequence of states and emissions during this interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76f6f-bf34-4a41-95fd-b3b8d3627a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('$s$')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(os))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation (1=Food)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d988dfb-1e05-4e95-b459-06a8433d81c2",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "\n",
    "## Belief Update\n",
    "\n",
    "An active inference agent holds a belief about the state of the environment at time $t$, modelled as a probability distribution $Q(s; \\theta_t)$ over states $s \\in \\mathcal{S}$. There is uncertainty associated with this belief as the state cannot be observed directly and must instead be inferred from stochastic observations.\n",
    "\n",
    "### Update through Time\n",
    "\n",
    "At any time $t$ an agent holds a prior belief about $s_t$ before taking in an observation from its environment. This belief could be uniform, for example at the start of an interaction. It could also be informed by propagating the belief $Q(s; \\theta_{t-1})$ about state $s_{t-1}$ through its model of the environment transition dynamics, taking into account the action $a_{t-1}$ taken at the previous time step.\n",
    "\n",
    "$$Q(s; \\theta_{t}) = \\mathbb{E}_{s\\sim Q_{t-1}}[p(s_t|s, a_{t-1})]$$\n",
    "\n",
    "Note that updating the belief in light of the chosen action involves fitting the parameters $\\theta_t$. For our minimal agent, we represent $Q(s;\\theta)$ as the categorical distribution with $\\theta_t = {p_i}$ representing probability of each discrete state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1760fa-19a1-4d46-9025-23f8a9546a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief_a(env, q_last, a):\n",
    "  return q_last @ env.p_s1_given_s_a[:,a,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a91fb-8cad-4923-bf96-5b95097bab6f",
   "metadata": {},
   "source": [
    "Let's see the effect of this belief update in action. Say we believed strongly that the environment was in states 1 or 4 with equal probability at time $t$ and we took action 1 (move right). Recall that according to the environment dynamics there is some probability of the state remaining unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb0157-065c-4429-804c-1af01a9ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "\n",
    "# prior belief\n",
    "q0 = np.ones(env.s_N) * 0.1 + np.eye(env.s_N)[1] + np.eye(env.s_N)[4]\n",
    "q0 = q0 / q0.sum()\n",
    "\n",
    "# propagating belief through dynamics, conditioned on action a\n",
    "q1 = update_belief_a(env, q0, a=1)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=q0, alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=q1, alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Propagating prior beliefs through environment dynamics.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa51f1-a4af-4718-b3ff-2ae364bcd651",
   "metadata": {},
   "source": [
    "### Update based on new observation\n",
    "\n",
    "Now that we have updated our prior taking into account the action taken in the previous time step and the (agent's model of the) environment dynamics, we turn our attention to updating beliefs in light of a new observation.\n",
    "\n",
    "In active inference, this belief update is cast as minimizing the variational free energy, i.e. minimizing the KL-divergence between $Q(s;\\theta')$ and $p(o, s) = Q(s; \\theta) p(o|s)$ with respect to $\\theta'$.\n",
    "\n",
    "$$D_{KL}(\\quad Q_{\\theta'}(s), p(o|s)Q_{\\theta}(s) \\quad) \\quad = \\quad \\mathbb{E}_{s \\sim Q_{\\theta'}}[\\quad \\log Q_{\\theta'}(s) - \\log p(o|s)Q_{\\theta}(s) \\quad]$$\n",
    "\n",
    "Again, for diagnostics and debugging, we optionally log the optimization loss (`debug=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfab13-ef79-42bc-aaca-ef00c90b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, q_last, o):\n",
    "  joint = env.p_o_given_s[:,o] * q_last\n",
    "  return joint / joint.sum()\n",
    "\n",
    "def update_belief_all(env, q_last):\n",
    "  joint = env.p_o_given_s.T * q_last\n",
    "  return joint / joint.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9788a-ec59-4f1f-a93a-1adb6bfc6e78",
   "metadata": {},
   "source": [
    "Let's see the effect of this in action. We start with a uniform prior belief. Recall that the food source is in state 0 and that the probability of observing food decreases exponentially with the distance of a state from the food source, with the state space wrapping around.\n",
    "\n",
    "If we observed no food ($o=0$), then it is most likely that we are in the state furthest away from the food source. If we observed food ($o=1$), then it is most likely that we are at the food source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3ff0d-e87c-46c9-8eda-b1ea2be43a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1\n",
    "\n",
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "q0 = np.ones(env.s_N)/env.s_N\n",
    "q1 = update_belief(env, q0, o=o)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=q0, alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=q1, alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Updating beliefs in light of a new observation.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656b29-cd84-4a3a-b16c-3867020ed1c9",
   "metadata": {},
   "source": [
    "### Accumulating observations over time\n",
    "\n",
    "Note that, based on a single observation, we cannot refine our belief to become most confident in a state that is not the food source itself or furthest away from the food source. This can be improved if we accumulate observations from a single state over time. Because the probability of observing food decreases symmetrically to the left and right of the food source, the agent is likely to hold strong beliefs about all states that share the same rate of observing food, but cannot disambiguate these states without action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494d505-8062-4d25-98be-7dadcf49995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3\n",
    "n_timesteps = 50\n",
    "\n",
    "env = me.MinimalEnv(N=12, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=s) # starting location\n",
    "\n",
    "q = np.ones(env.s_N)/env.s_N # initialize state prior as uniform\n",
    "o = env.reset() # set state to starting state\n",
    "\n",
    "# refine belief by sampling N observations but without taking any action\n",
    "qq = []\n",
    "for i in range(n_timesteps):\n",
    "    q = update_belief(env, q_last=q, o=int(o))\n",
    "    qq.append(q)\n",
    "    o = env.sample_o()\n",
    "\n",
    "plt.bar(range(env.s_N), qq[int(len(qq)*0.25)], alpha=0.3, label='25% observations');\n",
    "plt.bar(range(env.s_N), qq[-1], alpha=0.5, label='all observations');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739bb20-6a71-45b0-85ed-78d674953104",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "In active inference, sequences of actions (plans, policies) are scored by the negative expected free energy, and selected by exponentiating and normalizing, i.e. sampling from the softmax over plans. Plans $\\pi: a_0, a_1, ..., a_{K-1}$ define sequences of actions up to a finite horizon of $K$ timesteps into the future.\n",
    "\n",
    "The expected free energy can be decomposed in various ways and here we chose one that we find most intuitive, involving a _pragmatic_ term and an _information gain_ term (See Eq 4.9, page 73 in the book). \n",
    "\n",
    "The _pragmatic_ term assesses the probability of arriving in states following $\\pi$ that the agent desires, or of encountering observations that the agent desires. In this context, $Q_\\theta$ is estimated by propagating beliefs through the environment transition dynamics following the sequence of actions defined by $\\pi$, and observations are halucinated by sampling from the emission probability distributions.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}}\\left[\\log p_c(s)\\right] \\quad \\text{or} \\quad \\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[\\log p_c(o)\\right]$\n",
    "\n",
    "The _information gain_ term quantifies the belief update due to making observations in future states.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[ D_{KL}(\\, Q_{\\theta'}(s|o),  Q_{\\theta}(s) \\,) \\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb5bf7-7c3a-465a-a675-f6ddd9569512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - future state distributions can be computed in one sweep\n",
    "# - pragmatic value can be computed in parallel for all future belief states\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641c6a0-f2b4-4d9f-98a6-589388212b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "  \n",
    "def vkl(q, p):\n",
    "    \"\"\" Discrete KL-divergence, vectorized along axis 0 \"\"\"\n",
    "    return (q * (np.log(q) - np.log(p))).sum(axis=1)\n",
    "  \n",
    "def qs_π( env: , q_start, pi ):\n",
    "  \"\"\" propagate q through env following all steps in pi. \"\"\"\n",
    "  q_ss = np.empty(shape=pi.shape+ q_start.shape)\n",
    "  q = q_start\n",
    "  for i, a in enumerate(pi):\n",
    "    q = q @ env.p_s1_given_s_a[:,a,:]\n",
    "    q_ss[i] = q\n",
    "  return q_ss\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "\n",
    "def select_action(env, q_star, q_start, \n",
    "                  k=4, # planning horizon (number of sequential actions per plan)\n",
    "                  β=1., # Bolzmann inverse temperature for policy selection\n",
    "                  use_info_gain=True, \n",
    "                  use_pragmatic_value=True,\n",
    "                  select_max_π=False, # replace sampling with best action selection\n",
    "                  debug=False, # return plans, p of selecting each, and marginal p of actions\n",
    "                 ):\n",
    "  \n",
    "    # genrate all plans\n",
    "    πs = np.asarray( \n",
    "      [x for x in itertools.product( range(env.a_N), repeat=k )]\n",
    "    )\n",
    "\n",
    "    # evaluate negative expected free energy of all plans\n",
    "    num_πs = πs.shape[0]\n",
    "    nefes = np.zeros( shape=num_πs )\n",
    "    log_p_c = np.log( q_star )\n",
    "    for i, π in tqdm( zip( range(num_πs), πs ) ):\n",
    "                     \n",
    "        q_ss = qs_π( env, q_start, π )\n",
    "        \n",
    "        # pragmatic value vectorized over timestep t\n",
    "        if use_pragmatic_value:\n",
    "          nefes[i] = np.dot(q_ss, log_p_c).sum()\n",
    "\n",
    "        # info gain vectorized over timestep t\n",
    "        if use_info_gain:\n",
    "          p_oo = q_ss @ env.p_o_given_s\n",
    "          joint = q_ss[...,None] * env.p_o_given_s # shape: T x S x O\n",
    "          q_oo = joint / joint.sum( axis=1, keepdims=True ) # sum over state dimension\n",
    "          d_oo = (q_oo * (np.log( q_oo ) - np.log( q_ss )[...,None])).sum( axis=1 )\n",
    "          nefes[i] += ( d_oo * p_oo ).sum() # sum over o and t\n",
    "        \n",
    "    # compute probability of following each plan\n",
    "    p_πs = softmax( β * nefes )\n",
    "    \n",
    "    if select_max_π:\n",
    "        π = πs[ np.argmax(nefes) ]\n",
    "    else:    \n",
    "        π = πs[ np.random.choice( num_πs, p=p_πs ) ]\n",
    "    \n",
    "    if debug:\n",
    "        p_a = np.zeros( env.a_N )\n",
    "        for a in range( env.a_N ):\n",
    "          p_a[a] = ( πs[:,0] == a) @ p_πs\n",
    "            \n",
    "        return π, p_a, πs, p_πs\n",
    "    \n",
    "    return π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72190c5-69c0-4516-b548-9788e81fffa0",
   "metadata": {},
   "source": [
    "Let's explore action selection from plans with horizon $k$ by specifying sharp priors on the starting state and target state $k-1$ steps apart.\n",
    "\n",
    "If the starting state is to the right of the target (recall the state space wraps around), then policies that take a sequence of left actions ($a=0$)) are scored higher. Note that this holds true irrespective of the food source location. \n",
    "\n",
    "If the starting state is to the left of the target (e.g., $s_0=11$), then policies that take a sequence of right actions ($a=1$) are scored higher.\n",
    "\n",
    "If the starting state and the target state coincide, then policies that take equal numbers of left and right actions are scored highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6e97b-16a7-4e04-9a86-b73806d0f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "k = 15\n",
    "starting_state = 8\n",
    "target_state = ( starting_state + k - 3) % N\n",
    "print(f'Move from {starting_state}/{N} to {target_state}/{N} in at most {k} steps')\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=0) # location of the food source\n",
    "\n",
    "# initialize belief\n",
    "q_start = np.eye(env.s_N)[starting_state] + np.ones(env.s_N) * 0.01\n",
    "q_start = q_start / q_start.sum()\n",
    "\n",
    "# initialize preference\n",
    "q_star = np.eye(env.s_N)[target_state] + 1e-6 * np.ones(env.s_N)\n",
    "q_star = q_star / q_star.sum()\n",
    "\n",
    "a, p_a, πs, p_πs = select_action(env, q_star, q_start, \n",
    "                                    k=k, # planning horizon\n",
    "                                    β=1., # Bolzmann inverse temperature\n",
    "                                    use_pragmatic_value=True,\n",
    "                                    use_info_gain=True,\n",
    "                                    debug=True)\n",
    "\n",
    "# and explore what the agent prefers\n",
    "#fig, ax = plt.subplots( figsize=(64, 6) )\n",
    "#plt.bar(x = range( πs.shape[0] ), height=p_πs )\n",
    "#plt.xlabel('π id')\n",
    "#plt.ylabel('p(π)')\n",
    "\n",
    "# estimate marginal probability of selecting a plan with first action 0 or 1\n",
    "print('marginal probability of next action')\n",
    "print(p_a)\n",
    "\n",
    "print('most likely plan')\n",
    "print(πs[ np.argmax(p_πs) ], f'p(π): {np.max(p_πs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c1d7f-fb80-417f-88f0-142ca6d13d18",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we have all components required to define a complete Active Infererence agent. Let's encapsulate it into a class that manages the target state and current belief state over time and provides a minimal interface with reset and step methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4dc051-75d5-4718-b95f-5f7a5f95f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgent:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, # planning horizon\n",
    "                 β=1, # Bolzmann inverse temperature for policy sampling\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_π=False): # sample plan (False), select max negEFE (True).\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.β = β\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_π = select_max_π\n",
    "        \n",
    "        self.πs = np.asarray( \n",
    "          [x for x in itertools.product( range(self.env.a_N), repeat=self.k )]\n",
    "        )\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] \\\n",
    "                 + 0.01 * np.ones(self.env.s_N)\n",
    "        self.q_star = q_star / q_star.sum()\n",
    "        self.log_p_c = np.log( self.q_star )\n",
    "        # initialize state prior as uniform\n",
    "        self.q = np.ones(self.env.s_N) / self.env.s_N\n",
    "        \n",
    "    def step(self, o):\n",
    "        # update belief based on observation\n",
    "        joint = self.q * self.env.p_o_given_s[:,int(o)]\n",
    "        self.q = joint / joint.sum()\n",
    "        π = self._select_π()\n",
    "        # propagate belief through time assuming we take action a\n",
    "        self.q = self.q @ self.env.p_s1_given_s_a[:,π[0],:]\n",
    "        return π[0]\n",
    "      \n",
    "    @staticmethod\n",
    "    def _qs_π(p, q_start, π):\n",
    "        \"\"\" propagate q through env following all steps in pi. \"\"\"\n",
    "        num_t = π.shape[0]\n",
    "        q_ss = np.empty(shape= π.shape + q_start.shape)\n",
    "        q = q_start\n",
    "        for i, a in zip( range(num_t), π ):\n",
    "          q = q @ p[:,a,:]\n",
    "          q_ss[i] = q\n",
    "        return q_ss\n",
    "      \n",
    "    def _select_π(self, debug=False):\n",
    "      # rollout\n",
    "      q_ss = np.stack([self._qs_π(self.env.p_s1_given_s_a, self.q, π) for π in self.πs]) # policies x T x states\n",
    "      # pragmatic value\n",
    "      pragmatic = (q_ss @ self.log_p_c).sum(axis=1)\n",
    "      # state info gain\n",
    "      p_oo = q_ss @ self.env.p_o_given_s # prior\n",
    "      joint = q_ss[...,None] * self.env.p_o_given_s\n",
    "      q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "      d_oo = (q_oo * (np_safelog( q_oo ) - np_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "      info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "      #action selection\n",
    "      nefe = self.use_pragmatic_value* pragmatic + self.use_info_gain * info_gain\n",
    "      p_πs = softmax(self.β * nefe)\n",
    "      if self.select_max_π:\n",
    "          return self.πs[ np.argmax(nefe) ]\n",
    "      else:    \n",
    "          return self.πs[ np.random.choice( num_πs, p=p_πs ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47bd8d-514a-45f3-8a4c-d6fa0cdc667e",
   "metadata": {},
   "source": [
    "\n",
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e289b7c-c765-4671-8cb5-aa571ed1f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"jax[cpu]===0.3.14\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax_key = jax.random.PRNGKey(42)\n",
    "jax.devices(\"gpu\")\n",
    "jax.default_device(jax.devices(\"gpu\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e83ac8-51f2-4d34-8779-5799b59e76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgentJax:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, # planning horizon\n",
    "                 β=1, # Bolzmann inverse temperature for policy sampling\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_π=False): # sample plan (False), select max negEFE (True).\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.β = β\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_π = select_max_π\n",
    "        \n",
    "        self.p_t = jnp.asarray(env.p_s1_given_s_a)\n",
    "        self.p_o = jnp.asarray(env.p_o_given_s)\n",
    "        self.πs = jnp.asarray( \n",
    "          [x for x in itertools.product( range(self.env.a_N), repeat=self.k )]\n",
    "        )\n",
    "        self._select_π = jax.jit(self._jax_select_π)\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] \\\n",
    "                 + 0.01 * np.ones(self.env.s_N)\n",
    "        self.q_star = q_star / q_star.sum()\n",
    "        self.log_p_c = jnp.log( self.q_star )\n",
    "        # initialize state prior as uniform\n",
    "        self.q = jnp.asarray(np.ones(self.env.s_N) / self.env.s_N )\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward_step(q, a):\n",
    "      q_ = q @ self.p_t[:,a,:]\n",
    "      return q_, q_ # 'carry', 'y'\n",
    "        \n",
    "    def step(self, o, key):\n",
    "        # update belief based on observation\n",
    "        joint = self.q * self.p_o[:,int(o)]\n",
    "        self.q = joint / joint.sum()\n",
    "        π = self._select_π(key)\n",
    "        # propagate belief through time assuming we take action a\n",
    "        self.q = self.q @ self.p_t[:,π[0],:]\n",
    "        return π[0]\n",
    "      \n",
    "    @staticmethod\n",
    "    def _qs_π(p, q_start, π):\n",
    "        \"\"\" propagate q through env following all steps in pi. \"\"\"\n",
    "        num_t = π.shape[0]\n",
    "        q_ss = np.empty(shape= π.shape + q_start.shape)\n",
    "        q = q_start\n",
    "        for i, a in zip( range(num_t), π ):\n",
    "          q = q @ p[:,a,:]\n",
    "          q_ss[i] = q\n",
    "        return q_ss\n",
    "        \n",
    "    def _jax_select_π(self, key):\n",
    "      \n",
    "      # policy rollout\n",
    "      def jax_qs_π(q_start, π):\n",
    "        _, q_ss = jax.lax.scan(forward_step, init=q_start, xs=π)\n",
    "        return q_ss\n",
    "      q_ss = jax.vmap(jax_qs_π, in_axes=(None, 0), out_axes=0)(self.q, self.πs)\n",
    "      \n",
    "      # pragmatic value\n",
    "      pragmatic = (q_ss @ self.log_p_c).sum(axis=1)\n",
    "      # state info gain\n",
    "      p_oo = q_ss @ self.p_o # prior\n",
    "      joint = q_ss[...,None] * self.p_o\n",
    "      q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "      d_oo = (q_oo * (jax_safelog( q_oo ) - jax_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "      info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "      # action selection\n",
    "      nefe = pragmatic + info_gain\n",
    "      p_πs = jax.nn.softmax(self.β * nefe)\n",
    "      sample_fun = lambda x: jax.random.choice(key, self.πs, p=p_πs)\n",
    "      return jax.lax.cond(self.select_max_π, \n",
    "                       true_fun = lambda x: self.πs[ jnp.argmax(nefe) ], \n",
    "                       false_fun = sample_fun,\n",
    "                       operand=None)\n",
    "      return π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593474a-e694-4689-8452-4c860a4fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import minimal_agent as ma\n",
    "importlib.reload(ma)\n",
    "\n",
    "s_food = 0\n",
    "s_0 = 7\n",
    "target_state = s_0 + 10\n",
    "N = 24 # number states\n",
    "k = 12 # planning horizon; run time increases exponentially with planning horizon\n",
    "T = 128 # number of simulation steps\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=s_food, # location of the food source\n",
    "                    s_0=s_0, \n",
    "                    o_decay=0.15) # starting location \n",
    "\n",
    "agent = MinimalAgentJax(env=env, \n",
    "                        target_state=target_state, \n",
    "                        k=k,\n",
    "                        use_info_gain=True,\n",
    "                        use_pragmatic_value=True,\n",
    "                        select_max_π=True)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "qq = [agent.q]\n",
    "aa = []\n",
    "    \n",
    "for i in tqdm( range(T) ):\n",
    "    jax_key, tmp_key = jax.random.split(jax_key)\n",
    "    a = agent.step(o, tmp_key)\n",
    "    #print(f\"step {i}, s: {env.s_t}, o: {['FOOD', 'NONE'][int(o)]}, p(a): {p_a}, a: {['UP', 'DOWN'][a]}\")\n",
    "    o = env.step(a)\n",
    "    ss.append(env.s_t)\n",
    "    qq.append(agent.q)\n",
    "    aa.append(a)\n",
    "\n",
    "\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(qq).T, label='belief')\n",
    "t = np.arange(len(aa))\n",
    "i_left = t[aa==0]\n",
    "i_right = t[aa==1]\n",
    "plt.scatter(i_left, ss[:-1][i_left], c='white', marker=CARETUP)\n",
    "plt.scatter(i_right, ss[:-1][i_right], c='white', marker=CARETDOWN)\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba4fbe-5f9b-412e-b444-42bb5bd431a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0770b7-e2ea-4fc0-b007-b475b0b9af84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we developed the individual components of an active inference agent that are responsible for updating the agents belief after a new observation arrived and after a new action was taken, and for selecting actions based on the expected free energy, which we separated into an infomation gain term and a pragmatic value term. Each of these components were tested against expected behavior and diagnostic visualisations were introduced that can help adjust learning rates and number of optimization steps and that let us interpret action selection based on the marginal probability of a first action across all evaluated plans.\n",
    "\n",
    "We put everything together, encapsulated properties, state and methods in the `MinimalAgent` class, and analyzed this agent's sequential interactions with the `MinimalEnvironment`.\n",
    "\n",
    "There may be potential for further optimizing the code by separating torch compute graph definitions and their application to optimizing each individual interaction step's data.\n",
    "\n",
    "## Outlook\n",
    "\n",
    "The agentwe  developed above can interact with environments with\n",
    "- discrete state spaces\n",
    "- discrete observation spaces\n",
    "- discrete action spaces\n",
    "\n",
    "In subsequent notebooks, we will modify the agent to enable interaction with environments that have\n",
    "\n",
    "1. continuous action spaces\n",
    "2. continuous observation spaces\n",
    "3. continuous state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b823d-7ea4-494b-9a60-56678dce36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = jnp.eye(env.s_N)[0]\n",
    "p_t = jnp.asarray(env.p_s1_given_s_a)\n",
    "p_o = jnp.asarray(env.p_o_given_s)\n",
    "πs = jnp.asarray(agent.πs)\n",
    "log_p_c = jnp.asarray(agent.log_p_c)\n",
    "select_max_π = False #agent.select_max_π\n",
    "\n",
    "# propagate belief through time following all policies\n",
    "def jax_qs_π(q_start, π):\n",
    "  def forward_step(q, a):\n",
    "    q_ = q @ p_t[:,a,:]\n",
    "    return q_, q_ # 'carry', 'y'\n",
    "\n",
    "  _, q_ss = jax.lax.scan(forward_step, init=q, xs=π)\n",
    "  return q_ss\n",
    "\n",
    "def jax_safelog(x):\n",
    "  return jnp.log( jnp.maximum(x, 1e-16 ))\n",
    "\n",
    "\n",
    "def select_action(key):\n",
    "  # policy rollout\n",
    "  q_ss = jax.vmap(jax_qs_π, in_axes=(None, 0), out_axes=0)(q, πs)\n",
    "  # pragmatic value\n",
    "  pragmatic = (q_ss @ log_p_c).sum(axis=1)\n",
    "  # state info gain\n",
    "  p_oo = q_ss @ p_o # prior\n",
    "  joint = q_ss[...,None] * p_o\n",
    "  q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "  d_oo = (q_oo * (jax_safelog( q_oo ) - jax_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "  info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "  # action selection\n",
    "  nefe = pragmatic + info_gain\n",
    "  p_πs = jax.nn.softmax(β * nefe)\n",
    "  sample_fun = lambda x: jax.random.choice(key, πs, p=p_πs)\n",
    "  return jax.lax.cond(select_max_π, \n",
    "                   true_fun = lambda x: πs[ jnp.argmax(nefe) ], \n",
    "                   false_fun = sample_fun,\n",
    "                   operand=None)\n",
    "  return π\n",
    "    \n",
    "jit_select_action = jax.jit(select_action)\n",
    "\n",
    "jax_key, tmp_key = jax.random.split(jax_key)\n",
    "out = jit_select_action(tmp_key)\n",
    "plt.plot(out)\n",
    "\n",
    "#%timeit jit_select_action()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aecc6d-3344-426a-88ed-3f895955f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoding\n",
    "x = [0, 1, 0, 1, 1]\n",
    "np.eye(2)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223d6e4-b32a-41f5-8471-a30093ced257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
