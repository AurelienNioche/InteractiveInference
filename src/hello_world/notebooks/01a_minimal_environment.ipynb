{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f01fc6-114e-4e7c-bbea-148c62215894",
   "metadata": {},
   "source": [
    "# Rationale\n",
    "\n",
    "This notebook is a variation on `01_minimal_environment`, where the active inference agent represents its internal belief with probabilities of the categorical distribution instead of the logits. This modification allows us to greatly simplify belief updates and action selection by avoiding the need to perform any optimisation of the variational distribution. We've observed a 5x speedup in the computation of the agent's `step`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67ad10-5d62-4df7-a6fd-7a2a62e7ea5a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206e212-0334-4e61-8ed6-80c9b23c9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b53e87-9197-456a-9716-5343a20bf23d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b69ee9-591a-4a3d-b883-a971b59a71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import minimal_environment as me\n",
    "importlib.reload(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d988dfb-1e05-4e95-b459-06a8433d81c2",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "\n",
    "## Belief Update\n",
    "\n",
    "An active inference agent holds a belief about the state of the environment at time $t$, modelled as a probability distribution $Q(s; \\theta_t)$ over states $s \\in \\mathcal{S}$. There is uncertainty associated with this belief as the state cannot be observed directly and must instead be inferred from stochastic observations.\n",
    "\n",
    "### Update through Time\n",
    "\n",
    "At any time $t$ an agent holds a prior belief about $s_t$ before taking in an observation from its environment. This belief could be uniform, for example at the start of an interaction. It could also be informed by propagating the belief $Q(s; \\theta_{t-1})$ about state $s_{t-1}$ through its model of the environment transition dynamics, taking into account the action $a_{t-1}$ taken at the previous time step.\n",
    "\n",
    "$$Q(s; \\theta_{t}) = \\mathbb{E}_{s\\sim Q_{t-1}}[p(s_t|s, a_{t-1})]$$\n",
    "\n",
    "Note that updating the belief in light of the chosen action usually involves fitting the parameters $\\theta_t$. For our minimal agent, we represent $Q(s;\\theta)$ as the categorical distribution with $\\theta_t = {p_i}$ representing probability of each discrete state. This allows exact inference avoiding the need for optimisation of variational parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1760fa-19a1-4d46-9025-23f8a9546a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief_a(env, q_last, a):\n",
    "  return q_last @ env.p_s1_given_s_a[:,a,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a91fb-8cad-4923-bf96-5b95097bab6f",
   "metadata": {},
   "source": [
    "Let's see the effect of this belief update in action. Say we believed strongly that the environment was in states 1 or 4 with equal probability at time $t$ and we took action 1 (move right). Recall that according to the environment dynamics there is some probability of the state remaining unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb0157-065c-4429-804c-1af01a9ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "\n",
    "# prior belief\n",
    "q0 = np.ones(env.s_N) * 0.1 + np.eye(env.s_N)[1] + np.eye(env.s_N)[4]\n",
    "q0 = q0 / q0.sum()\n",
    "\n",
    "# propagating belief through dynamics, conditioned on action a\n",
    "q1 = update_belief_a(env, q0, a=1)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=q0, alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=q1, alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Propagating prior beliefs through environment dynamics.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa51f1-a4af-4718-b3ff-2ae364bcd651",
   "metadata": {},
   "source": [
    "### Update based on new observation\n",
    "\n",
    "Now that we have updated our prior taking into account the action taken in the previous time step and the (agent's model of the) environment dynamics, we turn our attention to updating beliefs in light of a new observation.\n",
    "\n",
    "In active inference, this belief update is cast as minimizing the variational free energy, i.e. minimizing the KL-divergence between $Q(s;\\theta')$ and $p(o, s) = Q(s; \\theta) p(o|s)$ with respect to $\\theta'$.\n",
    "\n",
    "$$D_{KL}(\\quad Q_{\\theta'}(s), p(o|s)Q_{\\theta}(s) \\quad) \\quad = \\quad \\mathbb{E}_{s \\sim Q_{\\theta'}}[\\quad \\log Q_{\\theta'}(s) - \\log p(o|s)Q_{\\theta}(s) \\quad]$$\n",
    "\n",
    "As above, we can compute the posterior belief exactly due to our parameterisation of the belief distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfab13-ef79-42bc-aaca-ef00c90b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, q_last, o):\n",
    "  joint = env.p_o_given_s[:,o] * q_last\n",
    "  return joint / joint.sum()\n",
    "\n",
    "def update_belief_all(env, q_last):\n",
    "  joint = env.p_o_given_s.T * q_last\n",
    "  return joint / joint.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9788a-ec59-4f1f-a93a-1adb6bfc6e78",
   "metadata": {},
   "source": [
    "Let's see the effect of this in action. We start with a uniform prior belief. Recall that the food source is in state 0 and that the probability of observing food decreases exponentially with the distance of a state from the food source, with the state space wrapping around.\n",
    "\n",
    "If we observed no food ($o=0$), then it is most likely that we are in the state furthest away from the food source. If we observed food ($o=1$), then it is most likely that we are at the food source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3ff0d-e87c-46c9-8eda-b1ea2be43a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1\n",
    "\n",
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "q0 = np.ones(env.s_N)/env.s_N\n",
    "q1 = update_belief(env, q0, o=o)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=q0, alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=q1, alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Updating beliefs in light of a new observation.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656b29-cd84-4a3a-b16c-3867020ed1c9",
   "metadata": {},
   "source": [
    "### Accumulating observations over time\n",
    "\n",
    "Note that, based on a single observation, we cannot refine our belief to become most confident in a state that is not the food source itself or furthest away from the food source. This can be improved if we accumulate observations from a single state over time. Because the probability of observing food decreases symmetrically to the left and right of the food source, the agent is likely to hold strong beliefs about all states that share the same rate of observing food, but cannot disambiguate these states without action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494d505-8062-4d25-98be-7dadcf49995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3\n",
    "n_timesteps = 50\n",
    "\n",
    "env = me.MinimalEnv(N=12, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=s) # starting location\n",
    "\n",
    "q = np.ones(env.s_N)/env.s_N # initialize state prior as uniform\n",
    "o = env.reset() # set state to starting state\n",
    "\n",
    "# refine belief by sampling N observations but without taking any action\n",
    "qq = []\n",
    "for i in range(n_timesteps):\n",
    "    q = update_belief(env, q_last=q, o=int(o))\n",
    "    qq.append(q)\n",
    "    o = env.sample_o()\n",
    "\n",
    "plt.bar(range(env.s_N), qq[int(len(qq)*0.25)], alpha=0.3, label='25% observations');\n",
    "plt.bar(range(env.s_N), qq[-1], alpha=0.5, label='all observations');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739bb20-6a71-45b0-85ed-78d674953104",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "In active inference, sequences of actions (plans, policies) are scored by the negative expected free energy, and selected by exponentiating and normalizing, i.e. sampling from the softmax over plans. Plans $\\pi: a_0, a_1, ..., a_{K-1}$ define sequences of actions up to a finite horizon of $K$ timesteps into the future.\n",
    "\n",
    "The expected free energy can be decomposed in various ways and here we chose one that we find most intuitive, involving a _pragmatic_ term and an _information gain_ term (See Eq 4.9, page 73 in the book). \n",
    "\n",
    "The _pragmatic_ term assesses the probability of arriving in states following $\\pi$ that the agent desires, or of encountering observations that the agent desires. In this context, $Q_\\theta$ is estimated by propagating beliefs through the environment transition dynamics following the sequence of actions defined by $\\pi$, and observations are halucinated by sampling from the emission probability distributions.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}}\\left[\\log p_c(s)\\right] \\quad \\text{or} \\quad \\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[\\log p_c(o)\\right]$\n",
    "\n",
    "The _information gain_ term quantifies the belief update due to making observations in future states.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[ D_{KL}(\\, Q_{\\theta'}(s|o),  Q_{\\theta}(s) \\,) \\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641c6a0-f2b4-4d9f-98a6-589388212b37",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def qs_π( env , q_start, pi ):\n",
    "  \"\"\" propagate q through env following all steps in pi. \"\"\"\n",
    "  q_ss = np.empty(shape=pi.shape+ q_start.shape)\n",
    "  q = q_start\n",
    "  for i, a in enumerate(pi):\n",
    "    q = q @ env.p_s1_given_s_a[:,a,:]\n",
    "    q_ss[i] = q\n",
    "  return q_ss\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "\n",
    "def select_action(env, q_star, q_start, \n",
    "                  k=4, # planning horizon (number of sequential actions per plan)\n",
    "                  β=1., # Bolzmann inverse temperature for policy selection\n",
    "                  use_info_gain=True, \n",
    "                  use_pragmatic_value=True,\n",
    "                  select_max_π=False, # replace sampling with best action selection\n",
    "                  debug=False, # return plans, p of selecting each, and marginal p of actions\n",
    "                 ):\n",
    "  \n",
    "    # genrate all plans\n",
    "    πs = np.asarray( \n",
    "      [x for x in itertools.product( range(env.a_N), repeat=k )]\n",
    "    )\n",
    "\n",
    "    # evaluate negative expected free energy of all plans\n",
    "    num_πs = πs.shape[0]\n",
    "    nefes = np.zeros( shape=num_πs )\n",
    "    log_p_c = np.log( q_star )\n",
    "    for i, π in tqdm( zip( range(num_πs), πs ) ):\n",
    "                     \n",
    "        q_ss = qs_π( env, q_start, π )\n",
    "        \n",
    "        # pragmatic value vectorized over timestep t\n",
    "        if use_pragmatic_value:\n",
    "          nefes[i] = np.dot(q_ss, log_p_c).sum()\n",
    "\n",
    "        # info gain vectorized over timestep t\n",
    "        if use_info_gain:\n",
    "          p_oo = q_ss @ env.p_o_given_s\n",
    "          joint = q_ss[...,None] * env.p_o_given_s # shape: T x S x O\n",
    "          q_oo = joint / joint.sum( axis=1, keepdims=True ) # sum over state dimension\n",
    "          d_oo = (q_oo * (np.log( q_oo ) - np.log( q_ss )[...,None])).sum( axis=1 )\n",
    "          nefes[i] += ( d_oo * p_oo ).sum() # sum over o and t\n",
    "        \n",
    "    # compute probability of following each plan\n",
    "    p_πs = softmax( β * nefes )\n",
    "    \n",
    "    if select_max_π:\n",
    "        π = πs[ np.argmax(nefes) ]\n",
    "    else:    \n",
    "        π = πs[ np.random.choice( num_πs, p=p_πs ) ]\n",
    "    \n",
    "    if debug:\n",
    "        p_a = np.zeros( env.a_N )\n",
    "        for a in range( env.a_N ):\n",
    "          p_a[a] = ( πs[:,0] == a) @ p_πs\n",
    "            \n",
    "        return π, p_a, πs, p_πs\n",
    "    \n",
    "    return π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72190c5-69c0-4516-b548-9788e81fffa0",
   "metadata": {},
   "source": [
    "Let's explore action selection from plans with horizon $k$ by specifying sharp priors on the starting state and target state $k-1$ steps apart.\n",
    "\n",
    "If the starting state is to the right of the target (recall the state space wraps around), then policies that take a sequence of left actions ($a=0$)) are scored higher. Note that this holds true irrespective of the food source location. \n",
    "\n",
    "If the starting state is to the left of the target (e.g., $s_0=11$), then policies that take a sequence of right actions ($a=1$) are scored higher.\n",
    "\n",
    "If the starting state and the target state coincide, then policies that take equal numbers of left and right actions are scored highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6e97b-16a7-4e04-9a86-b73806d0f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "k = 15\n",
    "starting_state = 8\n",
    "target_state = ( starting_state + k - 3) % N\n",
    "print(f'Move from {starting_state}/{N} to {target_state}/{N} in at most {k} steps')\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=0) # location of the food source\n",
    "\n",
    "# initialize belief\n",
    "q_start = np.eye(env.s_N)[starting_state] + np.ones(env.s_N) * 0.01\n",
    "q_start = q_start / q_start.sum()\n",
    "\n",
    "# initialize preference\n",
    "q_star = np.eye(env.s_N)[target_state] + 1e-6 * np.ones(env.s_N)\n",
    "q_star = q_star / q_star.sum()\n",
    "\n",
    "a, p_a, πs, p_πs = select_action(env, q_star, q_start, \n",
    "                                    k=k, # planning horizon\n",
    "                                    β=1., # Bolzmann inverse temperature\n",
    "                                    use_pragmatic_value=True,\n",
    "                                    use_info_gain=True,\n",
    "                                    debug=True)\n",
    "\n",
    "# and explore what the agent prefers\n",
    "#fig, ax = plt.subplots( figsize=(64, 6) )\n",
    "#plt.bar(x = range( πs.shape[0] ), height=p_πs )\n",
    "#plt.xlabel('π id')\n",
    "#plt.ylabel('p(π)')\n",
    "\n",
    "# estimate marginal probability of selecting a plan with first action 0 or 1\n",
    "print('marginal probability of next action')\n",
    "print(p_a)\n",
    "\n",
    "print('most likely plan')\n",
    "print(πs[ np.argmax(p_πs) ], f'p(π): {np.max(p_πs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c1d7f-fb80-417f-88f0-142ca6d13d18",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we have all components required to define a complete Active Infererence agent. Let's encapsulate it into a class that manages the target state and current belief state over time and provides a minimal interface with reset and step methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4dc051-75d5-4718-b95f-5f7a5f95f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_safelog(x):\n",
    "  return np.log( np.maximum(x, 1e-16) )\n",
    "\n",
    "class MinimalAgent:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, # planning horizon\n",
    "                 β=1, # Bolzmann inverse temperature for policy sampling\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_π=False): # sample plan (False), select max negEFE (True).\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.β = β\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_π = select_max_π\n",
    "        print(f'Enumerating {self.env.a_N**k:,} candidate policies of length {k}')\n",
    "        self.πs = np.stack(np.meshgrid(*[np.arange(self.env.a_N) for _ in range(k)])).T.reshape(-1, k)\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] \\\n",
    "                 + 0.01 * np.ones(self.env.s_N)\n",
    "        self.q_star = q_star / q_star.sum()\n",
    "        self.log_p_c = np.log( self.q_star )\n",
    "        # initialize state prior as uniform\n",
    "        self.q = np.ones(self.env.s_N) / self.env.s_N\n",
    "        \n",
    "    def step(self, o):\n",
    "        # update belief based on observation\n",
    "        joint = self.q * self.env.p_o_given_s[:,int(o)]\n",
    "        self.q = joint / joint.sum()\n",
    "        π = self._select_π()\n",
    "        # propagate belief through time assuming we take action a\n",
    "        self.q = self.q @ self.env.p_s1_given_s_a[:,π[0],:]\n",
    "        return π[0]\n",
    "      \n",
    "    @staticmethod\n",
    "    def _qs_π(p, q_start, π):\n",
    "        \"\"\" propagate q through env following all steps in pi. \"\"\"\n",
    "        num_t = π.shape[0]\n",
    "        q_ss = np.empty(shape= π.shape + q_start.shape)\n",
    "        q = q_start\n",
    "        for i, a in zip( range(num_t), π ):\n",
    "          q = q @ p[:,a,:]\n",
    "          q_ss[i] = q\n",
    "        return q_ss\n",
    "      \n",
    "    def _select_π(self, debug=False):\n",
    "      # rollout\n",
    "      q_ss = np.stack([self._qs_π(self.env.p_s1_given_s_a, self.q, π) for π in self.πs]) # policies x T x states\n",
    "      # pragmatic value\n",
    "      pragmatic = (q_ss @ self.log_p_c).sum(axis=1)\n",
    "      # state info gain\n",
    "      p_oo = q_ss @ self.env.p_o_given_s # prior\n",
    "      joint = q_ss[...,None] * self.env.p_o_given_s\n",
    "      q_oo = joint / joint.sum( axis=2, keepdims=True ) # conditional\n",
    "      d_oo = (q_oo * (np_safelog( q_oo ) - np_safelog( q_ss )[...,None])).sum( axis=2 ) # KL\n",
    "      info_gain = (d_oo * p_oo).sum(axis=(1, 2)) # sum over o and t\n",
    "      #action selection\n",
    "      nefe = self.use_pragmatic_value* pragmatic + self.use_info_gain * info_gain\n",
    "      p_πs = softmax(self.β * nefe)\n",
    "      if self.select_max_π:\n",
    "          return self.πs[ np.argmax(nefe) ]\n",
    "      else:    \n",
    "          return self.πs[ np.random.choice( num_πs, p=p_πs ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47bd8d-514a-45f3-8a4c-d6fa0cdc667e",
   "metadata": {},
   "source": [
    "\n",
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593474a-e694-4689-8452-4c860a4fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import minimal_agent as ma\n",
    "importlib.reload(ma)\n",
    "\n",
    "s_food = 0\n",
    "s_0 = 6\n",
    "target_state = 26\n",
    "N = 32 # number states\n",
    "k = 14 # planning horizon; run time increases exponentially with planning horizon\n",
    "T = 128 # number of simulation steps\n",
    "\n",
    "env = me.MinimalEnv(N=N, # number of states\n",
    "                    s_food=s_food, # location of the food source\n",
    "                    s_0=s_0, \n",
    "                    o_decay=0.1) # starting location \n",
    "\n",
    "agent = MinimalAgent(env=env, \n",
    "                        target_state=target_state, \n",
    "                        k=k,\n",
    "                        use_info_gain=True,\n",
    "                        use_pragmatic_value=True,\n",
    "                        select_max_π=True)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "qq = [agent.q]\n",
    "aa = []\n",
    "    \n",
    "for i in tqdm( range(T) ):\n",
    "    a = agent.step(o)\n",
    "    #print(f\"step {i}, s: {env.s_t}, o: {['FOOD', 'NONE'][int(o)]}, p(a): {p_a}, a: {['UP', 'DOWN'][a]}\")\n",
    "    o = env.step(a)\n",
    "    ss.append(env.s_t)\n",
    "    qq.append(agent.q)\n",
    "    aa.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec41862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(qq).T, label='belief', aspect='auto')\n",
    "t = np.arange(len(aa))\n",
    "i_left = t[aa==0]\n",
    "i_right = t[aa==1]\n",
    "plt.scatter(i_left, ss[:-1][i_left], c='white', marker=CARETUP)\n",
    "plt.scatter(i_right, ss[:-1][i_right], c='white', marker=CARETDOWN)\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba4fbe-5f9b-412e-b444-42bb5bd431a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.p_o_given_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0770b7-e2ea-4fc0-b007-b475b0b9af84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we developed the individual components of an active inference agent that are responsible for updating the agents belief after a new observation arrived and after a new action was taken, and for selecting actions based on the expected free energy, which we separated into an infomation gain term and a pragmatic value term. Each of these components were tested against expected behavior and diagnostic visualisations were introduced that can help adjust learning rates and number of optimization steps and that let us interpret action selection based on the marginal probability of a first action across all evaluated plans.\n",
    "\n",
    "We put everything together, encapsulated properties, state and methods in the `MinimalAgent` class, and analyzed this agent's sequential interactions with the `MinimalEnvironment`.\n",
    "\n",
    "There may be potential for further optimizing the code by separating torch compute graph definitions and their application to optimizing each individual interaction step's data.\n",
    "\n",
    "## Outlook\n",
    "\n",
    "The agentwe  developed above can interact with environments with\n",
    "- discrete state spaces\n",
    "- discrete observation spaces\n",
    "- discrete action spaces\n",
    "\n",
    "In subsequent notebooks, we will modify the agent to enable interaction with environments that have\n",
    "\n",
    "1. continuous action spaces\n",
    "2. continuous observation spaces\n",
    "3. continuous state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093ac33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
