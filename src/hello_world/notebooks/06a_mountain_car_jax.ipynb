{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67325b8c",
   "metadata": {},
   "source": [
    "# Housekeeping (execute once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d11a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pip\n",
    "!pip install --user --upgrade tqdm watermark\n",
    "!pip install --user --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "#!pip install \"jax[cpu]===0.3.14\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent for local imports\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "import minimal_agent_jax as jax_agents\n",
    "import mountain_car_environment as envs\n",
    "importlib.reload(envs)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c026c9",
   "metadata": {},
   "source": [
    "# Discrete Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51bec1",
   "metadata": {},
   "source": [
    "## Compute discrete transition dynamics\n",
    "\n",
    "**Note** Uncomment the code in the cell below if it throws an error.\n",
    "\n",
    "The stochastic discrete dynamics are estimated by Monte Carlo sampling the deterministic continuous dynamics, where stochasticity arises as a result of discretization. Actions performed at different continuous-valued starting states within each discrete state bin result in different continuous-valued successor states, which may fall into several different discrete successor state bins. Normalizing the sample of successor states yields a conditional probability distribution for each state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "τ = envs.MountainCarDiscrete.load_transition_dynamics()\n",
    "#τ = envs.MountainCarDiscrete.transition_dynamics(n_samples=50_000)\n",
    "#envs.MountainCarDiscrete.save_transition_dynamics(τ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b14509",
   "metadata": {},
   "source": [
    "## Explore transition dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229aaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.MountainCarDiscrete()\n",
    "env.render_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_i = [0]\n",
    "s0 = np.array([0, -1])\n",
    "s0_i = env.index_s_from_s(s0)\n",
    "\n",
    "plt.title(f\"$p(s|s_t={s0.tolist()}, a={a_i})$\");\n",
    "plt.xlabel('horizontal position')\n",
    "plt.ylabel('velocity')\n",
    "plt.plot([s0[0]], [s0[1]], 'r*', label='$s_t$', zorder=1)\n",
    "plt.legend()\n",
    "env.show_p_s(env.p_s1_given_s_a[s0_i, a_i, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05acc6c8",
   "metadata": {},
   "source": [
    "## Rollout deterministic continuous dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb24836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tranjectories (in continuous dynamics) from the paper\n",
    "s0 = [-0.06451613, -0.09677419]\n",
    "aa_fig4 = [4, 4, 3, 2, 2, 1, 1, 2, 3, 3, 3, 1, 2, 1, 2] \n",
    "        #, 4, 4, 3, 2, 2, 2, 2, 2, 3, 3, 2, 1, 2, 1, 2]\n",
    "aa_fig5 = [2, 1, 1, 3, 3, 3, 2, 2, 3, 1, 2, 2, 2, 2, 2, 1] #[2, 1, 1, 3, 3, 3, 2, 2, 3, 1, 2, 2, 2, 2, 2, 1]\n",
    "\n",
    "aa = aa_fig4\n",
    "a = [-2, -1, 0, 1, 2]\n",
    "aa = [a[x] for x in aa]\n",
    "\n",
    "ss = [s0]\n",
    "ss_full = [s0]\n",
    "for a in aa:\n",
    "  sol = env.forward_ode(s=ss[-1], a=a, debug=True)\n",
    "  ss.append(sol.y[:,-1])\n",
    "  ss_full.extend(sol.y.T.tolist())\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(2*8, 6))\n",
    "#env.plot_trajectory(ss_full, ax, label='full')\n",
    "env.plot_trajectory_continuous(ss, ax, label='steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074575d",
   "metadata": {},
   "source": [
    "## Rollout stochastic discrete dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_plan(env, plan):\n",
    "    ss = [env.reset()]\n",
    "    for a in plan:\n",
    "        s = env.step(a)\n",
    "        ss.append(np.copy(s))\n",
    "        \n",
    "    return ss\n",
    "\n",
    "# random agent\n",
    "def rollout_random(env, T):\n",
    "    plan = np.random.choice(env.a_N, size=T)\n",
    "    return rollout_plan(env, plan)\n",
    "    \n",
    "T = 16\n",
    "fig, ax = plt.subplots(1, 2, figsize=(2*8, 6))\n",
    "for i in range(100):\n",
    "    env.plot_trajectory(rollout_plan(env, aa_fig4), ax=ax, color='gray', alpha=0.1, label='steps', legend=False if i>0 else True)\n",
    "    #env.plot_trajectory(rollout_random(env, T), ax, label='steps')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169f3d",
   "metadata": {},
   "source": [
    "# Jax agent for fully observed environments\n",
    "\n",
    "Here, we want to apply the Jax active inference agent to navigate through the state space. Since the state is fully observed, we can simplify the implementation by removing the information gain component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - remove p_o_given_s\n",
    "# - jax_step_fun:\n",
    "#    - remove o; belief update from observation\n",
    "#    - remove state info_gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax_key = jax.random.PRNGKey(42)\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX device type: {jax.devices()[0].device_kind}\")\n",
    "\n",
    "def jax_safelog(x):\n",
    "  return jnp.log( jnp.maximum(x, 1e-16 ))\n",
    "\n",
    "def jax_fully_observed_batched_fun(q, πs, p_t, log_p_c):\n",
    "    \"\"\" Deterministic policy selection with pragmatic value in fully-observed environment.\n",
    "    Args:\n",
    "        num_actions (int): number of unique actions. Jit assumes this is static across calls.\n",
    "        q (jnp.ndarray): one-hot encoding of the observed state\n",
    "        πs (jnp.ndarray): 4D tensor of one-hot actions [batch, policy, timestep, one_hot_action].\n",
    "        p_t (jnp.ndarray): 3D tensor of transition dynamics [a, s0, s1].\n",
    "        log_p_c (jnp.ndarray): 1D tensor of normalized state preferences. One element per environment state.\n",
    "    \"\"\"\n",
    "    # policy rollout: \n",
    "    # one-hot action encoding ensures coalesced access to transition dynamics, \n",
    "    # speeding up vectorised rollouts across policies\n",
    "    def step(q, a):\n",
    "        q = a @ (q @ p_t)\n",
    "        return q, q\n",
    "    \n",
    "    qs_π = lambda q, π: jax.lax.scan(step, init=q, xs=π)[1] # carry, output\n",
    "    qs_batch = jax.vmap(qs_π, in_axes=(None, 0), out_axes=(0)) # parallel rollout of policy batch\n",
    "    # batch computation of negative free energy (here, only pragmatic value)\n",
    "    def nefe_batch(q, πs):\n",
    "        pragmatic = (qs_batch(q, πs) @ log_p_c).sum(axis=-1)\n",
    "        return q, pragmatic\n",
    "    \n",
    "    nefe = jax.lax.scan(nefe_batch, init=q, xs=πs)[1] # scan over batches\n",
    "    π = πs.reshape( (-1,) + πs.shape[2:] )[jnp.argmax(nefe)] # perform policy selection by squeezing first two dimensions\n",
    "    π = jnp.argmax(π, axis=-1)\n",
    "    return π\n",
    "\n",
    "jit_fully_observed_batched_fun = jax.jit(jax_fully_observed_batched_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyObservedAgentJax:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, \n",
    "                 num_batches=1):\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.num_batches = num_batches\n",
    "        \n",
    "        self.p_t = jnp.asarray(env.p_s1_given_s_a.swapaxes(0,1))\n",
    "        print(f'Enumerating {self.env.a_N**k:,} candidate policies of length {k}')\n",
    "        #batched policies\n",
    "        πs = np.stack(np.meshgrid(*[np.arange(self.env.a_N) for _ in range(k)])).T.reshape(self.num_batches, -1, k)\n",
    "        # one-hot encoded actions\n",
    "        self.πs = jax.nn.one_hot(jax.device_put(πs), self.env.a_N, dtype=int)\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] \\\n",
    "                 + 1/(self.env.s_N*5) * np.ones(self.env.s_N)\n",
    "        q_star = q_star / q_star.sum()\n",
    "        self.log_p_c = jnp.log( q_star )\n",
    "        # initialize state prior as uniform\n",
    "    \n",
    "    def step(self, o):\n",
    "        params = {\n",
    "            'q': jax.nn.one_hot(o, self.env.s_N),\n",
    "            'p_t': self.p_t,\n",
    "            'log_p_c': self.log_p_c,\n",
    "            'πs': self.πs, \n",
    "        }\n",
    "        return jit_fully_observed_batched_fun(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199174c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(jax_agents)\n",
    "\n",
    "k = 10 # planning time horizon\n",
    "# specify target state as continuous [position, velocity]\n",
    "targets = {\n",
    "    4: [-1.0, -0.5],\n",
    "    6: [-0.2, 1.3],\n",
    "    8: [1., 0.],\n",
    "    9: [1., 0.],\n",
    "    10: [1., 0.],\n",
    "}\n",
    "s_target = np.array(targets[k]) \n",
    "env = envs.MountainCarDiscrete()\n",
    "agent = jax_agents.FullyObservedAgentJax(env=env, \n",
    "                        target_state=env.index_s_from_s(s_target), \n",
    "                        k=k, \n",
    "                        num_batches=5**3)\n",
    "agent.reset()\n",
    "\n",
    "# plan once at t=0, then execute plan - measure latency\n",
    "for _ in tqdm([0]):\n",
    "    aa = np.copy(agent.step(o=env.reset()))\n",
    "    \n",
    "print(f\"plan: {aa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9801132",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rollouts = 1\n",
    "\n",
    "fig, ax = plt.subplots(1,2 , figsize=(2*8, 6))\n",
    "ax[0].scatter([s_target[0]], [s_target[1]], color='r', marker='o', s=10**2, label='target', zorder=1)\n",
    "for r in range(n_rollouts):\n",
    "    ss = [env.reset()]\n",
    "    for a in aa:\n",
    "        ss.append(env.step(a))\n",
    "\n",
    "    env.plot_trajectory(ss, ax=ax, color='gray', legend=False if r>0 else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'q': jax.nn.one_hot(env.reset(), env.s_N),\n",
    "    'p_t': agent.p_t,\n",
    "    'log_p_c': agent.log_p_c,\n",
    "    'πs': agent.πs.reshape(5**3,-1,agent.k),\n",
    "    'num_actions': env.a_N\n",
    "}\n",
    "\n",
    "jit_fully_observed_batched_fun(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_old = (8*390625)/55.52 # naive jax implementation required 55.52 seconds for 390k policies of 8 steps\n",
    "num_new = (10*9765625)/21.8 # new jax implementation requires 28.52 seconds for 9765k policies of 10 steps\n",
    "print(f'From {int(num_old):,} to {int(num_new):,} steps per second: {int(num_new/num_old)} x speedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efb88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b93d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient descent on probability (belief) over future actions\n",
    "# - SUCCESS explore actione expansion to one-hot with subsequent full matrix multiply in current jax kernel [t, num_a]\n",
    "# - initialise vector over actions as logits [t, num_a]\n",
    "# - perform gradient descent on \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
