{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79234ddf-95cb-4317-80d4-052bdbb98d1a",
   "metadata": {},
   "source": [
    "# Continuous Action Spaces\n",
    "\n",
    "In this notebook we develop the active inference agent for environments with continuous action spaces. \n",
    "\n",
    "We start by modifying the minimal environment to accept continous-valued actions that represent an agents intention to move by `[-2,2]` cells. Then, we modify the components of the minimal agent that currently exploit the discreteness of the action space, namely the belief-update after an action was taken (belief propagation through time) and policy sampling during action selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858938f-16a2-425b-b355-49c07e3d572f",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac89f44-47fb-44c3-ab3d-f3c45fa026d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97daeb-b972-40d1-8c2f-51f21baa7637",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c85006-24bf-4e73-b17e-638234a196b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702dff16-5cf1-465b-8a91-7394b6052967",
   "metadata": {},
   "source": [
    "# Continuous Action Environment\n",
    "\n",
    "## From continuous actions to discrete environment steps\n",
    "\n",
    "The environment transforms a continuous-valued action intent into a probability distribution over discrete actions (steps of size 0, 1 or 2 in either direction) and samples from this distribution to advance the environemnt state in each step. Below, we illustrate the relevant code and illustrate the distribution over discrete actions given a continuous-valued intent. The stochasticity of the environment transition dynamics is controlled by the variance of the clipped Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218354b1-c8b8-4009-bd73-3299919b74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate conditional probability table for a fixed action a, where the\n",
    "# discrete transition probability is proportional to some Gaussian around a.\n",
    "s_N = 5\n",
    "\n",
    "def p_a_discrete_given_a(a, a_lims=[-2,2], var=0.75**2):\n",
    "  # probability distribution of a discrete action (step) given a continuous\n",
    "  # action intent.\n",
    "  a = np.clip(a, a_lims[0], a_lims[1])\n",
    "  a_discrete = np.arange(2*s_N-1) - s_N + 1\n",
    "  p_a = np.exp(-0.5 * (a_discrete-a)**2 / var)\n",
    "  p_a[a_discrete > a_lims[1]] = 0\n",
    "  p_a[a_discrete < a_lims[0]] = 0\n",
    "  p_a = p_a/p_a.sum()\n",
    "  return a_discrete, p_a\n",
    "  \n",
    "a, p_a = p_a_discrete_given_a(a=2)\n",
    "plt.bar(x=a, height=p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed18e3-3854-4477-aedf-ceb5b53967d5",
   "metadata": {},
   "source": [
    "## Action-specific state transition table\n",
    "\n",
    "It might be convenient for the agent (we'll see) to have access to an action-dependent conditional state-transition probability table to avoid relying exclusively on sampling transitions. In order to calculate this, we compute the signed distance between all pairs of states and assign the relevant probability of the corresponding discrete action to each state transition. \n",
    "\n",
    "Note that the all-pairs signed wrap-around distances only need to be computed once and can then be reused for each new `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bc77a-2b06-4b7a-8559-184fc6ea5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate signed distance of destination from source (do this once)\n",
    "s_N = 8\n",
    "s = np.arange(s_N)\n",
    "other, this = np.meshgrid(s, s)\n",
    "d = other - this\n",
    "d1 = other - this + s_N\n",
    "d2 = other - this - s_N\n",
    "d[np.abs(d) > np.abs(d1)] = d1[np.abs(d) > np.abs(d1)]\n",
    "d[np.abs(d) > np.abs(d2)] = d2[np.abs(d) > np.abs(d2)]\n",
    "d_s = d\n",
    "\n",
    "def p_s1_given_s_a(a, d_s, var):\n",
    "  \"\"\" computes transition probability p(s'| s, a) for specific a\n",
    "\n",
    "  Returns:\n",
    "  p[s, s1] of size (s_N, s_N)\n",
    "  \"\"\"\n",
    "  a_d, p_a = p_a_discrete_given_a(a=a, var=var)\n",
    "  return p_a[d_s - a_d[0]]\n",
    "  \n",
    "plt.imshow(p_s1_given_s_a(a=-0.5, d_s=d_s, var=1.5**2), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41658d9c-e1db-4920-bfd1-9c3e987e76fd",
   "metadata": {},
   "source": [
    "## Full environment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebebc82-9942-4bdc-9fc2-b2ca5b6a7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "class ContinuousActionEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (bool): food detected yes/ no.\n",
    "  actions(float): [-2, 2] intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 0, # where is the food?\n",
    "               sigma_move = 0.75, # Gaussian stdev around continuous move\n",
    "               a_lims = [-2, 2], # maximum step in either direction.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.var_move = sigma_move**2\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.o_N = 2 # {False, True} indicating whether food has been found\n",
    "    self.a_lims = a_lims\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    \n",
    "    We pre-compute the full conditional emission probability table as before.\n",
    "    \"\"\"\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    \"\"\"\n",
    "    With continuous-valued actions, we can nolonger represent (1.) with a \n",
    "    single conditional probability table. However, we can generate one table of\n",
    "    size |S| x |S| for each continuous action value.\n",
    "    \"\"\"\n",
    "    self.d_s = self._signed_state_distances()\n",
    "    # self.p_s1_given_s_a(a=a) returns matrix p[s, s1] for given a\n",
    "    \n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "  def _signed_state_distances(self):\n",
    "    s = np.arange(self.s_N)\n",
    "    other, this = np.meshgrid(s, s)\n",
    "    d = other - this\n",
    "    d1 = other - this + self.s_N\n",
    "    d2 = other - this - self.s_N\n",
    "    d[np.abs(d) > np.abs(d1)] = d1[np.abs(d) > np.abs(d1)]\n",
    "    d[np.abs(d) > np.abs(d2)] = d2[np.abs(d) > np.abs(d2)]\n",
    "    return d\n",
    "  \n",
    "  def _p_a_discrete_given_a(self, a):\n",
    "    # probability distribution of a discrete action (step) given a continuous\n",
    "    # action intent.\n",
    "    a = np.clip(a, self.a_lims[0], self.a_lims[1])\n",
    "    a_discrete = np.arange(2*self.s_N-1) - self.s_N + 1\n",
    "    p_a = np.exp(-0.5 * (a_discrete-a)**2 / self.var_move)\n",
    "    p_a[a_discrete > self.a_lims[1]] = 0\n",
    "    p_a[a_discrete < self.a_lims[0]] = 0\n",
    "    p_a = p_a/p_a.sum()\n",
    "    return a_discrete, p_a\n",
    "  \n",
    "  def p_s1_given_s_a(self, a):\n",
    "    \"\"\" computes transition probability p(s'| s, a) for specific a\n",
    "    \n",
    "    Note: this is provided for convenience in the agent; it is not used within\n",
    "    the environment simulation.\n",
    "\n",
    "    Returns:\n",
    "    p[s, s1] of size (s_N, s_N)\n",
    "    \"\"\"\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return p_a[self.d_s - a_d[0]]\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" computes conditional probability table p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s, o] of size (s_N, o_N)\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.abs(s - self.s_N - self.s_food))\n",
    "    p = np.zeros((self.s_N, self.o_N))\n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    p[:,1] = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    p[:,0] = 1 - p[:,1]\n",
    "    return p\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "      \n",
    "    a_discrete = self.sample_a(a)\n",
    "    self.s_t = (self.s_t + a_discrete) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return np.random.random() < self.p_o_given_s[self.s_t,1]\n",
    "  \n",
    "  def sample_a(self, a):\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return np.random.choice(a_d, p=p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c5ae9-e426-4c1d-bb8f-4f3129ca3ce8",
   "metadata": {},
   "source": [
    "## Random Agent Behavior\n",
    "\n",
    "To test the environment we simulate a random agent's interactions with it. Here, the random agent samples actions uniformly in the interval `[-2, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75278d61-90cd-43eb-898b-4b3a93bfbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousActionEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    sigma_move=0.75, # Gaussian noise around continuous move\n",
    "                    a_lims=[-3,3], # maximum number of steps in either direction\n",
    "                    o_decay=0.4) # decay of observing food away from source \n",
    "\n",
    "n_steps = 100\n",
    "ss, oo, aa = [], [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "oo.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.uniform(low=env.a_lims[0], high=env.a_lims[1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c95a9b-0564-49f9-a97c-18285abc2930",
   "metadata": {},
   "source": [
    "We inspect the sequence of states, actions and emissions during this interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34d052-d9c3-4a77-9db8-9ee78138f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "for i in range(len(aa)):\n",
    "  ax[0].plot([i, i], [ss[i], ss[i]+aa[i]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] > 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "  \n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('$s$')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.array(os))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation (1=Food)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d940c78-451e-4610-a927-ffb44582c4af",
   "metadata": {},
   "source": [
    "# Continuous Action Agent\n",
    "\n",
    "We start implementing the continuous action agent by updating its belief through time (whenever a new action is taken).\n",
    "\n",
    "## Updating belief through time\n",
    "\n",
    "Here, we can make use of the action-specific state transition probability table, requiring only a minimal change to the code of the minimal agent.\n",
    "\n",
    "```\n",
    "q1 = torch.matmul(q, torch.tensor(env.p_s1_given_s_a(a=a)))\n",
    "```\n",
    "\n",
    "Instead of indexing the full transition dynamics `p[s, a, s1]` with action `a`, we generate the action-specific transition dynamics `p[s, s1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98230b9-e1d0-4357-8283-3b497856a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief_a(env, theta_prev, a, lr=4., n_steps=10, debug=False):\n",
    "    # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    q = torch.nn.Softmax(dim=0)(theta)\n",
    "    \n",
    "    # this is the prior for the distribution at time t\n",
    "    # if we worked on this level, we would be done. \n",
    "    # but we need to determine the parameters of Q that produce \n",
    "    # this distribution\n",
    "    q1 = torch.matmul(q, torch.tensor(env.p_s1_given_s_a(a=a)))\n",
    "\n",
    "    # initialize updated belief to uniform\n",
    "    theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "    loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    if debug:\n",
    "        ll = np.zeros(n_steps)\n",
    "        \n",
    "    for i in range(n_steps):\n",
    "        l = loss(theta1, q1)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeedf3-aa5b-437c-8c01-def5043c092a",
   "metadata": {},
   "source": [
    "Let's see the effect of this belief update in action. Say we believed strongly that the environment was in states 1 or 4 with equal probability at time $t$ and we took action 2 (move right by two). Recall that according to the environment dynamics there is some probability of the state remaining unchanged, some probability of moving by 1 and some probability of moving by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f75fe-56bf-4d47-b369-9d026f822bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousActionEnv(N=8, # number of states\n",
    "                          s_food=0) # location of the food source\n",
    "\n",
    "theta = np.eye(env.s_N)[1] * 2 + np.eye(env.s_N)[4] * 2\n",
    "theta1, ll = update_belief_a(env, theta, a=2, \n",
    "                             lr=4.0, n_steps=20, debug=True)\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(ll)\n",
    "plt.plot([0, ll.shape[0]-1], [ll.min()]*2, 'k--')\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta1), alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Propagating prior beliefs through environment dynamics.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab72b2-1d32-4387-bf3d-54caf199f3d0",
   "metadata": {},
   "source": [
    "## Policy sampling\n",
    "\n",
    "The minimal agent explored policies by enumerating and evaluating all possible combinations of discrete actions up to a finite time horizon via rollout simulations. With continuous-valued actions, we can nolonger evaluate all possible finite-horizon strategies and therefore need to resort to sampling.\n",
    "\n",
    "Before we start we copy some code unchanged from the previous notebook, because action selection requires belief updating in light of new observations and rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6b6e0-9c8c-463b-ab64-167686719f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, theta_prev, o, lr=4., n_steps=10, debug=False):\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    \n",
    "    # make p(s) from b\n",
    "    q = torch.nn.Softmax(dim=0)\n",
    "    p = torch.tensor(env.p_o_given_s[:,o]) * q(theta) # p(o|s)p(s)\n",
    "    log_p = torch.log(p)\n",
    "    \n",
    "    # initialize updated belief with current belief\n",
    "    theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "    \n",
    "    # estimate loss\n",
    "    def forward():\n",
    "        q1 = q(theta1)\n",
    "        # free energy: KL[ q(s) || p(s, o) ]\n",
    "        fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "        return fe\n",
    "    \n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    ll = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        l = forward()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1\n",
    "  \n",
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "def rollout_step(env, log_p_c, theta, pi, \n",
    "                 use_info_gain, use_pragmatic_value):\n",
    "    \n",
    "    if pi == []:\n",
    "        return []\n",
    "\n",
    "    a, pi_rest = pi[0], pi[1:]\n",
    "\n",
    "    # Where will I be after taking action a?\n",
    "    theta1 = update_belief_a(env, theta, a=a, lr=1.) \n",
    "    q = softmax(theta1)\n",
    "\n",
    "    # Do I like being there?\n",
    "    pragmatic = np.dot(q, log_p_c)\n",
    "\n",
    "    # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "    p_o = np.dot(q, env.p_o_given_s)\n",
    "\n",
    "    # Do I learn about s from by observing o?\n",
    "    # enumerate/ sample observations, update belief and estimate info gain\n",
    "    q_o = [softmax(update_belief(env, theta1, o=i)) for i in range(p_o.shape[0])]\n",
    "    d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "    info_gain = np.dot(p_o, d_o) # expected value of info gain\n",
    "\n",
    "    # negative expected free energy for this timestep\n",
    "    nefe = use_pragmatic_value * pragmatic + use_info_gain * info_gain\n",
    "\n",
    "    # nefe for remainder of policy rollout\n",
    "    nefe_rest = rollout_step(env, log_p_c, theta1, pi_rest, \n",
    "                        use_info_gain=use_info_gain, \n",
    "                        use_pragmatic_value=use_pragmatic_value)\n",
    "\n",
    "    # concatenate expected free energy across future time steps\n",
    "    return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b3a3a-17e8-4d3e-adf3-587603c010e0",
   "metadata": {},
   "source": [
    "We make the following modifications to the code. First, we replace enumeration of all possible plans (policies) with sampling, where we fetch the limits of continuous-valued actions from the environment and sample uniformly within this range; we take independent samples for each plan and each timestep up to the planning horizon k.\n",
    "\n",
    "```\n",
    "n_plans = 32\n",
    "plans = np.random.uniform(low=env.a_lims[0], high=env.a_lims[1], size=(n_plans, k))\n",
    "```\n",
    "\n",
    "For debugging purposes, we can nolonger compute the marginal probability of the first action. Instead, we would need to estimate its density from samples, e.g., by sampling with replacement from the evaluated policies with probability proportional to selecting each policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64ea10-6ac5-463d-9f9b-feb8ba90dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, theta_star, theta_start, \n",
    "                  n_plans=32, # number of plans to evaluate\n",
    "                  k=4, # planning horizon (number of sequential actions per plan)\n",
    "                  use_info_gain=True, \n",
    "                  use_pragmatic_value=True,\n",
    "                  select_max_pi=False, # replace sampling with best action selection\n",
    "                  debug=False, # return plans, p of selecting each\n",
    "                 ):\n",
    "    log_p_c = np.log(softmax(theta_star))\n",
    "\n",
    "    # sampling\n",
    "    plans = np.random.uniform(low=env.a_lims[0], high=env.a_lims[1], size=(n_plans, k)).tolist()\n",
    "\n",
    "    # evaluate negative expected free energy of all plans\n",
    "    nefes = []\n",
    "    for pi in plans:\n",
    "        step_nefes = rollout_step(env, log_p_c, theta_start, pi, \n",
    "                                  use_info_gain=use_info_gain, \n",
    "                                  use_pragmatic_value=use_pragmatic_value)\n",
    "        nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "        nefes.append(nefe)\n",
    "        \n",
    "    # compute probability of following each plan\n",
    "    p_pi = softmax(np.array(nefes)).tolist()  \n",
    "\n",
    "    if select_max_pi:\n",
    "        a = plans[np.argmax(nefes)]\n",
    "    else:\n",
    "        a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "    \n",
    "    if debug:\n",
    "        return a, plans, p_pi\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8084d69-106e-4dfc-a180-9f7b7e464d93",
   "metadata": {},
   "source": [
    "Let's explore action selection from plans with horizon $k$ by specifying sharp priors on the starting state and target state $k$ steps apart.\n",
    "\n",
    "If the starting state is to the right of the target (recall the state space wraps around), then policies that take a sequence of left actions ($a<0$)) are scored higher. Note that this holds true irrespective of the food source location. \n",
    "\n",
    "If the starting state is to the left of the target (e.g., $s_0=11$), then policies that take a sequence of right actions ($a>0$) are scored higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8f38c-e492-4f1b-872f-f67c1afebb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = 11\n",
    "target_state = 14\n",
    "\n",
    "import continuous_action_environment as cae\n",
    "env = cae.ContinuousActionEnv(N=16, # number of states\n",
    "                          s_food=8) # location of the food source\n",
    "\n",
    "# initialize belief\n",
    "theta_start = np.eye(env.s_N)[starting_state] * 10 # believe we are in state 1\n",
    "\n",
    "# initialize preference\n",
    "theta_star_shaped = 10 * np.ones(env.s_N) - np.abs(env.d_s[target_state])\n",
    "theta_star = np.eye(env.s_N)[target_state] * 10\n",
    "theta_star = theta_star_shaped\n",
    "\n",
    "a, plans, p_pi = select_action(env, theta_star, theta_start, k=2, n_plans=128, debug=True)\n",
    "\n",
    "# and explore what the agent prefers\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 12))\n",
    "plt.sca(ax[0])\n",
    "plt.bar(x = range(len(plans)), height=p_pi)\n",
    "plt.xlabel('plan id')\n",
    "plt.ylabel('$p(\\pi)$')\n",
    "\n",
    "#print('sum of actions, plans and associated probability of selecting them.')\n",
    "#for p, pi in zip(p_pi, plans):\n",
    "#    print(np.sum(pi), pi, p)\n",
    "\n",
    "# estimate marginal probability of selecting a plan with first action 0 or 1\n",
    "print('marginal probability of next action')\n",
    "a_sample = np.random.choice([pi[0] for pi in plans], p=p_pi, size=10000, replace=True)\n",
    "plt.sca(ax[1])\n",
    "plt.title('marginal probability of next action across all evaluated policies')\n",
    "# visualise empirical cumulative density function\n",
    "plt.hist(a_sample, density=True, bins=20);\n",
    "#plt.plot(np.linspace(0, 1, a_sample.shape[0]), np.sort(a_sample));\n",
    "#plt.plot([0,1],[-2,2])\n",
    "\n",
    "print('best plan')\n",
    "pi_max = np.argmax(p_pi)\n",
    "print(plans[pi_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947f430-5e27-45d7-9696-2e4ace5add41",
   "metadata": {},
   "source": [
    "## Preference distribution shaping\n",
    "\n",
    "Note that this sampling approach appears to only work well for very short time horizons ($k=2$) and returns a flat action posterior when it is difficult to find plans with which the target state can be reached within the planning horizon's number of steps. This could be mittigated by shaping the preference distribution or employing more clever rollouts of promising action sequences. The former can be done, for example, by also specifying a preference for states that are close to the target state, for which we can make use of the state distance computation we developed near the start of this notebook.\n",
    "\n",
    "Running the cell above with `14: theta_star = theta_star_shaped` helps find useful policies even when the target state cannot be reached from the start state within the finite time horizon.\n",
    "\n",
    "The cell below illustrates one way to shape the state preference distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2005abe-d6db-4711-ad4e-2eafb2f9256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state = 14\n",
    "\n",
    "theta_star = np.eye(env.s_N)[target_state] * 10\n",
    "theta_star_shaped = np.ones_like(theta_star) - np.abs(env.d_s[target_state])\n",
    "plt.plot(softmax(theta_star))\n",
    "plt.plot(softmax(theta_star_shaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2772870-b6bf-430e-b6ea-22a3a5a8b3c0",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we have all components required to implement an Active Infererence agent for environments with discrete state spaces, discrete observation spaces and _continuous_ action spaces. The changes to the minimal agent that interacts discrete action space environments turned out to be few and small.\n",
    "\n",
    "1. Updating belief through time required an interface change from accessing the environments full conditional probability table $p(s,a,s_1)$ to an action-specific table $p_a(s,s_1)$.\n",
    "\n",
    "2. Action selection can nolonger be performed by enumerating all possible finite horizon action sequences. Instead, we sample finite length sequences from the action space.\n",
    "\n",
    "Note that both of these changes could be ported back into the minimal agent to derive an interface that works for both discrete and continuous action spaces. But sampling discrete action sequences is far less efficient than enumerating all combinations.\n",
    "\n",
    "Let's encapsulate it into a class that manages the target state and current belief state over time and provides a minimal interface with reset and step methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d7ffa-1d52-4ccb-a253-9ca2d1068173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActionAgent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 shape_target=False, # smooth preference distribution using poirwise state distances\n",
    "                 n_plans=128, # number of plans rolled out during action selection\n",
    "                 k=2, # planning horizon\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_pi=False, # sample plan (False), select max negEFE (True).\n",
    "                 n_steps_o=20, # optimization steps after new observation\n",
    "                 n_steps_a=20, # optimization steps after new action\n",
    "                 lr_o=4., # learning rate of optimization after new observation\n",
    "                 lr_a=4.): # learning rate of optimization after new action)\n",
    "        \n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.shape_target = shape_target\n",
    "        self.n_plans = n_plans\n",
    "        self.k = k\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_pi = select_max_pi\n",
    "        self.n_steps_o = n_steps_o\n",
    "        self.n_steps_a = n_steps_a\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_o = lr_o\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        if self.shape_target:\n",
    "            self.b_star = np.ones(shape=self.env.s_N) - \\\n",
    "                          np.abs(self.env.d_s[self.target_state])\n",
    "        else:\n",
    "            self.b_star = np.eye(self.env.s_N)[self.target_state] * 10\n",
    "        self.log_p_c = np.log(softmax(self.b_star))\n",
    "        # initialize state prior as uniform\n",
    "        self.b = np.zeros(self.env.s_N)\n",
    "        \n",
    "    def step(self, o, debug=False):\n",
    "        if debug:\n",
    "            return self._step_debug(o)\n",
    "        \n",
    "        self.b = self._update_belief(theta_prev=self.b, o=int(o))\n",
    "        a = select_action(theta_start=self.b)[0] # pop first action of selected plan\n",
    "        self.b = self._update_belief_a(theta_prev=self.b, a=a)\n",
    "        return a\n",
    "    \n",
    "    def _step_debug(self, o):\n",
    "        self.b, ll_o = self._update_belief(theta_prev=self.b, \n",
    "                                           o=int(o), debug=True)\n",
    "        a, plans, p_pi = self._select_action(theta_start=self.b, debug=True)\n",
    "        max_a = plans[np.argmax(p_pi)][0]\n",
    "        a = a[0]\n",
    "        self.b, ll_a = self._update_belief_a(theta_prev=self.b, a=a, debug=True)\n",
    "        return a, ll_o, ll_a, max_a\n",
    "    \n",
    "    def _update_belief_a(self, theta_prev, a, debug=False):\n",
    "        # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "        theta = torch.tensor(theta_prev)\n",
    "        q = torch.nn.Softmax(dim=0)(theta)\n",
    "\n",
    "        # this is the prior for the distribution at time t\n",
    "        q1 = torch.matmul(q, torch.tensor(self.env.p_s1_given_s_a(a=a)))\n",
    "\n",
    "        # initialize parameters of updated belief to uniform\n",
    "        theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "        loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_a)\n",
    "        if debug:\n",
    "            ll = np.zeros(self.n_steps_a)\n",
    "\n",
    "        for i in range(self.n_steps_a):\n",
    "            l = loss(theta1, q1)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "    \n",
    "    def _update_belief(self, theta_prev, o, debug=False):\n",
    "        theta = torch.tensor(theta_prev)\n",
    "\n",
    "        # make p(s) from b\n",
    "        q = torch.nn.Softmax(dim=0)\n",
    "        p = torch.tensor(self.env.p_o_given_s[:,o]) * q(theta) # p(o|s)p(s)\n",
    "        log_p = torch.log(p)\n",
    "\n",
    "        # initialize updated belief with current belief\n",
    "        theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "\n",
    "        # estimate loss\n",
    "        def forward():\n",
    "            q1 = q(theta1)\n",
    "            # free energy: KL[ q(s) || p(s, o) ]\n",
    "            fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "            return fe\n",
    "\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_o)\n",
    "        ll = np.zeros(self.n_steps_o)\n",
    "        for i in range(self.n_steps_o):\n",
    "            l = forward()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "\n",
    "    def _select_action(self, theta_start, debug=False): # return plans, p of selecting each, and marginal p of actions\n",
    "        # sampling\n",
    "        a_lims = self.env.a_lims\n",
    "        plans = np.random.uniform(low=a_lims[0], high=a_lims[1], size=(self.n_plans, self.k)).tolist()\n",
    "        \n",
    "        # evaluate negative expected free energy of all plans\n",
    "        nefes = []\n",
    "        for pi in plans:\n",
    "            step_nefes = self._rollout_step(theta_start, pi)\n",
    "            nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "            nefes.append(nefe)\n",
    "\n",
    "        # compute probability of following each plan\n",
    "        p_pi = softmax(np.array(nefes)).tolist()\n",
    "        if self.select_max_pi:\n",
    "            a = plans[np.argmax(nefes)]\n",
    "        else:\n",
    "            a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            return a, plans, p_pi\n",
    "\n",
    "        return a\n",
    "\n",
    "    def _rollout_step(self, theta, pi):\n",
    "        if pi == []:\n",
    "            return []\n",
    "\n",
    "        a, pi_rest = pi[0], pi[1:]\n",
    "        # Where will I be after taking action a?\n",
    "        theta1 = self._update_belief_a(theta, a=a) \n",
    "        q = softmax(theta1)\n",
    "        # Do I like being there?\n",
    "        pragmatic = np.dot(q, self.log_p_c)\n",
    "        # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "        p_o = np.dot(q, self.env.p_o_given_s)\n",
    "        # Do I learn about s from by observing o?\n",
    "        # enumerate/ sample observations, update belief and estimate info gain\n",
    "        q_o = [softmax(self._update_belief(theta1, o=i)) for i in range(p_o.shape[0])]\n",
    "        d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "        info_gain = np.dot(p_o, d_o) # expected value of info gain\n",
    "        # negative expected free energy for this timestep\n",
    "        nefe = self.use_pragmatic_value * pragmatic + \\\n",
    "               self.use_info_gain * info_gain\n",
    "        # nefe for remainder of policy rollout\n",
    "        nefe_rest = self._rollout_step(theta1, pi_rest)\n",
    "        # concatenate expected free energy across future time steps\n",
    "        return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631dff3-05b1-4666-8189-59dfafdae084",
   "metadata": {},
   "source": [
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d33da3-a58f-4cb9-bc68-7ec077a2aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import continuous_action_environment as cae\n",
    "import continuous_action_agent as caa\n",
    "importlib.reload(cae)\n",
    "importlib.reload(caa)\n",
    "\n",
    "target_state = 4\n",
    "k = 4 # planning horizon; run time increases exponentially with planning horizon\n",
    "\n",
    "# runtime increases linearly with optimization steps during belief update\n",
    "n_steps_o = 10 # optimization steps updating belief after observation\n",
    "n_steps_a = 10 # optimization steps updating belief after action\n",
    "lr_o = 4. # learning rate updating belief after observation\n",
    "lr_a = 4. # learning rate updating belief after action\n",
    "\n",
    "render_losses = True\n",
    "\n",
    "env = cae.ContinuousActionEnv(N=16, # number of states\n",
    "                              s_food=0, # location of the food source\n",
    "                              s_0=0, \n",
    "                              o_decay=0.6) # starting location \n",
    "\n",
    "agent = caa.ContinuousActionAgent(env=env, \n",
    "                             target_state=target_state, \n",
    "                             shape_target=True,\n",
    "                             n_plans=128,\n",
    "                             k=k, \n",
    "                             use_info_gain=True,\n",
    "                             use_pragmatic_value=True,\n",
    "                             select_max_pi=True,\n",
    "                             n_steps_o=n_steps_o, \n",
    "                             n_steps_a=n_steps_a, \n",
    "                             lr_a=lr_a, \n",
    "                             lr_o=lr_o)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "bb = [agent.b]\n",
    "aa = []\n",
    "if render_losses:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].set_title('updates from actions')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('optimization step')\n",
    "    ax[1].set_title('updates from observations')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('optimization step')\n",
    "    \n",
    "for i in range(64):\n",
    "    a, ll_o, ll_a, max_a = agent.step(o, debug=True)\n",
    "    print(f\"step {i}, s: {env.s_t}, o: {['FOOD', 'NONE'][int(o)]}, top a: {max_a}, a: {a}\")\n",
    "    if render_losses:\n",
    "        ax[0].plot(ll_a)\n",
    "        ax[1].plot(ll_o)\n",
    "    \n",
    "    o = env.step(a)\n",
    "    \n",
    "    ss.append(env.s_t)\n",
    "    bb.append(agent.b)\n",
    "    aa.append(a)\n",
    "\n",
    "\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(bb).T, label='belief')\n",
    "\n",
    "for i in range(len(aa)):\n",
    "  plt.plot([i, i], [ss[i], ss[i]+aa[i]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] < 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "\n",
    "\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9406c15-0f22-4628-8b87-33cc4612e671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
