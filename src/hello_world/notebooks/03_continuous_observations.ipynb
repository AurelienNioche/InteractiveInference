{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96a968-008c-49b4-876f-658573340145",
   "metadata": {},
   "source": [
    "# Continuous Observation Spaces\n",
    "\n",
    "In this notebook we develop the active inference agent for environments with continuous observation spaces. \n",
    "\n",
    "We start by modifying the minimal environment to emit continous-valued observations that represent the fraction of repeated experiments yielding food in a given state. Then, we modify the components of the minimal agent that currently exploit the discreteness of the observation space, namely the belief-update after a new observation occurred and the estimation of information gain during action selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec47cee-3ffb-47ca-a7f5-2ab72d8b1e8a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d2f20-c022-4a7f-9c50-96e405791052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50976152-1ae4-4cc2-8ea6-60bbe66c20e2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38e41e-bdaa-4a95-9ed9-cdf6c8398e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bcff9-8e53-4094-a34c-a20c3156fffd",
   "metadata": {},
   "source": [
    "# Continuous Observation Environment\n",
    "\n",
    "So far, the environment emitted binary observations `NO_FOOD` or `FOOD` with a probability that decayed exponentially with the distance of a state from the food source. To make observations continuous we replace this binary observation with samples from the _Beta_distribution with mean equal to the probability of observing `FOOD` , which represent (roughly) the proportion of positive observations in a finite set of repeated coin flips. The `sample_size` governs the number of repeated experiments with variance decreasing with increasing sample size. In the limit of inifinite observations, the _Beta_ distribution is zero everywhere except at the mean.\n",
    "\n",
    "## The Beta distribution\n",
    "\n",
    "Let's explore the density of the _Beta_ distribution with varying sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51065cf5-5208-4c9e-ba75-2e705ec43f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.3\n",
    "params = [(mean, 3), (mean, 10), (mean, 30), (mean, 200)]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.sca(ax)\n",
    "for mean, sample_size in params:\n",
    "  \n",
    "  x = np.linspace(0, 1, 1000)\n",
    "  p = beta.pdf(x, a=mean * sample_size, b=(1-mean) * sample_size)\n",
    "  plt.plot(x, p, label=f'mean:{mean}, sample size:{sample_size}')\n",
    "  \n",
    "plt.legend()\n",
    "plt.title('Density of the Beta distribution with varying sample size')\n",
    "plt.ylabel('density')\n",
    "plt.xlabel('observation o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248b490-fe5a-4dc2-919e-f6ab51fc1c39",
   "metadata": {},
   "source": [
    "## From discrete to continuous observations\n",
    "We can now define, sample from, and visualise the distribution of observations generated in each state of the environment, where observations are sampled from the _Beta_ distribution with mean equal to the probability of observing food in a single coin flip experiment as in the `MinimalEnvironment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadac5c6-6bf2-4527-a975-c8fb474ccb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minimal_environment as me\n",
    "importlib.reload(me)\n",
    "\n",
    "sample_size=10\n",
    "n_samples = 1000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.sca(ax)\n",
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_decay=0.2) # decay of observing food away from source\n",
    "\n",
    "def emission_probability(sample_size):\n",
    "  means = env.emission_probability()[:,1]\n",
    "  return [beta(a=m*sample_size, b=(1-m)*sample_size) for m in means]\n",
    "\n",
    "def sample_o(p_o_given_s, s, n_samples):\n",
    "  return p_o_given_s[s].rvs(size=n_samples)\n",
    "  \n",
    "p_o_given_s = emission_probability(sample_size)\n",
    "samples = []\n",
    "for s in range(env.s_N):\n",
    "  oo = sample_o(p_o_given_s, s, n_samples)\n",
    "  samples.append(oo)\n",
    "  \n",
    "df = pd.DataFrame(np.array(samples).T)\n",
    "sns.violinplot(df, cut=0, width=2)\n",
    "plt.xlabel('state s')\n",
    "plt.ylabel('p(o|s)')\n",
    "plt.title('Continuous environment emission probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59658105-c604-4899-9e79-a5eee9148f84",
   "metadata": {},
   "source": [
    "## Full environment specification\n",
    "\n",
    "We modify the code of `MinimalEnv` as follows\n",
    "- `emission_probability(sample_size)` returns a list of beta distributions, one for each state.\n",
    "- `p_o_given_s` stores a copy of this list, which is computed once at initialization.\n",
    "- `sample_o` samples from the beta distribution of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0cd66-a6b3-4587-9f68-a1480e28ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "# environment\n",
    "class ContinuousObservationEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (float): proportion of times food detected in finite sample.\n",
    "  actions(int): {-1, 1} intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 0, # where is the food?\n",
    "               p_move = 0.75, # execute intent with p, else don't move.\n",
    "               o_sample_size=10, # observation Beta distribution parameter.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.p_move = p_move\n",
    "    self.o_sample_size = o_sample_size\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.a_N = 2 # {0, 1} to move left/ right in wrap-around 1D state-space\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    although we only need to be able to sample from these distributions to \n",
    "    implement the environment, we pre-compute the full conditional probability\n",
    "    table (1.) and conditional emission random variables (2.) here so agents \n",
    "    can access the true dynamics if required.\n",
    "    \"\"\"\n",
    "    self.p_s1_given_s_a = self.transition_dynamics() # Matrix B\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "\n",
    "  def transition_dynamics(self):\n",
    "    \"\"\" computes transition probability p(s'| s, a) \n",
    "    \n",
    "    Returns:\n",
    "    p[s, a, s1] of size (s_N, a_N, s_N)\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.zeros((self.s_N, self.a_N, self.s_N))\n",
    "    p[:,0,:] = self.p_move * np.roll(np.identity(self.s_N), -1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    p[:,1,:] = self.p_move * np.roll(np.identity(self.s_N), 1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    return p\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" initialises conditional random variables p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s] of size (s_N) with one scipy.stats.rv_continuous per state\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.minimum(\n",
    "                   np.abs(s - self.s_N - self.s_food), \n",
    "                   np.abs(s + self.s_N - self.s_food)))\n",
    "  \n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    mean = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    # continuous relaxation: proportion of food detected in finite sample\n",
    "    sample_size = self.o_sample_size\n",
    "    return [beta(a=m*sample_size, b=(1-m)*sample_size) for m in mean]\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "\n",
    "    if (a not in [0, 1]):\n",
    "      print(\"Warning: only permitted actions are [0, 1].\")\n",
    "\n",
    "    # convert action index to action\n",
    "    a = [-1,1][a]\n",
    "\n",
    "    if np.random.random() < self.p_move:\n",
    "      self.s_t = (self.s_t + a) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return self.p_o_given_s[self.s_t].rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd478b-8db8-42a5-8d31-d0cdb6968e86",
   "metadata": {},
   "source": [
    "## Random Agent Behavior\n",
    "\n",
    "To test the environment we simulate a random agent's interactions with it. Here, the random agent samples actions uniformly in the interval `[-2, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc74e52-4750-4453-b89f-aa81ba258df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import continuous_observation_environment as coe\n",
    "importlib.reload(coe)\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_sample_size=100) # variance of observation decreases with increasing sample size.\n",
    "\n",
    "n_steps = 100\n",
    "ss, oo, aa = [], [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "oo.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26b146-6b07-482c-b6da-2b65beceada1",
   "metadata": {},
   "source": [
    "We inspect the sequence of states, actions and emissions during this interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2939d14-d933-46ba-9390-453a7b734a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "for i in range(len(aa)):\n",
    "  ax[0].plot([i, i], [ss[i], ss[i]+[-1,1][aa[i]]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] > 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "  \n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('state s')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(oo))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95dcd3-3f3d-4640-aa3f-89b1cfe4e717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
