{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96a968-008c-49b4-876f-658573340145",
   "metadata": {},
   "source": [
    "# Continuous Observation Spaces\n",
    "\n",
    "In this notebook we develop the active inference agent for environments with continuous observation spaces. \n",
    "\n",
    "We start by modifying the minimal environment to emit continous-valued observations that represent the proportion of repeated experiments yielding food in a given state. Then, we modify the components of the minimal agent that currently exploit the discreteness of the observation space, namely the belief-update after a new observation occurred and the estimation of information gain during action selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec47cee-3ffb-47ca-a7f5-2ab72d8b1e8a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d2f20-c022-4a7f-9c50-96e405791052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50976152-1ae4-4cc2-8ea6-60bbe66c20e2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38e41e-bdaa-4a95-9ed9-cdf6c8398e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bcff9-8e53-4094-a34c-a20c3156fffd",
   "metadata": {},
   "source": [
    "# Continuous Observation Environment\n",
    "\n",
    "So far, the environment emitted binary observations `NO_FOOD` or `FOOD` with a probability that decayed exponentially with the distance of a state from the food source. To make observations continuous we replace this binary observation with samples from the _Beta_distribution with mean equal to the probability of observing `FOOD` , which represent (roughly) the proportion of positive observations in a finite set of repeated coin flips. The `sample_size` governs the number of repeated experiments with variance decreasing with increasing sample size. In the limit of inifinite observations, the _Beta_ distribution is zero everywhere except at the mean.\n",
    "\n",
    "## The Beta distribution\n",
    "\n",
    "Let's explore the density of the _Beta_ distribution with varying sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51065cf5-5208-4c9e-ba75-2e705ec43f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.3\n",
    "params = [(mean, 3), (mean, 10), (mean, 30), (mean, 200)]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.sca(ax)\n",
    "for mean, sample_size in params:\n",
    "  \n",
    "  x = np.linspace(0, 1, 1000)\n",
    "  p = beta.pdf(x, a=mean * sample_size, b=(1-mean) * sample_size)\n",
    "  plt.plot(x, p, label=f'mean:{mean}, sample size:{sample_size}')\n",
    "  \n",
    "plt.legend()\n",
    "plt.title('Density of the Beta distribution with varying sample size')\n",
    "plt.ylabel('density')\n",
    "plt.xlabel('observation o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248b490-fe5a-4dc2-919e-f6ab51fc1c39",
   "metadata": {},
   "source": [
    "## From discrete to continuous observations\n",
    "We can now define, sample from, and visualise the distribution of observations generated in each state of the environment, where observations are sampled from the _Beta_ distribution with mean equal to the probability of observing food in a single coin flip experiment as in the `MinimalEnvironment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadac5c6-6bf2-4527-a975-c8fb474ccb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minimal_environment as me\n",
    "importlib.reload(me)\n",
    "\n",
    "sample_size=10\n",
    "n_samples = 1000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.sca(ax)\n",
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_decay=0.2) # decay of observing food away from source\n",
    "\n",
    "def emission_probability(sample_size):\n",
    "  means = env.emission_probability()[:,1]\n",
    "  return [beta(a=m*sample_size, b=(1-m)*sample_size) for m in means]\n",
    "\n",
    "def sample_o(p_o_given_s, s, n_samples):\n",
    "  return p_o_given_s[s].rvs(size=n_samples)\n",
    "  \n",
    "p_o_given_s = emission_probability(sample_size)\n",
    "samples = [sample_o(p_o_given_s, s, n_samples) for s in range(env.s_N)]\n",
    "df = pd.DataFrame(np.array(samples).T)\n",
    "sns.violinplot(df, cut=0, width=2)\n",
    "plt.xlabel('state s')\n",
    "plt.ylabel('p(o|s)')\n",
    "plt.title('Continuous environment emission probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59658105-c604-4899-9e79-a5eee9148f84",
   "metadata": {},
   "source": [
    "## Full environment specification\n",
    "\n",
    "We modify the code of `MinimalEnv` as follows\n",
    "- `emission_probability(sample_size)` returns a list of beta distributions, one for each state.\n",
    "- `p_o_given_s` stores a copy of this list, which is computed once at initialization.\n",
    "- `sample_o` samples from the beta distribution of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0cd66-a6b3-4587-9f68-a1480e28ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "# environment\n",
    "class ContinuousObservationEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (float): proportion of times food detected in finite sample.\n",
    "  actions(int): {-1, 1} intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 0, # where is the food?\n",
    "               p_move = 0.75, # execute intent with p, else don't move.\n",
    "               o_sample_size=10, # observation Beta distribution parameter.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.p_move = p_move\n",
    "    self.o_sample_size = o_sample_size\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.a_N = 2 # {0, 1} to move left/ right in wrap-around 1D state-space\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    although we only need to be able to sample from these distributions to \n",
    "    implement the environment, we pre-compute the full conditional probability\n",
    "    table (1.) and conditional emission random variables (2.) here so agents \n",
    "    can access the true dynamics if required.\n",
    "    \"\"\"\n",
    "    self.p_s1_given_s_a = self.transition_dynamics() # Matrix B\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "\n",
    "  def transition_dynamics(self):\n",
    "    \"\"\" computes transition probability p(s'| s, a) \n",
    "    \n",
    "    Returns:\n",
    "    p[s, a, s1] of size (s_N, a_N, s_N)\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.zeros((self.s_N, self.a_N, self.s_N))\n",
    "    p[:,0,:] = self.p_move * np.roll(np.identity(self.s_N), -1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    p[:,1,:] = self.p_move * np.roll(np.identity(self.s_N), 1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    return p\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" initialises conditional random variables p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s] of size (s_N) with one scipy.stats.rv_continuous per state\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.minimum(\n",
    "                   np.abs(s - self.s_N - self.s_food), \n",
    "                   np.abs(s + self.s_N - self.s_food)))\n",
    "  \n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    mean = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    # continuous relaxation: proportion of food detected in finite sample\n",
    "    sample_size = self.o_sample_size\n",
    "    return np.array([beta(a=m*sample_size, b=(1-m)*sample_size) for m in mean])\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "\n",
    "    if (a not in [0, 1]):\n",
    "      print(\"Warning: only permitted actions are [0, 1].\")\n",
    "\n",
    "    # convert action index to action\n",
    "    a = [-1,1][a]\n",
    "\n",
    "    if np.random.random() < self.p_move:\n",
    "      self.s_t = (self.s_t + a) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return self.p_o_given_s[self.s_t].rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd478b-8db8-42a5-8d31-d0cdb6968e86",
   "metadata": {},
   "source": [
    "## Random Agent Behavior\n",
    "\n",
    "To test the environment we simulate a random agent's interactions with it. Here, the random agent samples actions uniformly in the interval `[-2, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc74e52-4750-4453-b89f-aa81ba258df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import continuous_observation_environment as coe\n",
    "importlib.reload(coe)\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_sample_size=100) # variance of observation decreases with increasing sample size.\n",
    "\n",
    "n_steps = 100\n",
    "ss, oo, aa = [], [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "oo.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26b146-6b07-482c-b6da-2b65beceada1",
   "metadata": {},
   "source": [
    "We inspect the sequence of states, actions and emissions during this interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2939d14-d933-46ba-9390-453a7b734a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "for i in range(len(aa)):\n",
    "  ax[0].plot([i, i], [ss[i], ss[i]+[-1,1][aa[i]]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] > 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "  \n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('state s')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(oo))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bae0c-a050-4a49-8e7e-fd62c245aa65",
   "metadata": {},
   "source": [
    "# Continuous Observation Agent\n",
    "\n",
    "We start implementing the continuous observation agent by updating its belief in light of a new observation.\n",
    "\n",
    "## Update based on new observation\n",
    "\n",
    "The belief update based on new observations involves minimizing the KL-divergence between $Q(s;\\theta')$ and $p(o, s) = Q(s; \\theta) p(o|s)$ with respect to $\\theta'$. The new observation is used to compute the joint probability $p(o, s)$ for each discrete state $s$, for which we need to estimate the probability density at $o$ in each state $s$.\n",
    "\n",
    "```\n",
    "p_o_given_s = np.array([p.pdf(o) for p in env.p_o_given_s])\n",
    "p = torch.tensor(p_o_given_s * q(theta)) # p(o|s)p(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb356565-798d-468e-8a70-e1734a6f746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, theta_prev, o, lr=4., n_steps=10, debug=False):\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    \n",
    "    # make p(s) from b\n",
    "    q = torch.nn.Softmax(dim=0)\n",
    "    p_o_given_s = torch.tensor([p.pdf(o) for p in env.p_o_given_s])\n",
    "    p = p_o_given_s * q(theta) # p(o|s)p(s)\n",
    "    log_p = torch.log(p)\n",
    "    \n",
    "    # initialize updated belief with current belief\n",
    "    theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "    \n",
    "    # estimate loss\n",
    "    def forward():\n",
    "        q1 = q(theta1)\n",
    "        # free energy: KL[ q(s) || p(s, o) ]\n",
    "        fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "        return fe\n",
    "    \n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    ll = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        l = forward()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c96de-d7d9-43f9-8a5a-157a781a2bd3",
   "metadata": {},
   "source": [
    "Let's see the effect of this in action. We start with a uniform prior belief. Recall that the food source is in state 0 and that the probability of observing food decreases exponentially with the distance of a state from the food source, with the state space wrapping around.\n",
    "\n",
    "If we observed infrequent food ($o=0.15$), then it is most likely that we are in the state furthest away from the food source. If we observed frequent food ($o=0.95$), then it is most likely that we are at the food source. If we observed food half the time ($o=0.65$), then we infer a bi-modal distribution with two most likely states at equal distance on either side of the food source and a lot of uncertainty about the exact state. This uncertainty decreases as we increase the `sample_size` of the environment to, e.g., 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a1aff-f3a8-4084-823c-2edfec8ae938",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 0.15\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=8,s_food=0, o_sample_size=10)\n",
    "theta = np.zeros(env.s_N)\n",
    "theta1, ll = update_belief(env, theta, o=o, lr=2., n_steps=20, debug=True)\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(ll)\n",
    "plt.plot([0, ll.shape[0]-1], [ll.min()]*2, 'k--')\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta1), alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Updating beliefs in light of a new observation.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e266126-f36e-4981-acb6-de2380c4b674",
   "metadata": {},
   "source": [
    "## Information gain for action selection\n",
    "\n",
    "Now we update the estimation of information gain during action selection. Before we start we copy some code unchanged from the previous notebook, because action selection requires belief updating in light of new observations and rollouts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887cf0d-802c-4d68-9742-bc493668b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "  \n",
    "def update_belief_a(env, theta_prev, a, lr=4., n_steps=10, debug=False):\n",
    "    # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    q = torch.nn.Softmax(dim=0)(theta)\n",
    "    \n",
    "    # this is the prior for the distribution at time t\n",
    "    # if we worked on this level, we would be done. \n",
    "    # but we need to determine the parameters of Q that produce \n",
    "    # this distribution\n",
    "    q1 = torch.matmul(q, torch.tensor(env.p_s1_given_s_a[:,a,:]))\n",
    "\n",
    "    # initialize updated belief to uniform\n",
    "    theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "    loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    if debug:\n",
    "        ll = np.zeros(n_steps)\n",
    "        \n",
    "    for i in range(n_steps):\n",
    "        l = loss(theta1, q1)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c92647-0cef-48fe-b03d-6b300190adaa",
   "metadata": {},
   "source": [
    "Recall that the _information gain_ term quantifies the belief update due to making observations in future states.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[ D_{KL}(\\, Q_{\\theta'}(s|o),  Q_{\\theta}(s) \\,) \\right]$\n",
    "\n",
    "Because observations were discrete in the minimal environment, we were able to enumerate all possible observations, compute the belief update after making each observation and weigh the KL divergence by the probability of generating each observation given our current belief about the state distribution.\n",
    "\n",
    "Because observations are now continuous, we need to resort to sampling and Monte Carlo approximation of the information gain. For a finite set of samples, we \n",
    "- generate observations by first sampling a state $s\\sim Q_{\\theta}$ and then sampling an observation  $o \\sim p(o|s)$\n",
    "- perform one belief update based on each sampled observation\n",
    "- compute the KL between the discrete belief over states before and after each belief update\n",
    "- compute the mean of KL across samples.\n",
    "\n",
    "Note that while previously we performed one belief update per possible observation, i.e. 2 belief updates, we now perform one belief update in each rollout step per sampled observation, e.g., 100. This massively increases the computation time, because all belief updates are performed in sequence. Future work should explore how this could be vectorized. In order to explore how the estimate of info_gain changes with the number of sampled observations, we provide as optional debug output the list of information gain estimates per observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3700d-f415-4299-b162-024c48c8fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_step(env, log_p_c, theta, pi,\n",
    "                 n_samples, use_info_gain, use_pragmatic_value, debug=False):\n",
    "    \n",
    "    if pi == []:\n",
    "        return []\n",
    "\n",
    "    a, pi_rest = pi[0], pi[1:]\n",
    "\n",
    "    # Where will I be after taking action a?\n",
    "    theta1 = update_belief_a(env, theta, a=a, lr=1.) \n",
    "    q = softmax(theta1)\n",
    "\n",
    "    # Do I like being there?\n",
    "    pragmatic = np.dot(q, log_p_c)\n",
    "\n",
    "    # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "    ss = np.random.choice(range(env.s_N), p=q, size=n_samples)\n",
    "    oo = [rv.rvs() for rv in env.p_o_given_s[ss]]\n",
    "    # Do I learn about s from observing o?\n",
    "    q_o = [softmax(update_belief(env, theta1, o=o)) for o in oo]\n",
    "    d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "    info_gain = np.mean(d_o) # expected value of info gain\n",
    "\n",
    "    # negative expected free energy for this timestep\n",
    "    nefe = use_pragmatic_value * pragmatic + use_info_gain * info_gain\n",
    "\n",
    "    # nefe for remainder of policy rollout\n",
    "    nefe_rest = rollout_step(env, log_p_c, theta1, pi_rest, \n",
    "                        n_samples=n_samples,\n",
    "                        use_info_gain=use_info_gain, \n",
    "                        use_pragmatic_value=use_pragmatic_value, debug=False)\n",
    "\n",
    "    # concatenate expected free energy across future time steps\n",
    "    if debug:\n",
    "      return [nefe] + nefe_rest, d_o\n",
    "    \n",
    "    return [nefe] + nefe_rest\n",
    "\n",
    "def select_action(env, theta_star, theta_start, \n",
    "                  k=4, # planning horizon (number of sequential actions per plan)\n",
    "                  n_samples=100,\n",
    "                  use_info_gain=True, \n",
    "                  use_pragmatic_value=True,\n",
    "                  select_max_pi=False, # replace sampling with best action selection\n",
    "                  debug=False, # return plans, p of selecting each, and marginal p of actions\n",
    "                 ):\n",
    "    log_p_c = np.log(softmax(theta_star))\n",
    "\n",
    "    # genrate all plans\n",
    "    plans = [ list(x) for x in itertools.product(range(env.a_N), repeat=k)]\n",
    "\n",
    "    # evaluate negative expected free energy of all plans\n",
    "    nefes = []\n",
    "    for pi in plans:\n",
    "      if debug:\n",
    "        step_nefes, info_gains = rollout_step(env, log_p_c, theta_start, pi, \n",
    "                                  n_samples=n_samples,\n",
    "                                  use_info_gain=use_info_gain, \n",
    "                                  use_pragmatic_value=use_pragmatic_value,\n",
    "                                  debug=True)\n",
    "      else:\n",
    "        step_nefes = rollout_step(env, log_p_c, theta_start, pi, \n",
    "                                  n_samples=n_samples,\n",
    "                                  use_info_gain=use_info_gain, \n",
    "                                  use_pragmatic_value=use_pragmatic_value)\n",
    "        \n",
    "      nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "      nefes.append(nefe)\n",
    "        \n",
    "    # compute probability of following each plan\n",
    "    p_pi = softmax(np.array(nefes)).tolist()  \n",
    "\n",
    "    if select_max_pi:\n",
    "        a = plans[np.argmax(nefes)]\n",
    "    else:\n",
    "        a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "    \n",
    "    if debug:\n",
    "        p_a = np.zeros(env.a_N)\n",
    "        for p, pi in zip(p_pi, plans):\n",
    "            p_a[pi[0]] += p\n",
    "            \n",
    "        return a, p_a, plans, p_pi, info_gains\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec5818-0508-498d-8eaa-95723cc21f02",
   "metadata": {},
   "source": [
    "Let's explore action selection from plans with horizon $k$ by specifying sharp priors on the starting state and target state $k-1$ steps apart.\n",
    "\n",
    "If the starting state is to the right of the target (recall the state space wraps around), then policies that take a sequence of left actions ($a=0$)) are scored higher. Note that this holds true irrespective of the food source location. \n",
    "\n",
    "If the starting state is to the left of the target (e.g., $s_0=11$), then policies that take a sequence of right actions ($a=1$) are scored higher.\n",
    "\n",
    "If the starting state and the target state coincide, then policies that take equal numbers of left and right actions are scored highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31852ae6-ac58-4a2f-b330-57895044c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = 11\n",
    "target_state = 14\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=16,s_food=0, o_sample_size=10)\n",
    "\n",
    "# initialize belief\n",
    "theta_start = np.eye(env.s_N)[starting_state] * 10 # believe we are in state 1\n",
    "\n",
    "# initialize preference\n",
    "theta_star = np.eye(env.s_N)[target_state] * 10\n",
    "\n",
    "a, p_a, plans, p_pi, info_gains = select_action(env, theta_star, theta_start, debug=True, n_samples=100)\n",
    "\n",
    "# and explore what the agent prefers\n",
    "plt.bar(x = range(len(plans)), height=p_pi)\n",
    "plt.xlabel('plan id')\n",
    "plt.ylabel('$p(\\pi)$')\n",
    "\n",
    "print('plans and associated probability of selecting them.')\n",
    "for p, pi in zip(p_pi, plans):\n",
    "    print(pi, p)\n",
    "\n",
    "# estimate marginal probability of selecting a plan with first action 0 or 1\n",
    "print('marginal probability of next action')\n",
    "print(p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e6ec0-8b7d-49e8-b372-3d02fde2d02a",
   "metadata": {},
   "source": [
    "## How many observation samples do we need?\n",
    "\n",
    "Since the action selection runtime increases linearly with the number of observation samples used to approximate information gain, it would be useful to quantify the effect of sample size on downstream tasks.\n",
    "\n",
    "### Uncertainty in information gain estimates\n",
    "\n",
    "First, it is useful to explore how the uncertainty in the information gain estimate decreases with an increased number of observation samples. This lets us make an informed decision about the tradeoff between runtime and information gain estimation precision. In order to estimate this we can sample information gain from a large number of sampled observations and use bootsrap estimates of the information gain with varying number of samples to quantify uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bc4eb-799f-4eed-8956-57de1dd06f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore how the estimate of info gain changes with increasing number of samples\n",
    "print('number of info gain estimates', len(info_gains))\n",
    "n_bootstrap = 100 # bootstrap samples of observation subsets\n",
    "n_oo = [10, 20, 50, 100]\n",
    "for n_o in n_oo:\n",
    "  ig = np.random.choice(info_gains, size=(n_bootstrap, n_o), replace=True).mean(axis=1)\n",
    "  plt.hist(ig, density=True, label=f'n={n_o}', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('Bootstrap estimate of information gain with varying number of sampled observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3956e76-e4b7-4785-bd0b-bc3f5b0a18f9",
   "metadata": {},
   "source": [
    "### Uncertainty in action selection\n",
    "\n",
    "Ultimately, though, the estimate of information gain is only relevant to the extent that it influences action selection. If this estimate was very uncertain or noisy due of a low number of observation samples but the probability of selecting each plan is virtually the same across repetitions, then the noise would be irrelevant for all practical purposes. So, another way to explore this tradeoff that is closer to what matters ultimately, action selection, is to explore how policies are scored differently with and without information gain estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86134bc2-e3cf-4636-882b-35514e94ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # number of observation samples in rollout used for info gain\n",
    "n_trials = 10 # number of repeated estimates of plan probabilities with info gain\n",
    "starting_state = 11\n",
    "target_state = 14\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=16,s_food=0, o_sample_size=10)\n",
    "theta_start = np.eye(env.s_N)[starting_state] * 10 # initialize belief\n",
    "theta_star = np.eye(env.s_N)[target_state] * 10 # initialize preference\n",
    "pp_pi = []\n",
    "pp_a = []\n",
    "for _ in range(n_trials):\n",
    "  _, p_a, _, p_pi, _ = select_action(env, theta_star, theta_start, debug=True, \n",
    "                                   n_samples=n_samples)\n",
    "  pp_a.append(p_a)\n",
    "  pp_pi.append(p_pi)\n",
    "\n",
    "a1, p_a1, plans1, p_pi1, info_gain1 = select_action(env, \n",
    "                                                    theta_star, \n",
    "                                                    theta_start, \n",
    "                                                    use_info_gain=False,\n",
    "                                                    debug=True, \n",
    "                                                    n_samples=1)\n",
    "\n",
    "pp_pi = np.array(pp_pi)\n",
    "pp_a = np.array(pp_a)\n",
    "mean_pi, std_pi = pp_pi.mean(axis=0), pp_pi.std(axis=0)\n",
    "mean_a, std_a = pp_a.mean(axis=0), pp_a.std(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 2*6))\n",
    "plt.sca(ax[0])\n",
    "plt.errorbar(x=range(len(p_pi)), y=mean_pi, yerr=std_pi, color='red', label='with info gain')\n",
    "plt.bar(range(len(p_pi)), height= p_pi1, color='blue', label='without info gain')\n",
    "plt.legend()\n",
    "plt.title('Probabilty of selecting plans with noisy information gain.')\n",
    "\n",
    "print('plans and associated probability of selecting them.')\n",
    "for p, pi in zip(p_pi1, plans):\n",
    "    print(pi, p)\n",
    "    \n",
    "plt.sca(ax[1])\n",
    "plt.errorbar(x=range(len(p_a)), y=mean_a, yerr=std_a, color='red', label='with info gain')\n",
    "plt.bar(range(len(p_a)), height= p_a1, color='blue', label='without info gain')\n",
    "plt.legend()\n",
    "plt.title('Probabilty of selecting next actions with noisy information gain.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c56f62-fc09-4d54-9f84-ece92a23d647",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we have all components required to implement an Active Infererence agent for environments with discrete state spaces, discrete action spaces and _continuous_ observation spaces. The changes to the minimal agent that interacts with discrete observation space environments, again, turned out to be few and small.\n",
    "\n",
    "1. Updating belief based on new observations required an interface change from accessing the environments conditional probability table $p[s,o]$ to state specific random variables that let us evaluate the likelihood.\n",
    "\n",
    "2. Information gain during action selection can nolonger be performed by enumerating all possible discrete observations. Instead, we sample a finite set of observations from the observation space based on our current belief distribution over states.\n",
    "\n",
    "Note that both of these changes could be ported back into the minimal agent to derive an interface that works for both discrete and continuous observation spaces. But sampling observations is far less efficient than enumerating all possibly observations and making use of their state-condtional probabilities.\n",
    "\n",
    "Let's encapsulate these changes into an agent class that, as the other agents before it, manages the target state and current belief state over time and provides a minimal interface with reset and step methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484932b-fcb3-4e75-8cb2-fb4cceae301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "class ContinuousObservationAgent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, # planning horizon\n",
    "                 n_o_samples=10, # observation samples for information gain\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_pi=False, # sample plan (False), select max negEFE (True).\n",
    "                 n_steps_o=20, # optimization steps after new observation\n",
    "                 n_steps_a=20, # optimization steps after new action\n",
    "                 lr_o=4., # learning rate of optimization after new observation\n",
    "                 lr_a=4.): # learning rate of optimization after new action)\n",
    "        \n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.n_o_samples = n_o_samples\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_pi = select_max_pi\n",
    "        self.n_steps_o = n_steps_o\n",
    "        self.n_steps_a = n_steps_a\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_o = lr_o\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        self.b_star = np.eye(self.env.s_N)[self.target_state] * 10\n",
    "        self.log_p_c = np.log(softmax(self.b_star))\n",
    "        # initialize state prior as uniform\n",
    "        self.b = np.zeros(self.env.s_N)\n",
    "        \n",
    "    def step(self, o, debug=False):\n",
    "        if debug:\n",
    "            return self._step_debug(o)\n",
    "        \n",
    "        self.b = self._update_belief(theta_prev=self.b, o=o)\n",
    "        a = select_action(theta_start=self.b)[0] # pop first action of selected plan\n",
    "        self.b = self._update_belief_a(theta_prev=self.b, a=a)\n",
    "        return a\n",
    "    \n",
    "    def _step_debug(self, o):\n",
    "        self.b, ll_o = self._update_belief(theta_prev=self.b, o=o, debug=True)\n",
    "        a, p_a, _, _, _ = self._select_action(theta_start=self.b, debug=True)\n",
    "        a = a[0]\n",
    "        self.b, ll_a = self._update_belief_a(theta_prev=self.b, a=a, debug=True)\n",
    "        return a, ll_o, ll_a, p_a\n",
    "    \n",
    "    def _update_belief_a(self, theta_prev, a, debug=False):\n",
    "        # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "        theta = torch.tensor(theta_prev)\n",
    "        q = torch.nn.Softmax(dim=0)(theta)\n",
    "\n",
    "        # this is the prior for the distribution at time t\n",
    "        q1 = torch.matmul(q, torch.tensor(self.env.p_s1_given_s_a[:,a,:]))\n",
    "\n",
    "        # initialize parameters of updated belief to uniform\n",
    "        theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "        loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_a)\n",
    "        if debug:\n",
    "            ll = np.zeros(self.n_steps_a)\n",
    "\n",
    "        for i in range(self.n_steps_a):\n",
    "            l = loss(theta1, q1)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "    \n",
    "    def _update_belief(self, theta_prev, o, debug=False):\n",
    "        theta = torch.tensor(theta_prev)\n",
    "\n",
    "        # make p(s) from b\n",
    "        q = torch.nn.Softmax(dim=0)\n",
    "        p_o_given_s = torch.tensor([p.pdf(o) for p in self.env.p_o_given_s])\n",
    "        p = p_o_given_s * q(theta) # p(o|s)p(s)\n",
    "        log_p = torch.log(p)\n",
    "\n",
    "        # initialize updated belief with current belief\n",
    "        theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "\n",
    "        # estimate loss\n",
    "        def forward():\n",
    "            q1 = q(theta1)\n",
    "            # free energy: KL[ q(s) || p(s, o) ]\n",
    "            fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "            return fe\n",
    "\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_o)\n",
    "        ll = np.zeros(self.n_steps_o)\n",
    "        for i in range(self.n_steps_o):\n",
    "            l = forward()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "\n",
    "    def _select_action(self, theta_start, debug=False): # return plans, p of selecting each, and marginal p of actions\n",
    "      \n",
    "        # genrate all plans\n",
    "        plans = [ list(x) for x in itertools.product(range(self.env.a_N), repeat=self.k)]\n",
    "        # evaluate negative expected free energy of all plans\n",
    "        nefes = []\n",
    "        for pi in plans:\n",
    "          \n",
    "          if debug:\n",
    "            step_nefes, info_gains = self._rollout_step(theta_start, pi, \n",
    "                                                        debug=True)\n",
    "          else:\n",
    "            step_nefes = self._rollout_step(theta_start, pi)\n",
    "            \n",
    "          nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "          nefes.append(nefe)\n",
    "\n",
    "        # compute probability of following each plan\n",
    "        p_pi = softmax(np.array(nefes)).tolist()\n",
    "        if self.select_max_pi:\n",
    "            a = plans[np.argmax(nefes)]\n",
    "        else:\n",
    "            a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "\n",
    "        if debug:\n",
    "            # compute marginal action probabilities\n",
    "            p_a = np.zeros(self.env.a_N)\n",
    "            for p, pi in zip(p_pi, plans):\n",
    "                p_a[pi[0]] += p\n",
    "\n",
    "            return a, p_a, plans, p_pi, info_gains\n",
    "\n",
    "        return a\n",
    "\n",
    "    def _rollout_step(self, theta, pi, debug=False):\n",
    "        if pi == []:\n",
    "            return []\n",
    "\n",
    "        a, pi_rest = pi[0], pi[1:]\n",
    "        # Where will I be after taking action a?\n",
    "        theta1 = self._update_belief_a(theta, a=a) \n",
    "        q = softmax(theta1)\n",
    "        #print('--------------')\n",
    "        #print(theta)\n",
    "        #print(a)\n",
    "        #print(theta1)\n",
    "        #print(q)\n",
    "        \n",
    "        # Do I like being there?\n",
    "        pragmatic = np.dot(q, self.log_p_c)\n",
    "        # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "        ss = np.random.choice(range(self.env.s_N), p=q, size=self.n_o_samples)\n",
    "        oo = [rv.rvs() for rv in self.env.p_o_given_s[ss]]\n",
    "        # Do I learn about s from observing o?\n",
    "        q_o = [softmax(self._update_belief(theta1, o=o)) for o in oo]\n",
    "        d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "        info_gain = np.mean(d_o) # expected value of info gain\n",
    "        # negative expected free energy for this timestep\n",
    "        nefe = self.use_pragmatic_value * pragmatic + \\\n",
    "               self.use_info_gain * info_gain\n",
    "        \n",
    "        # nefe for remainder of policy rollout\n",
    "        nefe_rest = self._rollout_step(theta1, pi_rest)\n",
    "        # concatenate expected free energy across future time steps\n",
    "        if debug:\n",
    "          return [nefe] + nefe_rest, d_o\n",
    "\n",
    "        return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32761c20-2fb0-4c14-9164-d272f666dc8b",
   "metadata": {},
   "source": [
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461a202-af30-4818-9e34-b1f29067ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import continuous_observation_environment as coe\n",
    "import continuous_observation_agent as coa\n",
    "importlib.reload(coe)\n",
    "importlib.reload(coa)\n",
    "\n",
    "target_state = 4\n",
    "k = 4 # planning horizon; run time increases exponentially with planning horizon\n",
    "\n",
    "# runtime increases linearly with optimization steps during belief update\n",
    "n_steps_o = 20 # optimization steps updating belief after observation\n",
    "n_steps_a = 10 # optimization steps updating belief after action\n",
    "lr_o = 8. # learning rate updating belief after observation\n",
    "lr_a = 4. # learning rate updating belief after action\n",
    "\n",
    "render_losses = True\n",
    "\n",
    "env = coe.ContinuousObservationEnv(N=16, # number of states\n",
    "                              s_food=0, # location of the food source\n",
    "                              s_0=10, # starting location \n",
    "                              o_sample_size=5) # observation Beta distribution parameter.\n",
    "\n",
    "# visualise emission probability\n",
    "samples = [env.p_o_given_s[s].rvs(size=1000) for s in range(env.s_N)]\n",
    "df = pd.DataFrame(np.array(samples).T)\n",
    "sns.violinplot(df, cut=0, width=2)\n",
    "plt.xlabel('state s')\n",
    "plt.ylabel('p(o|s)')\n",
    "plt.title('Continuous environment emission probability')\n",
    "\n",
    "agent = coa.ContinuousObservationAgent(env=env, \n",
    "                             target_state=target_state,\n",
    "                             k=k, \n",
    "                             use_info_gain=True,\n",
    "                             use_pragmatic_value=True,\n",
    "                             select_max_pi=True,\n",
    "                             n_steps_o=n_steps_o, \n",
    "                             n_steps_a=n_steps_a, \n",
    "                             lr_a=lr_a, \n",
    "                             lr_o=lr_o)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "bb = [agent.b]\n",
    "aa = []\n",
    "if render_losses:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].set_title('updates from actions')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('optimization step')\n",
    "    ax[1].set_title('updates from observations')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('optimization step')\n",
    "    \n",
    "for i in range(64):\n",
    "    a, ll_o, ll_a, p_a = agent.step(o, debug=True)\n",
    "    print(f\"step {i}, s: {env.s_t}, max b:{bb[-1].argmax()}, o: {o:.2f}, p(a): {p_a}, a: {a}\")\n",
    "    if render_losses:\n",
    "        ax[0].plot(ll_a)\n",
    "        ax[1].plot(ll_o)\n",
    "    \n",
    "    o = env.step(a)\n",
    "    \n",
    "    ss.append(env.s_t)\n",
    "    bb.append(agent.b)\n",
    "    aa.append(a)\n",
    "\n",
    "\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(bb).T, label='belief')\n",
    "\n",
    "for i in range(len(aa)):\n",
    "  plt.plot([i, i], [ss[i], ss[i]+[-1,1][aa[i]]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETDOWN if aa[i] > 0 else CARETUP,\n",
    "             label=None if i > 0 else 'action')\n",
    "\n",
    "\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f854d-57f3-45f9-9801-c39b2438af36",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "We highlighted that during rollout for action selection we perform a vast number of belief updates due to sampling hypothetical observations in each timestep. In future work, we should explore how this could be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc41267-49f7-4938-9f17-fb2b83a30abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [env.p_o_given_s[s].rvs(size=1000) for s in range(env.s_N)]\n",
    "df = pd.DataFrame(np.array(samples).T)\n",
    "sns.violinplot(df, cut=0, width=2)\n",
    "plt.xlabel('state s')\n",
    "plt.ylabel('p(o|s)')\n",
    "plt.title('Continuous environment emission probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cab63-fac1-49ce-bdf0-6176aab68372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
