{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent for local imports\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax_rng = jax.random.PRNGKey(1337)\n",
    "\n",
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1337)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # show_p_s()\n",
    "from tqdm import tqdm\n",
    "\n",
    "import minimal_agent_jax as jax_agents\n",
    "import mountain_car_environment as envs\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef057d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_fully_observed_nefe(π_logits, q, p_t, log_p_c):\n",
    "    \"\"\" Deterministic policy selection with pragmatic value in fully-observed environment.\n",
    "    Args:\n",
    "        num_actions (int): number of unique actions. Jit assumes this is static across calls.\n",
    "        q (jnp.ndarray): one-hot encoding of the observed state\n",
    "        πs (jnp.ndarray): 4D tensor of one-hot actions [batch, policy, timestep, one_hot_action].\n",
    "        p_t (jnp.ndarray): 3D tensor of transition dynamics [a, s0, s1].\n",
    "        log_p_c (jnp.ndarray): 1D tensor of normalized state preferences. One element per environment state.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(q, a):\n",
    "        q = a @ (q @ p_t)\n",
    "        return q, q\n",
    "    \n",
    "    # calculate nefe\n",
    "    π = jax.nn.softmax(π_logits)\n",
    "    qs_π = jax.lax.scan(step, init=q, xs=π)[1] # carry, output\n",
    "    pragmatic = (qs_π @ log_p_c).sum(axis=-1)\n",
    "    nefe = pragmatic\n",
    "    return nefe\n",
    "\n",
    "def jax_update(π_logits, q, p_t, log_p_c, lr):\n",
    "    eval_fun = jax.value_and_grad(jax_fully_observed_nefe)\n",
    "    v, g = eval_fun(π_logits, q, p_t, log_p_c)\n",
    "    π_logits = π_logits + lr * g\n",
    "    return v, π_logits\n",
    "\n",
    "#embed function into agent\n",
    "class FullyObservedGradAgentJax:\n",
    "    \"\"\" Minimal agent performing exact inference in fully discrete POMDPs\"\"\"\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, \n",
    "                 num_grad_steps=100, \n",
    "                 learning_rate=2.):\n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.p_t = jnp.asarray(env.p_s1_given_s_a.swapaxes(0,1))\n",
    "        self.num_gradient_steps = num_grad_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        q_star = np.eye(self.env.s_N)[self.target_state] * 10\n",
    "        self.log_p_c = jnp.log( jnp.exp( q_star ))\n",
    "    \n",
    "    def step(self, o, debug=True):\n",
    "        π_logits = jnp.zeros( (self.k, self.env.a_N) ) # initialize uniform\n",
    "        params = {\n",
    "            'q': jax.nn.one_hot(o, self.env.s_N),\n",
    "            'p_t': self.p_t,\n",
    "            'log_p_c': self.log_p_c,\n",
    "            'lr': self.learning_rate\n",
    "        }\n",
    "        vv = []\n",
    "        for _ in range(self.num_gradient_steps):\n",
    "            v, π_logits = jax.jit(jax_update)(π_logits, **params)\n",
    "            vv.append(v)\n",
    "        \n",
    "        π = π_logits.argmax(axis=-1)\n",
    "        return π, vv, π_logits if debug else π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c788a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(jax_agents)\n",
    "importlib.reload(envs)\n",
    "\n",
    "k = 32 # planning time horizon\n",
    "s_target = np.array([1., 0.]) # specify target state as continuous [position, velocity]\n",
    "env = envs.MountainCarDiscrete()\n",
    "agent = jax_agents.FullyObservedGradAgentJax(env=env, \n",
    "                                  target_state=env.index_s_from_s(s_target), \n",
    "                                  k=k,\n",
    "                                  num_grad_steps=10, \n",
    "                                  learning_rate=1e2)\n",
    "agent.reset()\n",
    "\n",
    "ss = [env.reset()]\n",
    "ππ = []\n",
    "vvv = []\n",
    "ππ_logits = []\n",
    "for t in tqdm(range(k)):\n",
    "    π, vv, π_logits = agent.step(ss[-1], debug=True)\n",
    "    vvv.append(vv)\n",
    "    ππ_logits.append(π_logits)\n",
    "    ππ.append(π)\n",
    "    ss.append(env.step(ππ[-1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc966a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_trajectory(ss, color='gray', legend=False if r>0 else True)\n",
    "plt.suptitle('Observed trajectory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6db7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_q_a(π_logits):\n",
    "    q_π = jax.nn.softmax(π_logits)\n",
    "    aa = q_π.argmax(axis=-1)\n",
    "    im = plt.imshow(q_π.T, aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "plt.title('Plan at time $t=0$');\n",
    "show_q_a(ππ_logits[0])\n",
    "plt.ylabel('q(a)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, vv in enumerate(vvv):\n",
    "    plt.plot(vv, label=f'{t}')\n",
    "    \n",
    "plt.title('Plan optimisation at each interaction step');\n",
    "plt.xlabel('Gradient descent step');\n",
    "plt.ylabel('negative expected free energy $-\\mathbb{E}_{q}[ \\mathcal{G} ]$\\n(higher is better)');\n",
    "#plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would happen if we followed the same sequence of actions in an alternate sequence of sampled state transitions?\n",
    "n_rollouts = 16\n",
    "π = [π[0] for π in ππ]\n",
    "\n",
    "fig, ax = plt.subplots(1,2 , figsize=(2*8, 6))\n",
    "ax[0].scatter([s_target[0]], [s_target[1]], color='r', marker='o', s=10**2, label='target', zorder=1)\n",
    "for r in range(n_rollouts):\n",
    "    ss = [env.reset()]\n",
    "    for a in π:\n",
    "        ss.append(env.step(a))\n",
    "\n",
    "    env.plot_trajectory(ss, ax=ax, color='gray', legend=False if r>0 else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "π_0 = ππ[0] # original plan\n",
    "π_true = [π[0] for π in ππ] # selected actions\n",
    "plt.plot(π_0, label='$π_0$', alpha=0.75);\n",
    "plt.plot(π_true, label='$π_{true}$', alpha=0.75);\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b264cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
