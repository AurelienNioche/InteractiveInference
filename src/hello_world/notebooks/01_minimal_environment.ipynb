{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad67ad10-5d62-4df7-a6fd-7a2a62e7ea5a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206e212-0334-4e61-8ed6-80c9b23c9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b53e87-9197-456a-9716-5343a20bf23d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b69ee9-591a-4a3d-b883-a971b59a71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import minimal_environment as me\n",
    "importlib.reload(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd34b0-801b-454d-88d0-927f4b91817d",
   "metadata": {},
   "source": [
    "# Environments\n",
    "\n",
    "Environments are defined as discrete time Partially Observable Markov Decision Processes ([POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)s) witout reward function:\n",
    "\n",
    "- a set of states $s \\in \\mathcal{S}$\n",
    "- a set of observations $o \\in \\mathcal{\\Omega}$\n",
    "- a set of actions $a \\in \\mathcal{A}$\n",
    "- a set of conditional transition probabilities $\\mathcal{\\tau}: p(s'|s, a)$\n",
    "- a set of emission/ observation probabilities $\\mathcal{O}: p(o|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0e300-3d76-4597-808e-511f02fcfca5",
   "metadata": {},
   "source": [
    "## MinimalEnv environment\n",
    "\n",
    "The `MinimalEnv` environment has a discrete set of states and generates discrete (binary) outputs: food `True` or no food `False`.\n",
    "\n",
    "### Emission probability \n",
    "The emission probability distributions $p(o|s)$ are defined as a conditional probability table `p[s,o]`, where each row $i$ defines the probability of observing `False` (column 0) and `True` (column 1) in state $s_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7604a-6361-4798-b258-30d017598236",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    o_decay=0.4) # decay of observing food away from source \n",
    "\n",
    "p_o_given_s = env.p_o_given_s #precomputed result of env.emission_probability()\n",
    "print(p_o_given_s)\n",
    "print('shape', p_o_given_s.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "o_food = 1\n",
    "ax.bar(range(env.p_o_given_s[:,1].shape[0]), env.p_o_given_s[:,o_food])\n",
    "ax.bar([env.s_food], env.p_o_given_s[env.s_food,o_food], color='red', label='food source')\n",
    "ax.set_xlabel('state')\n",
    "ax.set_ylabel('$p(o=True|s)$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91af27-3aff-4385-b3ae-b7b173819d1d",
   "metadata": {},
   "source": [
    "### Transition dynamics\n",
    "The transition dynamics $p(s'|s, a)$ are defined as a conditional probability table `p[s,a,s']`. The subarray `p[0,0,:]`, for example, defines the probability of transitioning from state $0$ to any successor state, given that action `0` (move left) was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a451a-af6a-44c7-9b9c-5ea24336de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s1_given_s_a = env.p_s1_given_s_a # precomputed result of env.transition_dynamics()\n",
    "print('shape', p_s1_given_s_a.shape)\n",
    "p_s1_given_s_a[0,0,:] # left: 0, right: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b2ed0-4a2a-46ac-a88d-1f52021bcf4a",
   "metadata": {},
   "source": [
    "### Random Agent Behavior\n",
    "\n",
    "Unlike a POMDP, the environment itself does not define a goal, motivation, or purpose for an agent that interacts with it. The envirment is indifferent about how agents interact with it. There is therefore no value (good, bad, high or low performance) associated with any individual sequence of behavior. \n",
    "\n",
    "The code below simulates the interaction between the minimal environment and an agent that behaves randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c348-28e7-4724-b779-0d647328e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "ss, os = [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "os.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  os.append(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c570a-728b-4de6-8d91-d23da96958a5",
   "metadata": {},
   "source": [
    "We inspect the sequence of states and emissions during this interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76f6f-bf34-4a41-95fd-b3b8d3627a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('$s$')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(os))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation (1=Food)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d988dfb-1e05-4e95-b459-06a8433d81c2",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "\n",
    "## Belief Update\n",
    "\n",
    "An active inference agent holds a belief about the state of the environment at time $t$, modelled as a probability distribution $Q(s; \\theta_t)$ over states $s \\in \\mathcal{S}$. There is uncertainty associated with this belief as the state cannot be observed directly and must instead be inferred from stochastic observations.\n",
    "\n",
    "### Update through Time\n",
    "\n",
    "At any time $t$ an agent holds a prior belief about $s_t$ before taking in an observation from its environment. This belief could be uniform, for example at the start of an interaction. It could also be informed by propagating the belief $Q(s; \\theta_{t-1})$ about state $s_{t-1}$ through its model of the environment transition dynamics, taking into account the action $a_{t-1}$ taken at the previous time step.\n",
    "\n",
    "$$Q(s; \\theta_{t}) = \\mathbb{E}_{s\\sim Q_{t-1}}[p(s_t|s, a_{t-1})]$$\n",
    "\n",
    "Note that updating the belief in light of the chosen action involves fitting the parameters $\\theta_t$. For our minimal agent, we represent $Q(s;\\theta)$ as a softmax with logits $\\theta$. For diagnostics and debugging, we optionally log the optimization loss (`debug=True`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a7852-3123-4c39-925a-e0e837343c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief_a(env, theta_prev, a, lr=4., n_steps=10, debug=False):\n",
    "    # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    q = torch.nn.Softmax(dim=0)(theta)\n",
    "    \n",
    "    # this is the prior for the distribution at time t\n",
    "    # if we worked on this level, we would be done. \n",
    "    # but we need to determine the parameters of Q that produce \n",
    "    # this distribution\n",
    "    q1 = torch.matmul(q, torch.tensor(env.p_s1_given_s_a[:,a,:]))\n",
    "\n",
    "    # initialize updated belief to uniform\n",
    "    theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "    loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    if debug:\n",
    "        ll = np.zeros(n_steps)\n",
    "        \n",
    "    for i in range(n_steps):\n",
    "        l = loss(theta1, q1)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad82f8b-a543-4582-b139-2c67496d49dd",
   "metadata": {},
   "source": [
    "Let's see the effect of this belief update in action. Say we believed strongly that the environment was in states 1 or 4 with equal probability at time $t$ and we took action 1 (move right). Recall that according to the environment dynamics there is some probability of the state remaining unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c7a52-521a-479d-8663-a5d346e2991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "\n",
    "theta = np.eye(env.s_N)[1] * 2 + np.eye(env.s_N)[4] * 2\n",
    "theta1, ll = update_belief_a(env, theta, a=1, \n",
    "                             lr=4.0, n_steps=20, debug=True)\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(ll)\n",
    "plt.plot([0, ll.shape[0]-1], [ll.min()]*2, 'k--')\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta1), alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Propagating prior beliefs through environment dynamics.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa51f1-a4af-4718-b3ff-2ae364bcd651",
   "metadata": {},
   "source": [
    "### Update based on new observation\n",
    "\n",
    "Now that we have updated our prior taking into account the action taken in the previous time step and the (agent's model of the) environment dynamics, we turn our attention to updating beliefs in light of a new observation.\n",
    "\n",
    "In active inference, this belief update is cast as minimizing the variational free energy, i.e. minimizing the KL-divergence between $Q(s;\\theta')$ and $p(o, s) = Q(s; \\theta) p(o|s)$ with respect to $\\theta'$.\n",
    "\n",
    "$$D_{KL}(\\quad Q_{\\theta'}(s), p(o|s)Q_{\\theta}(s) \\quad) \\quad = \\quad \\mathbb{E}_{s \\sim Q_{\\theta'}}[\\quad \\log Q_{\\theta'}(s) - \\log p(o|s)Q_{\\theta}(s) \\quad]$$\n",
    "\n",
    "*See book p. 28, Eq. (2.5).*\n",
    "\n",
    "Again, for diagnostics and debugging, we optionally log the optimization loss (`debug=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f8fd2-b8d3-477d-835e-c9fe742a01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, theta_prev, o, lr=4., n_steps=10, debug=False):\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    \n",
    "    # make p(s) from b\n",
    "    q = torch.nn.Softmax(dim=0)\n",
    "    p = torch.tensor(env.p_o_given_s[:,o]) * q(theta) # p(o|s)p(s)\n",
    "    log_p = torch.log(p)\n",
    "    \n",
    "    # initialize updated belief with current belief (this is an arbitrary choice)\n",
    "    theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "    \n",
    "    # estimate loss\n",
    "    def forward():\n",
    "        q1 = q(theta1)\n",
    "        # free energy: KL[ q(s) || p(s, o) ]\n",
    "        fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "        return fe\n",
    "    \n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    ll = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        l = forward()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if debug:\n",
    "            ll[i] = l.detach().numpy()\n",
    "            \n",
    "    theta1 = theta1.detach().numpy()\n",
    "    if debug:\n",
    "        return theta1, ll\n",
    "        \n",
    "    return theta1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9788a-ec59-4f1f-a93a-1adb6bfc6e78",
   "metadata": {},
   "source": [
    "Let's see the effect of this in action. We start with a uniform prior belief. Recall that the food source is in state 0 and that the probability of observing food decreases exponentially with the distance of a state from the food source, with the state space wrapping around.\n",
    "\n",
    "If we observed no food ($o=0$), then it is most likely that we are in the state furthest away from the food source. If we observed food ($o=1$), then it is most likely that we are at the food source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3ff0d-e87c-46c9-8eda-b1ea2be43a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1 # what do we observe?\n",
    "\n",
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "theta = np.zeros(env.s_N) # uniform prior belief\n",
    "theta1, ll = update_belief(env, theta, o=o, lr=4., n_steps=20, debug=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(ll)\n",
    "plt.plot([0, ll.shape[0]-1], [ll.min()]*2, 'k--')\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta1), alpha=0.5, label='after') # belief before update\n",
    "plt.xlabel('env state')\n",
    "plt.ylabel('belief')\n",
    "plt.title('Updating beliefs in light of a new observation.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656b29-cd84-4a3a-b16c-3867020ed1c9",
   "metadata": {},
   "source": [
    "### Accumulating observations over time\n",
    "\n",
    "Note that, based on a single observation, we cannot refine our belief to become most confident in a state that is not the food source itself or furthest away from the food source. This can be improved if we accumulate observations from a single state over time. Because the probability of observing food decreases symmetrically to the left and right of the food source, the agent is likely to hold strong beliefs about all states that share the same rate of observing food, but cannot disambiguate these states without action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494d505-8062-4d25-98be-7dadcf49995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3 # true location\n",
    "n_timesteps = 50\n",
    "\n",
    "env = me.MinimalEnv(N=12, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=s) # starting location\n",
    "\n",
    "b = np.zeros(env.s_N) # initialize state prior as uniform\n",
    "o = env.reset() # set state to starting state\n",
    "\n",
    "# refine belief by sampling N observations but without taking any action\n",
    "qq = []\n",
    "for i in range(n_timesteps):\n",
    "    b = update_belief(env, theta_prev=b, o=int(o))\n",
    "    qq.append(softmax(b))\n",
    "    o = env.sample_o()\n",
    "\n",
    "plt.bar(range(env.s_N), qq[int(len(qq)*0.25)], alpha=0.3, label='25% observations');\n",
    "plt.bar(range(env.s_N), qq[-1], alpha=0.5, label='all observations');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739bb20-6a71-45b0-85ed-78d674953104",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "In active inference, sequences of actions (plans, policies) are scored by the negative expected free energy, and selected by exponentiating and normalizing, i.e. sampling from the softmax over plans. Plans $\\pi: a_0, a_1, ..., a_{K-1}$ define sequences of actions up to a finite horizon of $K$ timesteps into the future.\n",
    "\n",
    "The expected free energy can be decomposed in various ways and here we chose one that we find most intuitive, involving a _pragmatic_ term and an _information gain_ term (See Eq 4.9, page 73 in the book). \n",
    "\n",
    "The _pragmatic_ term assesses the probability of arriving in states following $\\pi$ that the agent desires, or of encountering observations that the agent desires. In this context, $Q_\\theta$ is estimated by propagating beliefs through the environment transition dynamics following the sequence of actions defined by $\\pi$, and observations are halucinated by sampling from the emission probability distributions.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}}\\left[\\log p_c(s)\\right] \\quad \\text{or} \\quad \\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[\\log p_c(o)\\right]$\n",
    "\n",
    "The _information gain_ term quantifies the belief update due to making observations in future states.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}, o \\sim p(o|s)}\\left[ D_{KL}(\\, Q_{\\theta'}(s|o),  Q_{\\theta}(s) \\,) \\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641c6a0-f2b4-4d9f-98a6-589388212b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "def rollout_step(env, log_p_c, theta, pi, \n",
    "                 use_info_gain, use_pragmatic_value):\n",
    "    \n",
    "    if pi == []:\n",
    "        return []\n",
    "\n",
    "    a, pi_rest = pi[0], pi[1:]\n",
    "\n",
    "    # Where will I be after taking action a?\n",
    "    theta1 = update_belief_a(env, theta, a=a, lr=1.) \n",
    "    q = softmax(theta1)\n",
    "\n",
    "    # Do I like being there?\n",
    "    pragmatic = np.dot(q, log_p_c)\n",
    "\n",
    "    # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "    p_o = np.dot(q, env.p_o_given_s)\n",
    "\n",
    "    # Do I learn about s from by observing o?\n",
    "    # enumerate/ sample observations, update belief and estimate info gain\n",
    "    q_o = [softmax(update_belief(env, theta1, o=i)) for i in range(p_o.shape[0])]\n",
    "    d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "    info_gain = np.dot(p_o, d_o) # expected value of info gain\n",
    "\n",
    "    # negative expected free energy for this timestep\n",
    "    nefe = use_pragmatic_value * pragmatic + use_info_gain * info_gain\n",
    "\n",
    "    # nefe for remainder of policy rollout\n",
    "    nefe_rest = rollout_step(env, log_p_c, theta1, pi_rest, \n",
    "                        use_info_gain=use_info_gain, \n",
    "                        use_pragmatic_value=use_pragmatic_value)\n",
    "\n",
    "    # concatenate expected free energy across future time steps\n",
    "    return [nefe] + nefe_rest\n",
    "\n",
    "def select_action(env, theta_star, theta_start, \n",
    "                  k=4, # planning horizon (number of sequential actions per plan)\n",
    "                  n_actions=2, # possible actions (assumed to be discrete and indexed 0)\n",
    "                  use_info_gain=True, \n",
    "                  use_pragmatic_value=True,\n",
    "                  select_max_pi=False, # replace sampling with best action selection\n",
    "                  debug=False, # return plans, p of selecting each, and marginal p of actions\n",
    "                 ):\n",
    "    log_p_c = np.log(softmax(theta_star))\n",
    "\n",
    "    # sampling\n",
    "    #n_plans = 32\n",
    "    #plans = np.random.choice(n_actions, size=(n_plans, k), replace=True).tolist()\n",
    "    # genrate all plans\n",
    "    plans = [ list(x) for x in itertools.product(range(n_actions), repeat=k)]\n",
    "\n",
    "    # evaluate negative expected free energy of all plans\n",
    "    nefes = []\n",
    "    for pi in plans:\n",
    "        step_nefes = rollout_step(env, log_p_c, theta_start, pi, \n",
    "                                  use_info_gain=use_info_gain, \n",
    "                                  use_pragmatic_value=use_pragmatic_value)\n",
    "        nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "        nefes.append(nefe)\n",
    "        \n",
    "    # compute probability of following each plan\n",
    "    p_pi = softmax(np.array(nefes)).tolist()  \n",
    "\n",
    "    if select_max_pi:\n",
    "        a = plans[np.argmax(nefes)]\n",
    "    else:\n",
    "        a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "    \n",
    "    if debug:\n",
    "        p_a = np.zeros(n_actions)\n",
    "        for p, pi in zip(p_pi, plans):\n",
    "            p_a[pi[0]] += p\n",
    "            \n",
    "        return a, p_a, plans, p_pi\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72190c5-69c0-4516-b548-9788e81fffa0",
   "metadata": {},
   "source": [
    "Let's explore action selection from plans with horizon $k$ by specifying sharp priors on the starting state and target state $k-1$ steps apart.\n",
    "\n",
    "If the starting state is to the right of the target (recall the state space wraps around), then policies that take a sequence of left actions ($a=0$)) are scored higher. Note that this holds true irrespective of the food source location. \n",
    "\n",
    "If the starting state is to the left of the target (e.g., $s_0=11$), then policies that take a sequence of right actions ($a=1$) are scored higher.\n",
    "\n",
    "If the starting state and the target state coincide, then policies that take equal numbers of left and right actions are scored highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6e97b-16a7-4e04-9a86-b73806d0f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = 11\n",
    "target_state = 14\n",
    "\n",
    "\n",
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=8) # location of the food source\n",
    "\n",
    "# initialize belief\n",
    "theta_start = np.eye(env.s_N)[starting_state] * 10 # believe we are in state 11\n",
    "\n",
    "# initialize preference\n",
    "theta_star = np.eye(env.s_N)[target_state] * 10\n",
    "\n",
    "a, p_a, plans, p_pi = select_action(env, theta_star, theta_start, debug=True)\n",
    "\n",
    "# and explore what the agent prefers\n",
    "plt.bar(x = range(len(plans)), height=p_pi)\n",
    "plt.xlabel('plan id')\n",
    "plt.ylabel('$p(\\pi)$')\n",
    "\n",
    "print('plans and associated probability of selecting them.')\n",
    "for p, pi in zip(p_pi, plans):\n",
    "    print(pi, p)\n",
    "\n",
    "# estimate marginal probability of selecting a plan with first action 0 or 1\n",
    "print('marginal probability of next action')\n",
    "print(p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c1d7f-fb80-417f-88f0-142ca6d13d18",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we have all components required to define a complete Active Infererence agent. Let's encapsulate it into a class that manages the target state and current belief state over time and provides a minimal interface with reset and step methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4dc051-75d5-4718-b95f-5f7a5f95f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 k=2, # planning horizon\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_pi=False, # sample plan (False), select max negEFE (True).\n",
    "                 n_steps_o=20, # optimization steps after new observation\n",
    "                 n_steps_a=20, # optimization steps after new action\n",
    "                 lr_o=4., # learning rate of optimization after new observation\n",
    "                 lr_a=4.): # learning rate of optimization after new action)\n",
    "        \n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.k = k\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_pi = select_max_pi\n",
    "        self.n_steps_o = n_steps_o\n",
    "        self.n_steps_a = n_steps_a\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_o = lr_o\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        self.b_star = np.eye(self.env.s_N)[self.target_state] * 10\n",
    "        self.log_p_c = np.log(softmax(self.b_star))\n",
    "        # initialize state prior as uniform\n",
    "        self.b = np.zeros(self.env.s_N)\n",
    "        \n",
    "    def step(self, o, debug=False):\n",
    "        if debug:\n",
    "            return self._step_debug(o)\n",
    "        \n",
    "        self.b = self._update_belief(theta_prev=self.b, o=int(o))\n",
    "        a = select_action(theta_start=self.b)[0] # pop first action of selected plan\n",
    "        self.b = self._update_belief_a(theta_prev=self.b, a=a)\n",
    "        return a\n",
    "    \n",
    "    def _step_debug(self, o):\n",
    "        self.b, ll_o = self._update_belief(theta_prev=self.b, \n",
    "                                           o=int(o), debug=True)\n",
    "        a, p_a, _, _ = self._select_action(theta_start=self.b, debug=True)\n",
    "        a = a[0]\n",
    "        self.b, ll_a = self._update_belief_a(theta_prev=self.b, a=a, debug=True)\n",
    "        return a, ll_o, ll_a, p_a\n",
    "    \n",
    "    def _update_belief_a(self, theta_prev, a, debug=False):\n",
    "        # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "        theta = torch.tensor(theta_prev)\n",
    "        q = torch.nn.Softmax(dim=0)(theta)\n",
    "\n",
    "        # this is the prior for the distribution at time t\n",
    "        q1 = torch.matmul(q, torch.tensor(self.env.p_s1_given_s_a[:,a,:]))\n",
    "\n",
    "        # initialize parameters of updated belief to uniform\n",
    "        theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "        loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_a)\n",
    "        if debug:\n",
    "            ll = np.zeros(self.n_steps_a)\n",
    "\n",
    "        for i in range(self.n_steps_a):\n",
    "            l = loss(theta1, q1)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "    \n",
    "    def _update_belief(self, theta_prev, o, debug=False):\n",
    "        theta = torch.tensor(theta_prev)\n",
    "\n",
    "        # make p(s) from b\n",
    "        q = torch.nn.Softmax(dim=0)\n",
    "        p = torch.tensor(self.env.p_o_given_s[:,o]) * q(theta) # p(o|s)p(s)\n",
    "        log_p = torch.log(p)\n",
    "\n",
    "        # initialize updated belief with current belief\n",
    "        theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "\n",
    "        # estimate loss\n",
    "        def forward():\n",
    "            q1 = q(theta1)\n",
    "            # free energy: KL[ q(s) || p(s, o) ]\n",
    "            fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "            return fe\n",
    "\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_o)\n",
    "        ll = np.zeros(self.n_steps_o)\n",
    "        for i in range(self.n_steps_o):\n",
    "            l = forward()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "\n",
    "    def _select_action(self, theta_start,\n",
    "                       debug=False): # return plans, p of selecting each, and marginal p of actions\n",
    "        # sampling\n",
    "        #n_plans = 32\n",
    "        #plans = np.random.choice(n_actions, size=(n_plans, k), replace=True).tolist()\n",
    "        # genrate all plans\n",
    "        plans = [ list(x) for x in itertools.product(range(self.env.a_N), repeat=self.k)]\n",
    "        # evaluate negative expected free energy of all plans\n",
    "        nefes = []\n",
    "        for pi in plans:\n",
    "            step_nefes = self._rollout_step(theta_start, pi)\n",
    "            nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "            nefes.append(nefe)\n",
    "\n",
    "        # compute probability of following each plan\n",
    "        p_pi = softmax(np.array(nefes)).tolist()\n",
    "        if self.select_max_pi:\n",
    "            a = plans[np.argmax(nefes)]\n",
    "        else:\n",
    "            a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "\n",
    "        if debug:\n",
    "            # compute marginal action probabilities\n",
    "            p_a = np.zeros(self.env.a_N)\n",
    "            for p, pi in zip(p_pi, plans):\n",
    "                p_a[pi[0]] += p\n",
    "\n",
    "            return a, p_a, plans, p_pi\n",
    "\n",
    "        return a\n",
    "\n",
    "    def _rollout_step(self, theta, pi):\n",
    "        if pi == []:\n",
    "            return []\n",
    "\n",
    "        a, pi_rest = pi[0], pi[1:]\n",
    "        # Where will I be after taking action a?\n",
    "        theta1 = self._update_belief_a(theta, a=a) \n",
    "        q = softmax(theta1)\n",
    "        # Do I like being there?\n",
    "        pragmatic = np.dot(q, self.log_p_c)\n",
    "        # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "        p_o = np.dot(q, self.env.p_o_given_s)\n",
    "        # Do I learn about s from by observing o?\n",
    "        # enumerate/ sample observations, update belief and estimate info gain\n",
    "        q_o = [softmax(self._update_belief(theta1, o=i)) for i in range(p_o.shape[0])]\n",
    "        d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "        info_gain = np.dot(p_o, d_o) # expected value of info gain\n",
    "        # negative expected free energy for this timestep\n",
    "        nefe = self.use_pragmatic_value * pragmatic + \\\n",
    "               self.use_info_gain * info_gain\n",
    "        # nefe for remainder of policy rollout\n",
    "        nefe_rest = self._rollout_step(theta1, pi_rest)\n",
    "        # concatenate expected free energy across future time steps\n",
    "        return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47bd8d-514a-45f3-8a4c-d6fa0cdc667e",
   "metadata": {},
   "source": [
    "\n",
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593474a-e694-4689-8452-4c860a4fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import minimal_agent_torch_vi as ma\n",
    "importlib.reload(ma)\n",
    "\n",
    "target_state = 4\n",
    "k = 4 # planning horizon; run time increases exponentially with planning horizon\n",
    "\n",
    "# runtime increases linearly with optimization steps during belief update\n",
    "n_steps_o = 20 # optimization steps updating belief after observation\n",
    "n_steps_a = 20 # optimization steps updating belief after action\n",
    "lr_o = 4. # learning rate updating belief after observation\n",
    "lr_a = 4. # learning rate updating belief after action\n",
    "\n",
    "render_losses = True\n",
    "\n",
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=0) # starting location \n",
    "\n",
    "agent = ma.MinimalAgentVI(env=env, \n",
    "                     target_state=target_state, \n",
    "                     k=k, \n",
    "                     use_info_gain=True,\n",
    "                     use_pragmatic_value=True,\n",
    "                     select_max_pi=False,\n",
    "                     n_steps_o=n_steps_o, \n",
    "                     n_steps_a=n_steps_a, \n",
    "                     lr_a=lr_a, \n",
    "                     lr_o=lr_o)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "bb = [agent.b]\n",
    "aa = []\n",
    "if render_losses:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].set_title('updates from actions')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('optimization step')\n",
    "    ax[1].set_title('updates from observations')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('optimization step')\n",
    "    \n",
    "for i in range(64):\n",
    "    a, ll_o, ll_a, p_a = agent.step(o, debug=True)\n",
    "    print(f\"step {i}, s: {env.s_t}, o: {['FOOD', 'NONE'][int(o)]}, p(a): {p_a}, a: {['UP', 'DOWN'][a]}\")\n",
    "    if render_losses:\n",
    "        ax[0].plot(ll_a)\n",
    "        ax[1].plot(ll_o)\n",
    "    \n",
    "    o = env.step(a)\n",
    "    \n",
    "    ss.append(env.s_t)\n",
    "    bb.append(agent.b)\n",
    "    aa.append(a)\n",
    "\n",
    "\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(bb).T, label='belief')\n",
    "t = np.arange(len(aa))\n",
    "i_left = t[aa==0]\n",
    "i_right = t[aa==1]\n",
    "plt.scatter(i_left, ss[:-1][i_left], c='white', marker=CARETUP)\n",
    "plt.scatter(i_right, ss[:-1][i_right], c='white', marker=CARETDOWN)\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0770b7-e2ea-4fc0-b007-b475b0b9af84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we developed the individual components of an active inference agent that are responsible for updating the agents belief after a new observation arrived and after a new action was taken, and for selecting actions based on the expected free energy, which we separated into an infomation gain term and a pragmatic value term. Each of these components were tested against expected behavior and diagnostic visualisations were introduced that can help adjust learning rates and number of optimization steps and that let us interpret action selection based on the marginal probability of a first action across all evaluated plans.\n",
    "\n",
    "We put everything together, encapsulated properties, state and methods in the `MinimalAgent` class, and analyzed this agent's sequential interactions with the `MinimalEnvironment`.\n",
    "\n",
    "There may be potential for further optimizing the code by separating torch compute graph definitions and their application to optimizing each individual interaction step's data.\n",
    "\n",
    "## Outlook\n",
    "\n",
    "The agentwe  developed above can interact with environments with\n",
    "- discrete state spaces\n",
    "- discrete observation spaces\n",
    "- discrete action spaces\n",
    "\n",
    "In subsequent notebooks, we will modify the agent to enable interaction with environments that have\n",
    "\n",
    "1. continuous action spaces\n",
    "2. continuous observation spaces\n",
    "3. continuous state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0822b-da3b-4c3f-98b0-20bded71e1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
