{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad67ad10-5d62-4df7-a6fd-7a2a62e7ea5a",
   "metadata": {},
   "source": [
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206e212-0334-4e61-8ed6-80c9b23c9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b53e87-9197-456a-9716-5343a20bf23d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b69ee9-591a-4a3d-b883-a971b59a71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import minimal_environment as me\n",
    "importlib.reload(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd34b0-801b-454d-88d0-927f4b91817d",
   "metadata": {},
   "source": [
    "# Environments\n",
    "\n",
    "Environments are defined as discrete time Partially Observable Markov Decision Processes ([POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)s) witout reward function:\n",
    "\n",
    "- a set of states $\\mathcal{S}$\n",
    "- a set of observations $\\mathcal{\\Omega}$\n",
    "- a set of conditional transition probabilities $\\mathcal{\\tau}: p(s'|s, a)$\n",
    "- a set of emission/ observation probabilities $\\mathcal{O}: p(o|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0e300-3d76-4597-808e-511f02fcfca5",
   "metadata": {},
   "source": [
    "## MinimalEnv environment\n",
    "\n",
    "The `MinimalEnv` environment has a discrete set of states and generates discrete outputs: either food `True` or no food `False`.\n",
    "\n",
    "### Emission probability \n",
    "The emission probability distributions $p(o|s)$ are defined as a conditional probability table `p[s,o]`, where each row $i$ defines the probability of observing `False` (column 0) or `True` (column 1) in state $s_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7604a-6361-4798-b258-30d017598236",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=8, # number of states\n",
    "                    s_food=0) # location of the food source\n",
    "\n",
    "p_o_given_s = env.p_o_given_s #precomputed result of env.emission_probability()\n",
    "print(p_o_given_s)\n",
    "print('shape', p_o_given_s.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "o_food = 1\n",
    "ax.bar(range(env.p_o_given_s[:,1].shape[0]), env.p_o_given_s[:,o_food])\n",
    "ax.bar([env.s_food], env.p_o_given_s[env.s_food,o_food], color='red', label='food source')\n",
    "ax.set_xlabel('state')\n",
    "ax.set_ylabel('$p(o=True|s)$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91af27-3aff-4385-b3ae-b7b173819d1d",
   "metadata": {},
   "source": [
    "### Transition dynamics\n",
    "The transition dynamics $p(s'|s, a)$ are defined as a conditional probability table `p[s,a,s']`. The subarray `p[0,0,:]` defines the probability of transitioning from state $0$ to any successor state, given that action `0` (move left) was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a451a-af6a-44c7-9b9c-5ea24336de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s1_given_s_a = env.p_s1_given_s_a # precomputed result of env.transition_dynamics()\n",
    "print('shape', p_s1_given_s_a.shape)\n",
    "p_s1_given_s_a[0,0,:] # left: 0, right: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b2ed0-4a2a-46ac-a88d-1f52021bcf4a",
   "metadata": {},
   "source": [
    "### Random Agent Behavior\n",
    "\n",
    "Unlike a POMDP, the environment itself does not define a goal, motivation, or purpose for an agent that interacts with it. The envirment is indifferent about how agents interact with it. There is therefore no value (good, bad, high or low performance) associated with any individual sequence of behavior. \n",
    "\n",
    "The code below simulates the interaction between the minimal environment and an agent that behaves randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c348-28e7-4724-b779-0d647328e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "ss, os = [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "os.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  os.append(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c570a-728b-4de6-8d91-d23da96958a5",
   "metadata": {},
   "source": [
    "We inspect the sequence of states and emissions during this interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76f6f-bf34-4a41-95fd-b3b8d3627a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('$s$')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(os))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation (1=Food)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d988dfb-1e05-4e95-b459-06a8433d81c2",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "\n",
    "## Belief Update\n",
    "\n",
    "An active inference agent holds a belief about the state of the environment at time $t$, modelled as a probability distribution $Q(s; \\theta_t)$ over states $s \\in \\mathcal{S}$. There is uncertainty associated with this belief as the state cannot be observed directly and must instead be inferred from observations.\n",
    "\n",
    "### Update through Time\n",
    "\n",
    "At any time $t$ an agent holds a prior belief about $s_t$ before taking in an observation from its environment. This belief could be uniform, for example at the start of an interaction. It could also be informed by propagating the belief $Q(s; \\theta_{t-1})$ about state $s_{t-1}$ through its model of the environment transition dynamics, taking into account the action $a_{t-1}$ it took at the previous time step.\n",
    "\n",
    "$$Q(s; \\theta_{t}) = \\mathbb{E}_{s\\sim Q_{t-1}}[p(s_t|s, a_{t-1})]$$\n",
    "\n",
    "Note that updating the belief in light of the chosen action involves fitting the parameters $\\theta_t$. For our minimal agent, we represent $Q(s;\\theta)$ as a softmax with logits $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a7852-3123-4c39-925a-e0e837343c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief_a(env, theta_prev, a, lr=1.):\n",
    "    # prior assumed to be parameters of the softmax (logits)\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    q = torch.nn.Softmax(dim=0)(theta)\n",
    "    \n",
    "    # this is the prior for the distribution at time t\n",
    "    # if we worked on this level, we would be done. \n",
    "    # but we need to determine the parameters of Q that produce \n",
    "    # this distribution\n",
    "    q1 = torch.matmul(q, torch.tensor(env.p_s1_given_s_a[:,a,:]))\n",
    "\n",
    "    # initialize updated belief to uniform\n",
    "    theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    \n",
    "    for i in range(50):\n",
    "        l = loss(theta1, q1)\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #print(l.item())\n",
    "        \n",
    "    return theta1.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad82f8b-a543-4582-b139-2c67496d49dd",
   "metadata": {},
   "source": [
    "Let's see the effect of this belief update in action. Say we believed strongly that the environment was in states 1 or 4 with equal probability at time $t$ and we took action 1 (move right). Recall that according to the environment dynamics there is some probability of the state remaining unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c7a52-521a-479d-8663-a5d346e2991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.eye(env.s_N)[1] * 2 + np.eye(env.s_N)[4] * 2\n",
    "\n",
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "theta_ = update_belief_a(env, theta, a=1, lr=1.0)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta_), alpha=0.5, label='after') # belief before update\n",
    "plt.title('Propagating prior beliefs through environment dynamics.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa51f1-a4af-4718-b3ff-2ae364bcd651",
   "metadata": {},
   "source": [
    "### Update based on new observation\n",
    "\n",
    "Now that we have updated our prior taking into account the action taken in the previous time step and the (agent's model of the) environment dynamics, we turn our attention to updating beliefs in light of a new observation.\n",
    "\n",
    "In active inference, this belief update is cast as minimizing the variational free energy, i.e. minimizing the KL-divergence between $Q(s;\\theta')$ and $p(o, s) = Q(s; \\theta) p(o|s)$ with respect to $\\theta'$.\n",
    "\n",
    "$$D_{KL}(\\quad Q_{\\theta'}(s), p(o|s)Q_{\\theta}(s) \\quad) \\quad = \\quad \\mathbb{E}_{s \\sim Q_{\\theta'}}[\\quad \\log Q_{\\theta'}(s) - \\log p(o|s)Q_{\\theta}(s) \\quad]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f8fd2-b8d3-477d-835e-c9fe742a01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(env, theta_prev, o, lr=1.):\n",
    "    theta = torch.tensor(theta_prev)\n",
    "    \n",
    "    # make p(s) from b\n",
    "    q = torch.nn.Softmax(dim=0)\n",
    "    p = torch.tensor(env.p_o_given_s[:,o]) * q(theta) # p(o|s)p(s)\n",
    "    log_p = torch.log(p)\n",
    "    \n",
    "    # initialize updated belief with current belief\n",
    "    theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "    \n",
    "    # estimate loss\n",
    "    def forward():\n",
    "        q1 = q(theta1)\n",
    "        # free energy: KL[ q(s) || p(s, o) ]\n",
    "        fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "        return fe\n",
    "    \n",
    "    optimizer = torch.optim.SGD([theta1], lr=lr)\n",
    "    for i in range(100):\n",
    "        loss = forward()\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #print(loss.item())\n",
    "        \n",
    "    return theta1.detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9788a-ec59-4f1f-a93a-1adb6bfc6e78",
   "metadata": {},
   "source": [
    "Let's see the effect of this in action. We start with a uniform prior belief. Recall that the food source is in state 0 and that the probability of observing food decreases exponentially with the distance of a state from the food source, with the state space wrapping around.\n",
    "\n",
    "If we observed no food ($o=0$), then it is most likely that we are in the state furthest away from the food source. If we observed food ($o=1$), then it is most likely that we are at the food source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3ff0d-e87c-46c9-8eda-b1ea2be43a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 0\n",
    "\n",
    "env = me.MinimalEnv(N=8,s_food=0)\n",
    "theta = np.zeros(env.s_N)\n",
    "theta_ = update_belief(env, theta, o=o, lr=1.)\n",
    "\n",
    "plt.bar(np.arange(env.s_N)-0.2, width=0.4, height=softmax(theta), alpha=0.5, label='before') # belief before update\n",
    "plt.bar(np.arange(env.s_N)+0.2, width=0.4, height=softmax(theta_), alpha=0.5, label='after') # belief before update\n",
    "plt.title('Updating beliefs in light of a new observation.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656b29-cd84-4a3a-b16c-3867020ed1c9",
   "metadata": {},
   "source": [
    "### Accumulating observations over time\n",
    "\n",
    "Note that, based on a single observation, we cannot refine our belief to become most confident in a state that is not the food source itself or furthest away from the food source. This can be improved if we accumulate observations from a single state over time. Because the probability of observing food decreases symmetrically to the left and right of the food source, the agent is likely to hold strong beliefs about all states that share the same rate of observing food, but cannot disambiguate these states without action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494d505-8062-4d25-98be-7dadcf49995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 2\n",
    "\n",
    "env = me.MinimalEnv(N=8, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=s) # starting location\n",
    "\n",
    "b = np.zeros(env.s_N) # initialize state prior as uniform\n",
    "o = env.reset() # set state to starting state\n",
    "\n",
    "# refine belief by sampling 10 observations\n",
    "for i in range(50):\n",
    "    b = update_belief(env, theta_prev=b, o=int(o))\n",
    "    o = env.sample_o()\n",
    "\n",
    "q = softmax(b)\n",
    "plt.bar(range(env.s_N), q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739bb20-6a71-45b0-85ed-78d674953104",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "In active inference, sequences of actions (plans, policies) are scored by the negative expected free energy, and selected by exponentiating and normalizing, i.e. sampling from the softmax over plans. Plans $\\pi: a_0, a_1, ..., a_K$ define sequences of actions up to a finite horizon $K$.\n",
    "\n",
    "The expected free energy can be decomposed in various ways and here we chose one that we find most intuitive. The pragmatic term assesses the probability of observing or arriving in states that the agent desires following $\\pi$. In this context, $Q_\\theta$ is estimated by propagating beliefs through the environment model.\n",
    "\n",
    "$\\mathbb{E}_{s \\sim Q_{\\theta}}[\\quad \\log p_c(y|s) \\quad]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641c6a0-f2b4-4d9f-98a6-589388212b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=s) # starting location\n",
    "\n",
    "# initialize belief\n",
    "theta_start = np.eye(env.s_N)[1] * 5\n",
    "\n",
    "# initialize preference\n",
    "target_state = 14\n",
    "theta_star = np.eye(env.s_N)[target_state] * 5\n",
    "log_p_c = np.log(softmax(theta_star))\n",
    "\n",
    "# create plans\n",
    "n_plans = 32 # number of plans to evaluate\n",
    "k = 4 # planning horizon (number of sequential actions per plan)\n",
    "n_actions = 2 # possible actions (assumed to be discrete and indexed 0)\n",
    "plans = np.random.choice(n_actions, size=(n_plans, k), replace=True).tolist()\n",
    "\n",
    "def kl(a, b):\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "def rollout(theta, pi, use_info_gain=True, use_pragmatic_value=True):\n",
    "    if pi == []:\n",
    "        return pi\n",
    "    \n",
    "    a, rest = pi[0], pi[1:]\n",
    "\n",
    "    # Do I want to get to where I will be?\n",
    "    theta = update_belief_a(env, theta, a=a, lr=1.) \n",
    "    q = softmax(theta)\n",
    "    pragmatic = np.dot(q, log_p_c)\n",
    "    \n",
    "    # Do I learn from observing what happens after doing what I plan?\n",
    "    # What might I observe if I took action a? marginalize p(o, s) over s\n",
    "    p_o = np.dot(q, env.p_o_given_s)\n",
    "    # enumerate/ sample observations, update belief and estimate info gain\n",
    "    q_o = [softmax(update_belief(env, theta, o=i)) for i in range(p_o.shape[0])]\n",
    "    d_o = [kl(a, q) for a in q_o]\n",
    "    info_gain = np.dot(p_o, d_o)\n",
    "    \n",
    "    # negative expected free energy for this timestep\n",
    "    nefe = use_pragmatic_value * pragmatic + use_info_gain * info_gain\n",
    "    \n",
    "    # nefe for remainder of policy rollout\n",
    "    rest = rollout(theta, rest)\n",
    "    \n",
    "    # concatenate expected free energy across future time steps\n",
    "    return [nefe] + rest\n",
    "\n",
    "nefes = []\n",
    "for pi in plans:\n",
    "    step_nefes = rollout(theta_start, pi)\n",
    "    nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "    nefes.append(nefe)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b318a-e4ad-49b0-b35d-1e2acf31c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.array(plans) # (sample, step)\n",
    "p_pi = softmax(np.array(nefes))\n",
    "\n",
    "# aggregate p(action| policy) across policies with same first action\n",
    "[p_pi[pi[:,0]==a].sum() for a in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d52499-c1bd-4203-9aaf-133f8cae74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, theta_star, b):\n",
    "    log_p_c = np.log(softmax(b_star)) # log probability of preferred states\n",
    "    \n",
    "    # with a discrete set of actions we can enumerate all possible plans.\n",
    "    # here, we consider only plans of length K=1 for simplicity.\n",
    "    #num_plans = 10\n",
    "    #k = 4\n",
    "    #plans = [np.choice(2, size=(n_plans, k), replace=True)\n",
    "    pi = []\n",
    "    g = []\n",
    "    #for i in range(num_plans):\n",
    "    for a in range(2):\n",
    "        \n",
    "        #pi = plans[i]\n",
    "        q = softmax(b)\n",
    "        q_ = np.dot(q, env.p_s1_given_s_a[:,a,:]) # next state given action\n",
    "        pragmatic_value = np.sum(q_ * log_p_c)\n",
    "        pi.append([a])\n",
    "        g.append(pragmatic_value)\n",
    "\n",
    "    # sample from softmax(g(policy))\n",
    "    i = np.random.choice(len(g), p=softmax(np.array(g)))\n",
    "    a = pi[i][0]\n",
    "    #print(a)\n",
    "    \n",
    "    return a\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593474a-e694-4689-8452-4c860a4fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state = 5\n",
    "\n",
    "env = me.MinimalEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    s_0=0) # starting location\n",
    "b_star = np.eye(env.s_N)[target_state] * 5 # state preference \n",
    "b = np.zeros(env.s_N) # initialize state prior as uniform\n",
    "o = env.reset() # set state to starting state\n",
    "\n",
    "ss = [env.s_t]\n",
    "bs = [b]\n",
    "for i in range(100):\n",
    "    b = update_belief(env, theta_prev=b, o=int(o))\n",
    "    a = select_action(env, theta_star=b_star, b=b)\n",
    "    b = update_belief_a(env, theta_prev=b, a=a)\n",
    "    o = env.step(a)\n",
    "    ss.append(env.s_t)\n",
    "    bs.append(b)\n",
    "\n",
    "q = softmax(b)\n",
    "plt.bar(range(env.s_N), q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998e097-1d61-4c70-b0c1-20a041033d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(s)\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(bs).T)\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3d0ef-e8e6-4757-8ef2-5b94a1a141cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
