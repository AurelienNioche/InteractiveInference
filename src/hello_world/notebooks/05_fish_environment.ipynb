{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c43ae27-32fa-4b21-a6a0-411b7a4c0c94",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d0a62-d648-4e43-8c82-dcc6a015ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d5643-2583-4cd3-8750-57d9009f5e7d",
   "metadata": {},
   "source": [
    "# GridViewer Environment\n",
    "\n",
    "The environment manages a wrap-around 2D grid of cells, a zoom camera and a cursor in camera coordinates. In each timestep, there is the opportunity to move the cursor (for the user to hover over a cell of interest) and to move the camera (for the assistant to support the user's goal).\n",
    "\n",
    "Currently, both cursor and camera movement are restricted to shifts along x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd703b-4da5-4a22-b874-66374dc90e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = namedtuple('Size', 'w h')\n",
    "Point = namedtuple('Point', 'x y')\n",
    "Rect = namedtuple('Rect', 'xy w h') # xy is of type Point\n",
    "Cell = namedtuple('Cell', 'xy w h c') #c is color\n",
    "\n",
    "class GridViewer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Interface showing a cropped view onto a grid where cells could represent,\n",
    "    for example, local regions on a map or buttons as on a keyboard.\n",
    "    \n",
    "    A shift-zoom camera shows a translated version of the grid with, depending\n",
    "    on the zoom level, only a subset of cells visible or cells appearing in\n",
    "    multiple locations wrapping the grid around along both x and y axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    State = namedtuple('State', 'camera cells cursor grid_size hovered_cell visible_cells')\n",
    "    \n",
    "    def __init__(self, grid_size, n_cells, camera_0, cursor_0):\n",
    "        \"\"\"\n",
    "        grid_size (Size): width and height of the world in world coordinates\n",
    "        n_cells (Point): number of cells in x and y direction\n",
    "        camera_0 (Rect): initial aperture settings after reset.\n",
    "        cursor_0 (Point): x, y coordinates of cursor in aperture view coordinates.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.n_cells = n_cells\n",
    "        self.camera_0 = camera_0\n",
    "        self.cursor_0 = self._clipped_cursor(cursor_0, self.camera_0)\n",
    "        self.user = user\n",
    "        self.cells = self.init_cells(self.grid_size, self.n_cells)\n",
    "        \n",
    "    @staticmethod\n",
    "    def init_cells(grid_size, n_cells):\n",
    "        w, h = grid_size.w / n_cells.x, grid_size.h / n_cells.y\n",
    "        cmap = mpl.colormaps['viridis'].resampled(n_cells.x * n_cells.y)\n",
    "        # (xy-bottom left, width, height, color)\n",
    "        cells = []\n",
    "        for ix in range(n_cells.x):\n",
    "            for iy in range(n_cells.y):\n",
    "                cells.append(Cell(Point(x=ix*w, y=iy*h), \n",
    "                                  w=w, h=h, c=cmap(iy*n_cells.x + ix)))\n",
    "\n",
    "        return cells\n",
    "    \n",
    "    def reset(self):\n",
    "        visible_cells = self._cells_in_camera_view(self.cells, \n",
    "                                                   self.camera_0, \n",
    "                                                   self.grid_size)\n",
    "        hovered_cell = self._hovered_cell(self.camera_0, \n",
    "                                          self.cells, \n",
    "                                          self.cursor_0, \n",
    "                                          self.grid_size)\n",
    "        self.s_t = self.State(camera=self.camera_0, \n",
    "                              cells=self.cells,\n",
    "                              cursor=self.cursor_0, \n",
    "                              grid_size=self.grid_size,\n",
    "                              hovered_cell=hovered_cell,\n",
    "                              visible_cells=visible_cells)\n",
    "        return self.s_t\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cells_in_camera_view(cells, camera, grid_size):\n",
    "        a_x0, a_x1 = camera.xy.x, camera.xy.x + camera.w\n",
    "        a_y0, a_y1 = camera.xy.y, camera.xy.y + camera.h\n",
    "\n",
    "        def clipped_cell(cell, filter_height=True):\n",
    "            x0, x1 = cell.xy.x, cell.xy.x + cell.w\n",
    "            y0, y1 = cell.xy.y, cell.xy.y + cell.h\n",
    "            x0_clip, x1_clip = np.clip([x0, x1], a_x0, a_x1)\n",
    "            y0_clip, y1_clip = np.clip([y0, y1], a_y0, a_y1)\n",
    "            w_clip, h_clip = x1_clip - x0_clip, y1_clip - y0_clip\n",
    "            \n",
    "            if w_clip > 0 and (not filter_height or h_clip > 0):\n",
    "                cell1 = Cell(xy=Point(x=x0_clip-a_x0,y=y0_clip-a_y0), \n",
    "                             w=w_clip, h=h_clip, c=cell.c)\n",
    "                return [cell1]\n",
    "            \n",
    "            return []\n",
    "\n",
    "        visible_cells = []\n",
    "        for cell in cells:\n",
    "            cell_i = cell._replace()\n",
    "            copies = []\n",
    "            x_copies = clipped_cell(cell_i, filter_height=False)\n",
    "            # repeat tiles along x\n",
    "            while True:\n",
    "                \n",
    "                cell_i = cell._replace(xy=cell.xy._replace(x=cell_i.xy.x + grid_size.w))\n",
    "                x_copies_i = clipped_cell(cell_i, filter_height=False)\n",
    "                if len(x_copies_i) == 0:\n",
    "                    break\n",
    "\n",
    "                x_copies.extend(x_copies_i)\n",
    "\n",
    "            if len(x_copies) > 0 and x_copies[0].h > 0:\n",
    "                # add cells along x to visible cells if they are visible\n",
    "                copies.extend(x_copies)\n",
    "            \n",
    "            # repeat subtiles along y\n",
    "            for cell_x in x_copies:\n",
    "                y_copies = []\n",
    "                cell_i = cell_x._replace(xy=cell_x.xy._replace(y=cell.xy.y))\n",
    "                while True:\n",
    "                    \n",
    "                    cell_i = cell_i._replace(xy=cell_i.xy._replace(y=cell_i.xy.y + grid_size.h))\n",
    "                    y_copies_i = clipped_cell(cell_i)\n",
    "                    if len(y_copies_i) == 0:\n",
    "                        break\n",
    "                        \n",
    "                    y_copies.exptend(y_copies_i)\n",
    "                copies.extend(y_copies)\n",
    "            visible_cells.append(copies)\n",
    "        return visible_cells\n",
    "\n",
    "    @classmethod\n",
    "    def transition_assistant(cls, s, a):\n",
    "        cam = s.camera\n",
    "        cam1 = Rect(Point(x=a*s.grid_size.w, y=cam.xy.y), w=cam.w, h=cam.h)\n",
    "        cam1 = cls._clipped_camera(cam1, s.grid_size)\n",
    "        cursor1 = cls._clipped_cursor(cursor=s.cursor, camera=cam1)\n",
    "        visible_cells1 = cls._cells_in_camera_view(s.cells, cam1, s.grid_size)\n",
    "        hovered_cell1 = cls._hovered_cell(cam1, s.cells, cursor1, s.grid_size)\n",
    "        return s._replace(camera=cam1, \n",
    "                          cursor=cursor1, \n",
    "                          hovered_cell=hovered_cell1,\n",
    "                          visible_cells=visible_cells1)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _clipped_camera(camera, grid_size):\n",
    "        # restrict xy to map limits\n",
    "        xy = Point(x=np.clip(camera.xy.x, 0, grid_size.w),\n",
    "                   y=np.clip(camera.xy.y, 0, grid_size.h))\n",
    "        return camera._replace(xy=xy)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clipped_cursor(cursor, camera):\n",
    "        # restrict xy to camera limits\n",
    "        return Point(x=np.clip(cursor.x, 0, camera.w),\n",
    "                     y=np.clip(cursor.y, 0, camera.h))\n",
    "        \n",
    "    def step_assistant(self, a):\n",
    "        \"\"\"\n",
    "        Executes the action of an assistant. Here, the action represents\n",
    "        the new normalized camera shift along x.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        a (float): normalized absolute camera x-shift in [0, 1].\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state (State): grid viewer state after adjusting the camera parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.s_t = self.transition_assistant(s=self.s_t, a=a)\n",
    "        return self.s_t\n",
    "    \n",
    "    @classmethod\n",
    "    def transition_user(cls, s, a):\n",
    "        # compute successor state after user takes action a in state s\n",
    "        cursor1 = Point(x=s.cursor.x + a, y=s.cursor.y)\n",
    "        cursor1 = cls._clipped_cursor(cursor1, s.camera)\n",
    "        hovered_cell1 = cls._hovered_cell(s.camera, s.cells, cursor1, s.grid_size)\n",
    "        return s._replace(cursor=cursor1, hovered_cell=hovered_cell1)\n",
    "        \n",
    "    def step_user(self, a):\n",
    "        \"\"\"\n",
    "        Executes an action on the cursor (performed by the user), which \n",
    "        represents a shift in cursor position along the x axes.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        a (float): intended cursor shift from latest position\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state (State): grid viewer state after adjusting cursor position.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.s_t = self.transition_user(s=self.s_t, a=a)\n",
    "        return self.s_t\n",
    "    \n",
    "    @classmethod\n",
    "    def render_world_view(cls, s, ax=None):\n",
    "        cursor = cls._absolute_cursor(s.cursor, s.camera, s.grid_size)\n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "        ax.set_title('World View')\n",
    "        ax.set_xlim([0., np.maximum(s.grid_size.w, s.camera.xy.x + s.camera.w)])\n",
    "        ax.set_ylim([0., np.maximum(s.grid_size.h, s.camera.xy.y + s.camera.h)])\n",
    "            \n",
    "        # draw cells\n",
    "        for c in s.cells:\n",
    "            ax.add_patch(mpl.patches.Rectangle(c.xy, c.w, c.h, color=c.c))\n",
    "            \n",
    "        # draw grid outline\n",
    "        ax.add_patch(mpl.patches.Rectangle([0, 0], s.grid_size.w, s.grid_size.h, \n",
    "                                           fill=False,\n",
    "                                           edgecolor='blue', \n",
    "                                           linewidth=2., \n",
    "                                           label='grid outline'))\n",
    "        # draw camera\n",
    "        ax.add_patch(mpl.patches.Rectangle(s.camera.xy, s.camera.w, s.camera.h, \n",
    "                                           fill=False,\n",
    "                                           edgecolor='red', \n",
    "                                           linewidth=2., \n",
    "                                           label='camera'))\n",
    "            \n",
    "        # draw cursor\n",
    "        ax.scatter(x=cursor.x, y=cursor.y, marker='x', s=10**2, color='red', \n",
    "                   linewidth=2, label='cursor')\n",
    "        ax.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def render_camera_view(s, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        ax.set_title('Camera View')\n",
    "        ax.set_xlim([0., s.camera.w])\n",
    "        ax.set_ylim([0., s.camera.h])\n",
    "        for cell in s.visible_cells:\n",
    "            # iterate over cell parts/ copies\n",
    "            for c in cell:\n",
    "                ax.add_patch(mpl.patches.Rectangle(c.xy, c.w, c.h, \n",
    "                                                   facecolor=c.c, \n",
    "                                                   edgecolor='white'))\n",
    "                \n",
    "        # draw cursor\n",
    "        ax.scatter(x=s.cursor.x, y=s.cursor.y, marker='x', s=10**2, color='red', \n",
    "                   linewidth=2, label='cursor')\n",
    "        ax.legend()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _absolute_cursor(cursor, camera, grid_size):\n",
    "        return Point(x = (cursor.x + camera.xy.x) % grid_size.w,\n",
    "                     y = (cursor.y + camera.xy.y) % grid_size.h)\n",
    "    \n",
    "    @classmethod\n",
    "    def _hovered_cell(cls, camera, cells, cursor, grid_size):\n",
    "        # determine the tile the cursor currently hovers over\n",
    "        cursor = cls._absolute_cursor(cursor, camera, grid_size)\n",
    "        for i in range(len(cells)):\n",
    "            c = cells[i]\n",
    "            if cursor.x >= c.xy.x and cursor.y >= c.xy.y and \\\n",
    "               cursor.x < c.xy.x + c.w and cursor.y < c.xy.y + c.h:\n",
    "                return i\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3839b-163a-4f9b-8bf8-b65111e2f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = Size(w=800, h=600)\n",
    "n_cells = Point(x=3, y=3)\n",
    "camera = Rect(xy=Point(x=0, y=0), w=grid_size.w, h=grid_size.h)\n",
    "cursor = Point(x=camera.w*0.5, y=camera.h*0.5)\n",
    "\n",
    "env = GridViewer(grid_size=grid_size, n_cells=n_cells, camera_0=camera, cursor_0=cursor)\n",
    "s = env.reset()\n",
    "print('camera:', s.camera.xy, 'cursor:', s.cursor, 'grid_size', s.grid_size)\n",
    "env.render_camera_view(s)\n",
    "env.render_world_view(s)\n",
    "s = env.step_assistant(a=0.25)\n",
    "s = env.step_user(a=0)\n",
    "print('camera:', s.camera.xy, 'cursor:', s.cursor, 'grid_size', s.grid_size)\n",
    "env.render_camera_view(s)\n",
    "env.render_world_view(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813d349-0e45-463d-936f-6e19e855807c",
   "metadata": {},
   "source": [
    "# User Model\n",
    "\n",
    "The user model has the desire to hover the cursor over a target cell, correctly identifies the direction of movement for the cursor to get there, and takes a fixed length step plus some noise along that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24ba95-3abb-4ab4-b927-5baf373723e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class Fish:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_targets, # total number of available targets\n",
    "                 goal, # user goal\n",
    "                 step_size, # amount of cursor movement (in aperture coordinates)\n",
    "                 sigma): # amount of Gaussian noise around movement\n",
    "        self.n_targets = n_targets\n",
    "        self.goal = goal % n_targets\n",
    "        self.step_size = step_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def reset(self):\n",
    "        return 0 # do nothing before first observation\n",
    "        \n",
    "    def step(self, s):\n",
    "        # compute likelihood\n",
    "        p_a = self._p_a_given_s_and_goal(s, self.goal)\n",
    "        a = p_a.rvs() # sample\n",
    "        return a\n",
    "    \n",
    "    def _p_a_given_s_and_goal(self, s, i):\n",
    "        # likelihood of action given environment state and use goal\n",
    "        mean = 0\n",
    "        if not (s.hovered_cell == i):\n",
    "            # the below approach estimates the shortest distance to the edge,\n",
    "            # which is assumed to be perpendicular to the direction of travel.\n",
    "            # if this is not the case we need a different method, i.e. when\n",
    "            # we move to 2D cursor movement this does not generalise\n",
    "            \n",
    "            # estimate shortest (signed) distance to edge of goal tiles along x\n",
    "            a_x = [[c.xy.x - s.cursor.x, c.xy.x+c.w - s.cursor.x] for c in s.visible_cells[i]]\n",
    "            a_x = np.array(a_x)\n",
    "            a_x = np.array([a_x[i, min_j] for i, min_j in zip(range(a_x.shape[0]), np.argmin(np.abs(a_x), axis=1))])\n",
    "            \n",
    "            # estimate shortest (signed) distance to edge of goal tiles along y\n",
    "            #a_y = [[c.xy.y - s.cursor.y, c.xy.y+c.h - s.cursor.y] for c in s.visible_cells[i]]\n",
    "            #a_y = np.array(a_y)\n",
    "            #a_y = np.array([a_y[i, min_j] for i, min_j in zip(range(a_y.shape[0]), np.argmin(np.abs(a_y), axis=1))])\n",
    "            \n",
    "            # currently, action is only along x, so we ignore y for the distance\n",
    "            d = np.abs(a_x) # np.sqrt(a_x**2 + a_y**2)\n",
    "            \n",
    "            # ideal action\n",
    "            #action = np.array([a_x[np.argmin(d)], a_y[np.argmin(d)]])/d * self.step_size\n",
    "            mean = a_x[np.argmin(d)]\n",
    "            \n",
    "            # discretized action\n",
    "            # TODO: normalize vector to step_size length in 2D\n",
    "            mean = np.sign(mean) * self.step_size\n",
    "            \n",
    "        return norm(loc=mean, scale=self.sigma)\n",
    "    \n",
    "    def p_o_given_s(self, s_env):\n",
    "        # returns a list of likelihoods p(a|s, s_env): [s]\n",
    "        return np.array([self._p_a_given_s_and_goal(s_env, i) for i in range(self.n_targets)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e992d-3523-4edc-aae9-8d96f47f7ab3",
   "metadata": {},
   "source": [
    "## User interaction with the GridViewer environmnet\n",
    "\n",
    "Let's simulate the interaction between the user and the environment. Below, we illustrate the movement of the cursor in world coordinates over time. We add some jitter along y to better illustrate _backwards_ steps that may occur depending on the amount of noise the user adds to the optimal action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f37e6-f31a-4004-9e53-4d48bf831047",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = Size(w=800, h=600)\n",
    "n_cells = Point(x=10, y=1)\n",
    "camera = Rect(xy=Point(x=0, y=0), w=grid_size.w, h=grid_size.h)\n",
    "cursor = Point(x=camera.w*0.5, y=camera.h*0.5)\n",
    "\n",
    "user = Fish(n_targets=n_cells.x*n_cells.y, \n",
    "            goal=0, \n",
    "            step_size = int(camera.w*0.05), \n",
    "            sigma=1e1)\n",
    "user.reset()\n",
    "\n",
    "env = GridViewer(grid_size=grid_size, n_cells=n_cells, camera_0=camera, cursor_0=cursor)\n",
    "\n",
    "ss = []\n",
    "aa_user = []\n",
    "ss.append(env.reset())\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    #env.render_camera_view(ss[-1])\n",
    "    #env.render_world_view(s)\n",
    "    aa_user.append(user.step(ss[-1]))\n",
    "    ss.append(env.step_user(aa_user[-1]))\n",
    "    \n",
    "env.render_camera_view(ss[-1])\n",
    "#plt.subplots()\n",
    "cursor_t = np.array([s.cursor.x for s in ss])\n",
    "plt.plot(cursor_t, env.s_t.cursor.y + 50*(np.random.uniform(size=cursor_t.shape)-0.5), 'r-x', label='$a_t$', zorder=100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d8dcd-91ff-4ede-b8ee-a1339f4c4c52",
   "metadata": {},
   "source": [
    "# Active Inference assistant\n",
    "\n",
    "An assistant that reasons about the user's hidden goal and adjusts the camera to support the user has the following properties:\n",
    "\n",
    "- the unobserved state space represents the **discrete** set of goals (target cells) of the user\n",
    "- the observed state space represents the state of the GridViewer, which has discrete and continuous elements\n",
    "- the observations generated by the unobserved state represent the user's actions and are **continuous**\n",
    "- the assistant's actions represent a shift of the camera as a **continuous** proportion of $[0, 1]$ the grid size along x.\n",
    "\n",
    "We develop the active inference agent for this problem starting with the `ContinuousAOAgent` developed in notebook 04. \n",
    "\n",
    "#### Markov Blanket\n",
    "The environment of the assistant now consists of the grid viewer and the user. It observes the user's actions and the grid viewer's state, reasons about the user's beliefs and selects actions that indirectly affect the user by modifying the camera position of the grid viewer. The assistant needs models of both to reason about this three-way interaction, update its belief about the user's goal and to select a supportive action.\n",
    "\n",
    "#### Belief update through time\n",
    "\n",
    "The goal of the user is assumed to be fixed in this scenario. It follows that the assistants actions do not change the unobserved state it reasons about, so that the assistants belief about the user's goal does not need updating in response to the assistants actions.\n",
    "\n",
    "#### Belief update after new observation\n",
    "\n",
    "The belief update after a new observation from the user involves evaluating the likelihood $p(o|s_{user}, s_{env})$ for all hypothetical discrete goals the user may have $s_{user}$ conditiond on the grid viewer state $s_{env}$. This requires \n",
    "\n",
    "#### Assistant Preference\n",
    "\n",
    "So far, we represented the assistants preference in variants of the minimal environment as a preference over environment states. Here we move away from that and develop a notion of preference that represents the assistants objective of supporting the user in reaching their (unobserved) goal. Intuitively, we want to evaluate how compatible the assistant's actions and user's goal are. In this scenario, the user's goal is for the cursor to hover over one particular cell $i$. If we knew the user's goal, we could define $p_c(s_{env})$ as the probability distribution over environment states $s_{env}$ that is zero in all states in which the cursor doesn't over over cell $i$ and one in those states that do.\n",
    "\n",
    "$\\mathbb{E}_{Q_{env}(s_{env}^{t+1}|s_{env}^{t}, a_a^t)}\\left[ \\ln p_c(\\text{cell}(s_{env}^t)) \\right] \\quad = \\quad \\ln p_c(\\text{cell}(s_{env}^t)) \\quad \\approx \\quad \\ln Q_{usr}(\\text{cell}(s_{env}^t))$,\n",
    "\n",
    "where we made use of the knowledge that the environment state is fully observed and its transition dynamics are deterministic. While we don't know the user's goal $i$, we have a belief about it represented as  $Q_{usr}(s_{usr})$. We use this to approximate $p_c$ by spreading the probability mass over goals in proportion to our belief that each goal is held by the user.\n",
    "\n",
    "\n",
    "\n",
    "#### Summary of changes to the agent implementation\n",
    "\n",
    "- replace the model of the environment with a model of the interface and a model of the user\n",
    "- remove the explicit representation of a target state\n",
    "- remove belief updating in response to the agent's actions\n",
    "- update environment state after belief updating and before action selection in the step function\n",
    "- hardcode limits of assistant action to [0, 1]. Move this into the grid viewer in the future.\n",
    "- update pragmatic value estimation to assistant's notion of preference.\n",
    "\n",
    "#### Summary of changes to the user model\n",
    "\n",
    "- a method taking $s_{env}$ as parameter and returning $p(o|s_{usr}, s_{env})$ as a list of likelihoods for each hypothetical user goal.\n",
    "\n",
    "#### TODO\n",
    "\n",
    "- move limits to assistant actions into GridViewer and add limits to user actions there, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade6606-1525-4038-a872-2993c11af86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "class ContinuousAOAssistant:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env_model,\n",
    "                 user_model,\n",
    "                 n_plans=128, # number of plans rolled out during action selection\n",
    "                 k=2, # planning horizon\n",
    "                 n_o_samples=10, # observation samples for information gain\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_pi=True, # sample plan (False), select max negEFE (True).\n",
    "                 n_steps_o=20, # optimization steps after new observation\n",
    "                 lr_o=4.): # learning rate of optimization after new observation\n",
    "        \n",
    "        self.env_model = env_model\n",
    "        self.user_model = user_model\n",
    "        self.n_plans = n_plans\n",
    "        self.k = k\n",
    "        self.n_o_samples = n_o_samples\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_pi = select_max_pi\n",
    "        self.n_steps_o = n_steps_o\n",
    "        self.lr_o = lr_o\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state prior as uniform\n",
    "        self.b = np.zeros(self.user_model.n_targets)\n",
    "        # in the absence of prior knowledge, initialise camera at 0\n",
    "        return 0\n",
    "        \n",
    "    def step(self, o_env, o_user, debug=False):\n",
    "        if debug:\n",
    "            return self._step_debug(o_env, o_user)\n",
    "        \n",
    "        self.b = self._update_belief(theta_prev=self.b, \n",
    "                                     o_env=o_env, \n",
    "                                     o_user=o_user)\n",
    "        # model next environment state after user action\n",
    "        o_env1 = self.env_model.transition_user(s=o_env, a=o_user)\n",
    "        a = select_action(theta_start=self.b, o_env=o_env1)[0] # pop first action of selected plan\n",
    "        return a\n",
    "    \n",
    "    def _step_debug(self, o_env, o_user):\n",
    "        self.b, ll_o = self._update_belief(theta_prev=self.b, \n",
    "                                           o_env=o_env, \n",
    "                                           o_user=o_user, \n",
    "                                           debug=True)\n",
    "        # model next environment state after user action\n",
    "        o_env1 = self.env_model.transition_user(s=o_env, a=o_user)\n",
    "        a, plans, p_pi, _ = self._select_action(theta_start=self.b, o_env=o_env1, debug=True)\n",
    "        max_a = plans[np.argmax(p_pi)][0]\n",
    "        a = a[0]\n",
    "        return a, ll_o, max_a\n",
    "    \n",
    "    def _update_belief(self, theta_prev, o_env, o_user, debug=False):\n",
    "        theta = torch.tensor(theta_prev)\n",
    "\n",
    "        # make p(s) from b\n",
    "        q = torch.nn.Softmax(dim=0)\n",
    "        p_o_given_s = [p.pdf(o_user) for p in self.user_model.p_o_given_s(o_env)]\n",
    "        p_o_given_s = torch.tensor(p_o_given_s)\n",
    "        p = p_o_given_s * q(theta) # p(o|s)p(s)\n",
    "        log_p = torch.log(p)\n",
    "\n",
    "        # initialize updated belief with current belief\n",
    "        theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "\n",
    "        # estimate loss\n",
    "        def forward():\n",
    "            q1 = q(theta1)\n",
    "            # free energy: KL[ q(s) || p(s, o) ]\n",
    "            fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "            return fe\n",
    "\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_o)\n",
    "        ll = np.zeros(self.n_steps_o)\n",
    "        for i in range(self.n_steps_o):\n",
    "            l = forward()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "\n",
    "    def _select_action(self, theta_start, o_env, debug=False): \n",
    "        # return plans, p of selecting each, and marginal p of actions\n",
    "        \n",
    "        # sampling\n",
    "        a_lims = [0, 1]\n",
    "        plans = np.random.uniform(low=a_lims[0], high=a_lims[1], size=(self.n_plans, self.k)).tolist()\n",
    "        \n",
    "        # evaluate negative expected free energy of all plans\n",
    "        nefes = []\n",
    "        info_gains = []\n",
    "        for pi in tqdm(plans):\n",
    "          \n",
    "          if debug:\n",
    "            step_nefes, info_gain = self._rollout_step(theta_start, o_env, pi, \n",
    "                                                        debug=True)\n",
    "            info_gains.append(info_gain)\n",
    "          else:\n",
    "            step_nefes = self._rollout_step(theta_start, o_env, pi)\n",
    "            \n",
    "          nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "          nefes.append(nefe)\n",
    "\n",
    "        # compute probability of following each plan\n",
    "        p_pi = softmax(np.array(nefes)).tolist()\n",
    "        if self.select_max_pi:\n",
    "            a = plans[np.argmax(nefes)]\n",
    "        else:\n",
    "            a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "\n",
    "        if debug:\n",
    "            return a, plans, p_pi, info_gains\n",
    "\n",
    "        return a\n",
    "      \n",
    "    def _rollout_step(self, theta, o_env, pi, debug=False):\n",
    "        if pi == []:\n",
    "            return []\n",
    "\n",
    "        a, pi_rest = pi[0], pi[1:]\n",
    "        # Where will I be after taking action a?\n",
    "        s_env1 = self.env_model.transition_assistant(s=o_env, a=a)\n",
    "        \n",
    "        # Do I like being there?\n",
    "        q = softmax(theta) # current belief (where user wants to go)\n",
    "        # cursor after action a is at s_env1.hovered_cell\n",
    "        # is the outcome of the action consistent with user goals?\n",
    "        pragmatic = np.log(q)[s_env1.hovered_cell]\n",
    "        \n",
    "        # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "        ss_user = np.random.choice(range(self.user_model.n_targets), p=q, size=self.n_o_samples)\n",
    "        oo_user = [rv.rvs() for rv in self.user_model.p_o_given_s(s_env1)[ss_user]]\n",
    "        # Do I learn about s_user from observing o_user?\n",
    "        q_o = [softmax(self._update_belief(theta, o_env=s_env1, o_user=o)) for o in oo_user]\n",
    "        d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "        info_gain = np.mean(d_o) # expected value of info gain\n",
    "        # negative expected free energy for this timestep\n",
    "        nefe = self.use_pragmatic_value * pragmatic + \\\n",
    "               self.use_info_gain * info_gain\n",
    "        \n",
    "        # nefe for remainder of policy rollout\n",
    "        # advance environment by taking one user action\n",
    "        s_env2 = self.env_model.transition_user(s=s_env1, a=oo_user[0])\n",
    "        nefe_rest = self._rollout_step(theta, s_env2, pi_rest)\n",
    "        # concatenate expected free energy across future time steps\n",
    "        if debug:\n",
    "          return [nefe] + nefe_rest, info_gain\n",
    "\n",
    "        return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360de35-d4ac-4fa2-ae58-19fdf0de9725",
   "metadata": {},
   "source": [
    "## Reasoning about user goals\n",
    "\n",
    "Let's simulate an interaction between the grid viewer and the user with the assistant simply observing the interaction and reasoning about the user's goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad1466-f59f-42c5-aae9-81b2ba24a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = Size(w=800, h=600)\n",
    "n_cells = Point(x=10, y=1)\n",
    "camera = Rect(xy=Point(x=0, y=0), w=grid_size.w, h=grid_size.h)\n",
    "cursor = Point(x=camera.w*0.5, y=camera.h*0.5)\n",
    "\n",
    "user = Fish(n_targets=n_cells.x*n_cells.y, \n",
    "            goal=7, \n",
    "            step_size = int(camera.w*0.05), \n",
    "            sigma=5e1)\n",
    "user.reset()\n",
    "\n",
    "env = GridViewer(grid_size=grid_size, n_cells=n_cells, camera_0=camera, cursor_0=cursor)\n",
    "\n",
    "assistant = ContinuousAOAssistant(env_model=env, \n",
    "                                  user_model=user, \n",
    "                                  n_steps_o=20,\n",
    "                                  lr_o=2e0)\n",
    "assistant.reset()\n",
    "\n",
    "ss = [env.reset()]\n",
    "aa_user = []\n",
    "bb = [assistant.b]\n",
    "\n",
    "fig, ax = plt.subplots() # plot belief update loss curves\n",
    "for i in range(20):\n",
    "    aa_user.append(user.step(ss[-1]))\n",
    "    b, ll_o = assistant._update_belief(theta_prev=assistant.b, \n",
    "                                 o_env=ss[-1], \n",
    "                                 o_user=aa_user[-1], debug=True)\n",
    "    assistant.b = b\n",
    "    plt.plot(ll_o)\n",
    "    bb.append(b)\n",
    "    ss.append(env.step_user(aa_user[-1]))\n",
    "    \n",
    "env.render_camera_view(ss[-1])\n",
    "cursor_t = np.array([s.cursor.x for s in ss])\n",
    "plt.plot(cursor_t, np.linspace(0,1,num=cursor_t.shape[0])*grid_size.h, 'r-x', label='$a_t$', zorder=100)\n",
    "plt.legend()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(np.array(bb).T)\n",
    "plt.plot([s.hovered_cell for s in ss], 'r-', label='cursor location')\n",
    "plt.plot([user.goal for s in ss], 'white', label='user goal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2b696-47b6-4aa8-a1d8-9dd7889ea556",
   "metadata": {},
   "source": [
    "## Acting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ff2a8-3cb5-43a5-9adf-81a4471b7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets explore which action the assistant selects given an environment state and a user action\n",
    "\n",
    "grid_size = Size(w=800, h=600)\n",
    "n_cells = Point(x=10, y=1)\n",
    "camera = Rect(xy=Point(x=0, y=0), w=grid_size.w, h=grid_size.h)\n",
    "cursor = Point(x=camera.w*0.5, y=camera.h*0.5)\n",
    "\n",
    "user = Fish(n_targets=n_cells.x*n_cells.y, \n",
    "            goal=7, \n",
    "            step_size = int(camera.w*0.05), \n",
    "            sigma=5e1)\n",
    "user.reset()\n",
    "env = GridViewer(grid_size=grid_size, n_cells=n_cells, camera_0=camera, cursor_0=cursor)\n",
    "assistant = ContinuousAOAssistant(env_model=env, \n",
    "                                  user_model=user,\n",
    "                                  use_pragmatic_value=False,\n",
    "                                  use_info_gain=True,\n",
    "                                  n_plans=64,\n",
    "                                  k=1,\n",
    "                                  n_o_samples=64, # observation samples for information gain\n",
    "                                  n_steps_o=10,\n",
    "                                  lr_o=2e0)\n",
    "\n",
    "s = env.reset()\n",
    "#env.render_world_view(s)\n",
    "\n",
    "user_action = 100.\n",
    "theta = np.eye(user.n_targets)[0] * 2 + \\\n",
    "        np.eye(user.n_targets)[1] * 2 + \\\n",
    "        np.eye(user.n_targets)[2] * 2 + \\\n",
    "        np.eye(user.n_targets)[3] * 2\n",
    "\n",
    "plt.subplots()\n",
    "plt.bar(range(user.n_targets), height=softmax(theta), label='prior')\n",
    "plt.legend()\n",
    "a, plans, p_pi, info_gains = assistant._select_action(theta_start=theta, o_env=s, debug=True)\n",
    "s1 = env.step_assistant(a=a[0])\n",
    "env.render_world_view(s)\n",
    "plt.title('State before action')\n",
    "env.render_camera_view(s1)\n",
    "plt.title('State after action')\n",
    "\n",
    "plt.subplots()\n",
    "aa = np.array([pi[0] for pi in plans])\n",
    "idx = np.argsort(aa)\n",
    "aa = aa[idx]\n",
    "p = np.array(p_pi)[idx]\n",
    "plt.plot(aa, p)\n",
    "ylim = plt.ylim()\n",
    "plt.ylim([0, ylim[1]])\n",
    "plt.title('action scoring')\n",
    "\n",
    "plt.subplots()\n",
    "aa = np.array([pi[0] for pi in plans])\n",
    "idx = np.argsort(aa)\n",
    "aa = aa[idx]\n",
    "ig = np.array(info_gains)[idx]\n",
    "plt.plot(aa, ig)\n",
    "ylim = plt.ylim()\n",
    "plt.ylim([0, ylim[1]])\n",
    "plt.title('information gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774c142-a138-4cfe-86b8-b1f068025545",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = Size(w=800, h=600)\n",
    "n_cells = Point(x=10, y=1)\n",
    "camera = Rect(xy=Point(x=0, y=0), w=grid_size.w, h=grid_size.h)\n",
    "cursor = Point(x=camera.w*0.5, y=camera.h*0.5)\n",
    "\n",
    "user = Fish(n_targets=n_cells.x*n_cells.y, \n",
    "            goal=9, \n",
    "            step_size = int(camera.w*0.05), \n",
    "            sigma=5e1)\n",
    "user.reset()\n",
    "\n",
    "env = GridViewer(grid_size=grid_size, n_cells=n_cells, camera_0=camera, cursor_0=cursor)\n",
    "assistant = ContinuousAOAssistant(env_model=env, \n",
    "                                  user_model=user, \n",
    "                                  n_plans=32,\n",
    "                                  k=1,\n",
    "                                  n_steps_o=5,\n",
    "                                  lr_o=2e0)\n",
    "assistant.reset()\n",
    "\n",
    "ss = [env.reset()]\n",
    "aa_user = []\n",
    "aa_assistant = []\n",
    "bb = [assistant.b]\n",
    "\n",
    "fig, ax = plt.subplots() # plot belief update loss curves\n",
    "for i in range(50):\n",
    "    aa_user.append(user.step(ss[-1]))\n",
    "    a_assistant, ll_o, max_a = assistant.step(o_env=ss[-1], \n",
    "                                              o_user=aa_user[-1], \n",
    "                                              debug=True)\n",
    "    print(i, 'assistant', a_assistant)\n",
    "    plt.plot(ll_o)\n",
    "    bb.append(assistant.b)\n",
    "    aa_assistant.append(a_assistant)\n",
    "    env.step_user(aa_user[-1])\n",
    "    ss.append(env.step_assistant(aa_assistant[-1]))\n",
    "    \n",
    "env.render_camera_view(ss[-1])\n",
    "cursor_t = np.array([s.cursor.x for s in ss])\n",
    "plt.plot(cursor_t, env.s_t.cursor.y + 50*(np.random.uniform(size=cursor_t.shape)-0.5), 'r-x', label='$a_t$', zorder=100)\n",
    "plt.legend()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(np.array(bb).T)\n",
    "plt.plot([s.hovered_cell for s in ss], 'r-', label='cursor location')\n",
    "plt.plot([user.goal for s in ss], 'white', label='user goal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38da97a-3aeb-475e-9213-fd4f83a2c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we evaluate the contribution of the assistant?\n",
    "# how often would the cursor have been outside the target had the camera not moved?\n",
    "# a. one-step counterfactual: did the intervention make an immediate positive difference\n",
    "# b. repeated full simulations: how quickly/ consistently does the user hover over target with/ without assistance\n",
    "\n",
    "# for each step, compute the successor state without assistant intervention\n",
    "ss_counterfactual = [env.transition_user(s=s, a=a) for s, a in zip(ss[:-1], aa_user)]\n",
    "hover = np.array([s.hovered_cell for s in ss[1:]])\n",
    "hover_counterfactual = np.array([s.hovered_cell for s in ss_counterfactual])\n",
    "\n",
    "plt.subplots()\n",
    "plt.title('One-step ahead effect of intervention on mean successes')\n",
    "a = np.cumsum(hover==user.goal)/(1+np.arange(hover.shape[0]))\n",
    "b = np.cumsum(hover_counterfactual==user.goal)/(1+np.arange(hover.shape[0]))\n",
    "plt.plot(a, 'x-', label='success with assistance')\n",
    "plt.plot(b, 'x-', label='success w/o assistance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206383c-78e8-4289-82f8-82cc74bb0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of the cursor relative to the center of the target cell\n",
    "c = env.cells[user.goal]\n",
    "c = (2*c.xy.x + c.w)/2\n",
    "c_a = np.array([env._absolute_cursor(s.cursor, s.camera, s.grid_size).x for s in ss[1:]])\n",
    "c_else = np.array([env._absolute_cursor(s.cursor, s.camera, s.grid_size).x for s in ss_counterfactual])\n",
    "# wrap-around distance\n",
    "def wrap_dist(target, x, width):\n",
    "   return np.minimum(np.abs(x - target), \n",
    "                   np.minimum(\n",
    "                   np.abs(x - width - target), \n",
    "                   np.abs(x + width - target)))\n",
    "d_a = wrap_dist(c, c_a, env.grid_size.w)\n",
    "d_else = wrap_dist(c, c_else, env.grid_size.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbc9c4-30ae-4a37-904a-074ba2af4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_a**2, label='assisted')\n",
    "plt.plot(d_else**2, label='unassisted')\n",
    "plt.xlabel('timestep t')\n",
    "plt.ylabel('squared distance from cell center')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a013195-edb3-4d88-b4e1-685a9544cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
