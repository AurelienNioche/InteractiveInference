{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89d8407-d5eb-4ce4-9f71-004cd6769140",
   "metadata": {},
   "source": [
    "# Continuous Actions and Continuous Observations\n",
    "\n",
    "In the previous two notebooks we developed active inference agents for environments with continuous action spaces and for environments with continuous observation spaces. The changes to the minimal environments and the minimal agent, where actions space and the observation space were discrete, required for each of these agent/ environment properties are non-overlapping, which makes it straight forward to combine these changes to create an active inference that can interact with environments where both of these spaces are continuous.\n",
    "\n",
    "We start by modifying the minimal environment to emit continous-valued observations and to accept continuous-valued actions. Then, we modify the components of the minimal agent that currently exploit the discreteness of observation and action spaces.\n",
    "\n",
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e1b0-212b-498d-95d9-21f701528e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de083b83-1eb5-4b41-a0a7-4b275437ddd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e81695-0a25-46b2-ace5-031efb0fdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0f069-e74b-4838-9b9f-58b57bde0ee1",
   "metadata": {},
   "source": [
    "# Continuous Action and Observation Environment\n",
    "\n",
    "Because here we simply merge modifications from the two previous notebooks we just present the final solution without analysing individual component changes again.\n",
    "\n",
    "## Full environment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07931aa3-c6db-4b74-b124-72bd2f4aa49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "class ContinuousAOEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (float): proportion of times food detected in finite sample.\n",
    "  actions(float): [-2, 2] intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 0, # where is the food?\n",
    "               sigma_move = 0.75, # Gaussian stdev around continuous move\n",
    "               o_sample_size=10, # observation Beta distribution parameter.\n",
    "               a_lims = [-2, 2], # maximum step in either direction.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.var_move = sigma_move**2\n",
    "    self.o_sample_size = o_sample_size\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.o_N = 2 # {False, True} indicating whether food has been found\n",
    "    self.a_lims = a_lims\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    \n",
    "    With continuous-valued actions, we can nolonger represent (1.) with a \n",
    "    single conditional probability table. However, we can generate one table of\n",
    "    size |S| x |S| for each continuous action value.\n",
    "    \"\"\"\n",
    "    self.d_s = self._signed_state_distances()\n",
    "    # self.p_s1_given_s_a(a=a) returns p[s, s1] for given a; slice of Matrix B\n",
    "    \n",
    "    \"\"\"\n",
    "    We pre-compute the conditional emission random variables (2.) here so agents \n",
    "    can access the true dynamics if required.\n",
    "    \"\"\"\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    \n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "  def _signed_state_distances(self):\n",
    "    s = np.arange(self.s_N)\n",
    "    other, this = np.meshgrid(s, s)\n",
    "    d = other - this\n",
    "    d1 = other - this + self.s_N\n",
    "    d2 = other - this - self.s_N\n",
    "    d[np.abs(d) > np.abs(d1)] = d1[np.abs(d) > np.abs(d1)]\n",
    "    d[np.abs(d) > np.abs(d2)] = d2[np.abs(d) > np.abs(d2)]\n",
    "    return d\n",
    "  \n",
    "  def _p_a_discrete_given_a(self, a):\n",
    "    # probability distribution of a discrete action (step) given a continuous\n",
    "    # action intent.\n",
    "    a = np.clip(a, self.a_lims[0], self.a_lims[1])\n",
    "    a_discrete = np.arange(2*self.s_N-1) - self.s_N + 1\n",
    "    p_a = np.exp(-0.5 * (a_discrete-a)**2 / self.var_move)\n",
    "    p_a[a_discrete > self.a_lims[1]] = 0\n",
    "    p_a[a_discrete < self.a_lims[0]] = 0\n",
    "    p_a = p_a/p_a.sum()\n",
    "    return a_discrete, p_a\n",
    "  \n",
    "  def p_s1_given_s_a(self, a):\n",
    "    \"\"\" computes transition probability p(s'| s, a) for specific a\n",
    "    \n",
    "    Note: this is provided for convenience in the agent; it is not used within\n",
    "    the environment simulation.\n",
    "\n",
    "    Returns:\n",
    "    p[s, s1] of size (s_N, s_N)\n",
    "    \"\"\"\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return p_a[self.d_s - a_d[0]]\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" initialises conditional random variables p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s] of size (s_N) with one scipy.stats.rv_continuous per state\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.minimum(\n",
    "                   np.abs(s - self.s_N - self.s_food), \n",
    "                   np.abs(s + self.s_N - self.s_food)))\n",
    "  \n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    mean = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    # continuous relaxation: proportion of food detected in finite sample\n",
    "    sample_size = self.o_sample_size\n",
    "    return np.array([beta(a=m*sample_size, b=(1-m)*sample_size) for m in mean])\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "      \n",
    "    a_discrete = self.sample_a(a)\n",
    "    self.s_t = (self.s_t + a_discrete) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return self.p_o_given_s[self.s_t].rvs()\n",
    "  \n",
    "  def sample_a(self, a):\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return np.random.choice(a_d, p=p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece325b-01db-4009-8c37-4c1be14a8bb9",
   "metadata": {},
   "source": [
    "## Random Agent Behavior\n",
    "\n",
    "To test the environment we simulate a random agent's interactions with it. Here, the random agent samples actions uniformly in the interval `[-2, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf95919-97c5-4022-8342-c2c0940984d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import continuous_action_and_observation_environment as caoe\n",
    "importlib.reload(caoe)\n",
    "\n",
    "env = caoe.ContinuousAOEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    sigma_move=0.75, # Gaussian noise around continuous move\n",
    "                    o_sample_size=100, # variance of observation decreases with increasing sample size.\n",
    "                    a_lims=[-3,3]) # maximum number of steps in either direction\n",
    "\n",
    "n_steps = 100\n",
    "ss, oo, aa = [], [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "oo.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.uniform(low=env.a_lims[0], high=env.a_lims[1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f47e9c-1610-4619-aade-e6721ce78e56",
   "metadata": {},
   "source": [
    "We inspect the sequence of states, actions and emissions during this interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3938da-54fc-4ac9-888b-7762bd032b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "for i in range(len(aa)):\n",
    "  ax[0].plot([i, i], [ss[i], ss[i]+aa[i]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] > 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "  \n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('state s')\n",
    "ax[0].legend(loc=1)\n",
    "\n",
    "ax[1].plot(np.array(oo))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb7b50-6c4f-430b-970b-3e7e09ec6a64",
   "metadata": {},
   "source": [
    "# Continuous Action and Observation Agent\n",
    "\n",
    "Because here we simply merge modifications from the two previous notebooks we just present the final solution without analysing individual component changes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db231d82-7e32-40b3-9698-bc06ea634b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "def kl(a, b):\n",
    "    \"\"\" Discrete KL-divergence \"\"\"\n",
    "    return (a * (np.log(a) - np.log(b))).sum()\n",
    "\n",
    "class ContinuousAOAgent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 target_state, \n",
    "                 shape_target=False, # smooth preference distribution using poirwise state distances\n",
    "                 n_plans=128, # number of plans rolled out during action selection\n",
    "                 k=2, # planning horizon\n",
    "                 n_o_samples=10, # observation samples for information gain\n",
    "                 use_info_gain=True, # score actions by info gain\n",
    "                 use_pragmatic_value=True, # score actions by pragmatic value\n",
    "                 select_max_pi=False, # sample plan (False), select max negEFE (True).\n",
    "                 n_steps_o=20, # optimization steps after new observation\n",
    "                 n_steps_a=20, # optimization steps after new action\n",
    "                 lr_o=4., # learning rate of optimization after new observation\n",
    "                 lr_a=4.): # learning rate of optimization after new action)\n",
    "        \n",
    "        self.env = env\n",
    "        self.target_state = target_state\n",
    "        self.shape_target = shape_target\n",
    "        self.n_plans = n_plans\n",
    "        self.k = k\n",
    "        self.n_o_samples = n_o_samples\n",
    "        self.use_info_gain = use_info_gain\n",
    "        self.use_pragmatic_value = use_pragmatic_value\n",
    "        self.select_max_pi = select_max_pi\n",
    "        self.n_steps_o = n_steps_o\n",
    "        self.n_steps_a = n_steps_a\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_o = lr_o\n",
    "        \n",
    "    def reset(self):\n",
    "        # initialize state preference\n",
    "        if self.shape_target:\n",
    "            self.b_star = np.ones(shape=self.env.s_N) - \\\n",
    "                          np.abs(self.env.d_s[self.target_state])\n",
    "        else:\n",
    "            self.b_star = np.eye(self.env.s_N)[self.target_state] * 10\n",
    "        self.log_p_c = np.log(softmax(self.b_star))\n",
    "        # initialize state prior as uniform\n",
    "        self.b = np.zeros(self.env.s_N)\n",
    "        \n",
    "    def step(self, o, debug=False):\n",
    "        if debug:\n",
    "            return self._step_debug(o)\n",
    "        \n",
    "        self.b = self._update_belief(theta_prev=self.b, o=o)\n",
    "        a = select_action(theta_start=self.b)[0] # pop first action of selected plan\n",
    "        self.b = self._update_belief_a(theta_prev=self.b, a=a)\n",
    "        return a\n",
    "    \n",
    "    def _step_debug(self, o):\n",
    "        self.b, ll_o = self._update_belief(theta_prev=self.b, o=o, debug=True)\n",
    "        a, plans, p_pi, _ = self._select_action(theta_start=self.b, debug=True)\n",
    "        max_a = plans[np.argmax(p_pi)][0]\n",
    "        a = a[0]\n",
    "        self.b, ll_a = self._update_belief_a(theta_prev=self.b, a=a, debug=True)\n",
    "        return a, ll_o, ll_a, max_a\n",
    "    \n",
    "    def _update_belief_a(self, theta_prev, a, debug=False):\n",
    "        # prior assumed to be expressed as parameters of the softmax (logits)\n",
    "        theta = torch.tensor(theta_prev)\n",
    "        q = torch.nn.Softmax(dim=0)(theta)\n",
    "\n",
    "        # this is the prior for the distribution at time t\n",
    "        q1 = torch.matmul(q, torch.tensor(self.env.p_s1_given_s_a(a=a)))\n",
    "\n",
    "        # initialize parameters of updated belief to uniform\n",
    "        theta1 = torch.zeros_like(theta, requires_grad=True)\n",
    "        loss = torch.nn.CrossEntropyLoss() # expects logits and target distribution.\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_a)\n",
    "        if debug:\n",
    "            ll = np.zeros(self.n_steps_a)\n",
    "\n",
    "        for i in range(self.n_steps_a):\n",
    "            l = loss(theta1, q1)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "    \n",
    "    def _update_belief(self, theta_prev, o, debug=False):\n",
    "        theta = torch.tensor(theta_prev)\n",
    "\n",
    "        # make p(s) from b\n",
    "        q = torch.nn.Softmax(dim=0)\n",
    "        p_o_given_s = torch.tensor([p.pdf(o) for p in self.env.p_o_given_s])\n",
    "        p = p_o_given_s * q(theta) # p(o|s)p(s)\n",
    "        log_p = torch.log(p)\n",
    "\n",
    "        # initialize updated belief with current belief\n",
    "        theta1 = torch.tensor(theta_prev, requires_grad=True)\n",
    "\n",
    "        # estimate loss\n",
    "        def forward():\n",
    "            q1 = q(theta1)\n",
    "            # free energy: KL[ q(s) || p(s, o) ]\n",
    "            fe = torch.sum(q1 * (torch.log(q1) - log_p))\n",
    "            return fe\n",
    "\n",
    "        optimizer = torch.optim.SGD([theta1], lr=self.lr_o)\n",
    "        ll = np.zeros(self.n_steps_o)\n",
    "        for i in range(self.n_steps_o):\n",
    "            l = forward()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if debug:\n",
    "                ll[i] = l.detach().numpy()\n",
    "\n",
    "        theta1 = theta1.detach().numpy()\n",
    "        if debug:\n",
    "            return theta1, ll\n",
    "\n",
    "        return theta1\n",
    "\n",
    "    def _select_action(self, theta_start, debug=False): # return plans, p of selecting each, and marginal p of actions\n",
    "        # sampling\n",
    "        a_lims = self.env.a_lims\n",
    "        plans = np.random.uniform(low=a_lims[0], high=a_lims[1], size=(self.n_plans, self.k)).tolist()\n",
    "        \n",
    "        # evaluate negative expected free energy of all plans\n",
    "        nefes = []\n",
    "        for pi in plans:\n",
    "          \n",
    "          if debug:\n",
    "            step_nefes, info_gains = self._rollout_step(theta_start, pi, \n",
    "                                                        debug=True)\n",
    "          else:\n",
    "            step_nefes = self._rollout_step(theta_start, pi)\n",
    "            \n",
    "          nefe = np.array(step_nefes).mean() # expected value over steps\n",
    "          nefes.append(nefe)\n",
    "\n",
    "        # compute probability of following each plan\n",
    "        p_pi = softmax(np.array(nefes)).tolist()\n",
    "        if self.select_max_pi:\n",
    "            a = plans[np.argmax(nefes)]\n",
    "        else:\n",
    "            a = plans[np.random.choice(len(plans), p=p_pi)]\n",
    "\n",
    "        if debug:\n",
    "            return a, plans, p_pi, info_gains\n",
    "\n",
    "        return a\n",
    "      \n",
    "    def _rollout_step(self, theta, pi, debug=False):\n",
    "        if pi == []:\n",
    "            return []\n",
    "\n",
    "        a, pi_rest = pi[0], pi[1:]\n",
    "        # Where will I be after taking action a?\n",
    "        theta1 = self._update_belief_a(theta, a=a) \n",
    "        q = softmax(theta1)\n",
    "        # Do I like being there?\n",
    "        pragmatic = np.dot(q, self.log_p_c)\n",
    "        # What might I observe after taking action a? (marginalize p(o, s) over s)\n",
    "        ss = np.random.choice(range(self.env.s_N), p=q, size=self.n_o_samples)\n",
    "        oo = [rv.rvs() for rv in self.env.p_o_given_s[ss]]\n",
    "        # Do I learn about s from observing o?\n",
    "        q_o = [softmax(self._update_belief(theta1, o=o)) for o in oo]\n",
    "        d_o = [kl(q_o_i, q) for q_o_i in q_o] # info gain for each observation\n",
    "        info_gain = np.mean(d_o) # expected value of info gain\n",
    "        # negative expected free energy for this timestep\n",
    "        nefe = self.use_pragmatic_value * pragmatic + \\\n",
    "               self.use_info_gain * info_gain\n",
    "        \n",
    "        # nefe for remainder of policy rollout\n",
    "        nefe_rest = self._rollout_step(theta1, pi_rest)\n",
    "        # concatenate expected free energy across future time steps\n",
    "        if debug:\n",
    "          return [nefe] + nefe_rest, d_o\n",
    "\n",
    "        return [nefe] + nefe_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96ea80-4f0c-4672-93e2-27871689db74",
   "metadata": {},
   "source": [
    "The code below iterates over all steps involved in the interaction between the environment and the active inference agent. In each interaction step, the agent updates its belief about the current state given a new observation and selects an action to minimise expected free energy. It then updates its belief assuming the selected action was taken and starts anew by updating its belief based on the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2ab78-0a66-4af9-bcc9-cbbf479a751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import continuous_action_and_observation_environment as caoe\n",
    "import continuous_action_and_observation_agent as caoa\n",
    "importlib.reload(caoe)\n",
    "importlib.reload(caoa)\n",
    "\n",
    "target_state = 4\n",
    "k = 4 # planning horizon; run time increases exponentially with planning horizon\n",
    "\n",
    "# runtime increases linearly with optimization steps during belief update\n",
    "n_steps_o = 20 # optimization steps updating belief after observation\n",
    "n_steps_a = 10 # optimization steps updating belief after action\n",
    "lr_o = 4. # learning rate updating belief after observation\n",
    "lr_a = 4. # learning rate updating belief after action\n",
    "\n",
    "render_losses = True\n",
    "\n",
    "env = caoe.ContinuousAOEnv(N=16, # number of states\n",
    "                           s_food=0, # location of the food source\n",
    "                           s_0=10, # starting location \n",
    "                           o_sample_size=3) # observation Beta distribution parameter.\n",
    "\n",
    "# visualise emission probability\n",
    "samples = [env.p_o_given_s[s].rvs(size=1000) for s in range(env.s_N)]\n",
    "df = pd.DataFrame(np.array(samples).T)\n",
    "sns.violinplot(df, cut=0, width=2.5)\n",
    "plt.xlabel('state s')\n",
    "plt.ylabel('p(o|s)')\n",
    "plt.title('Continuous environment emission probability')\n",
    "\n",
    "agent = caoa.ContinuousAOAgent(env=env, \n",
    "                               target_state=target_state,\n",
    "                               shape_target=False,\n",
    "                               k=k, \n",
    "                               n_plans=128,\n",
    "                               n_o_samples=1, # observation samples for information gain\n",
    "                               use_info_gain=True,\n",
    "                               use_pragmatic_value=True,\n",
    "                               select_max_pi=True,\n",
    "                               n_steps_o=n_steps_o, \n",
    "                               n_steps_a=n_steps_a, \n",
    "                               lr_a=lr_a, \n",
    "                               lr_o=lr_o)\n",
    "\n",
    "o = env.reset() # set state to starting state\n",
    "agent.reset() # initialize belief state and target state distribution\n",
    "\n",
    "ss = [env.s_t]\n",
    "bb = [agent.b]\n",
    "aa = []\n",
    "if render_losses:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].set_title('updates from actions')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('optimization step')\n",
    "    ax[1].set_title('updates from observations')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('optimization step')\n",
    "    \n",
    "for i in range(64):\n",
    "    a, ll_o, ll_a, max_a = agent.step(o, debug=True)\n",
    "    print(f\"step {i}, s: {env.s_t}, max b:{bb[-1].argmax()}, o: {o:.2f}, top a: {max_a}, a: {a}\")\n",
    "    if render_losses:\n",
    "        ax[0].plot(ll_a)\n",
    "        ax[1].plot(ll_o)\n",
    "    \n",
    "    o = env.step(a)\n",
    "    \n",
    "    ss.append(env.s_t)\n",
    "    bb.append(agent.b)\n",
    "    aa.append(a)\n",
    "\n",
    "\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "aa = np.array(aa)\n",
    "ss = np.array(ss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.imshow(np.array(bb).T, label='belief')\n",
    "\n",
    "for i in range(len(aa)):\n",
    "  plt.plot([i, i], [ss[i], ss[i]+aa[i]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETDOWN if aa[i] > 0 else CARETUP,\n",
    "             label=None if i > 0 else 'action')\n",
    "\n",
    "\n",
    "plt.plot(ss, label='state')\n",
    "plt.plot([0, len(ss)-1], [target_state]*2, label='target')\n",
    "plt.plot([0, len(ss)-1], [env.s_food]*2, 'w--', label='food')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fad82-d5fe-4df1-a66b-c985ef789604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
