{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89d8407-d5eb-4ce4-9f71-004cd6769140",
   "metadata": {},
   "source": [
    "# Continuous Actions and Continuous Observations\n",
    "\n",
    "In the previous two notebooks we developed active inference agents for environments with continuous action spaces and for environments with continuous observation spaces. The changes to the minimal environments and the minimal agent, where actions space and the observation space were discrete, required for each of these agent/ environment properties are non-overlapping, which makes it straight forward to combine these changes to create an active inference that can interact with environments where both of these spaces are continuous.\n",
    "\n",
    "We start by modifying the minimal environment to emit continous-valued observations and to accept continuous-valued actions. Then, we modify the components of the minimal agent that currently exploit the discreteness of observation and action spaces.\n",
    "\n",
    "#### Housekeeping (run once per kernel restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e1b0-212b-498d-95d9-21f701528e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de083b83-1eb5-4b41-a0a7-4b275437ddd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e81695-0a25-46b2-ace5-031efb0fdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import CARETUP, CARETDOWN\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0f069-e74b-4838-9b9f-58b57bde0ee1",
   "metadata": {},
   "source": [
    "# Continuous Action and Observation Environment\n",
    "\n",
    "Because here we simply merge modifications from the two previous notebooks we just present the final solution without analysing individual component changes again.\n",
    "\n",
    "## Full environment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07931aa3-c6db-4b74-b124-72bd2f4aa49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "class ContinuousAOEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (float): proportion of times food detected in finite sample.\n",
    "  actions(float): [-2, 2] intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 0, # where is the food?\n",
    "               sigma_move = 0.75, # Gaussian stdev around continuous move\n",
    "               o_sample_size=10, # observation Beta distribution parameter.\n",
    "               a_lims = [-2, 2], # maximum step in either direction.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.var_move = sigma_move**2\n",
    "    self.o_sample_size = o_sample_size\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.o_N = 2 # {False, True} indicating whether food has been found\n",
    "    self.a_lims = a_lims\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    \n",
    "    With continuous-valued actions, we can nolonger represent (1.) with a \n",
    "    single conditional probability table. However, we can generate one table of\n",
    "    size |S| x |S| for each continuous action value.\n",
    "    \"\"\"\n",
    "    self.d_s = self._signed_state_distances()\n",
    "    # self.p_s1_given_s_a(a=a) returns p[s, s1] for given a; slice of Matrix B\n",
    "    \n",
    "    \"\"\"\n",
    "    We pre-compute the conditional emission random variables (2.) here so agents \n",
    "    can access the true dynamics if required.\n",
    "    \"\"\"\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    \n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "  def _signed_state_distances(self):\n",
    "    s = np.arange(self.s_N)\n",
    "    other, this = np.meshgrid(s, s)\n",
    "    d = other - this\n",
    "    d1 = other - this + self.s_N\n",
    "    d2 = other - this - self.s_N\n",
    "    d[np.abs(d) > np.abs(d1)] = d1[np.abs(d) > np.abs(d1)]\n",
    "    d[np.abs(d) > np.abs(d2)] = d2[np.abs(d) > np.abs(d2)]\n",
    "    return d\n",
    "  \n",
    "  def _p_a_discrete_given_a(self, a):\n",
    "    # probability distribution of a discrete action (step) given a continuous\n",
    "    # action intent.\n",
    "    a = np.clip(a, self.a_lims[0], self.a_lims[1])\n",
    "    a_discrete = np.arange(2*self.s_N-1) - self.s_N + 1\n",
    "    p_a = np.exp(-0.5 * (a_discrete-a)**2 / self.var_move)\n",
    "    p_a[a_discrete > self.a_lims[1]] = 0\n",
    "    p_a[a_discrete < self.a_lims[0]] = 0\n",
    "    p_a = p_a/p_a.sum()\n",
    "    return a_discrete, p_a\n",
    "  \n",
    "  def p_s1_given_s_a(self, a):\n",
    "    \"\"\" computes transition probability p(s'| s, a) for specific a\n",
    "    \n",
    "    Note: this is provided for convenience in the agent; it is not used within\n",
    "    the environment simulation.\n",
    "\n",
    "    Returns:\n",
    "    p[s, s1] of size (s_N, s_N)\n",
    "    \"\"\"\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return p_a[self.d_s - a_d[0]]\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" initialises conditional random variables p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s] of size (s_N) with one scipy.stats.rv_continuous per state\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.minimum(\n",
    "                   np.abs(s - self.s_N - self.s_food), \n",
    "                   np.abs(s + self.s_N - self.s_food)))\n",
    "  \n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    mean = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    # continuous relaxation: proportion of food detected in finite sample\n",
    "    sample_size = self.o_sample_size\n",
    "    return np.array([beta(a=m*sample_size, b=(1-m)*sample_size) for m in mean])\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "      \n",
    "    a_discrete = self.sample_a(a)\n",
    "    self.s_t = (self.s_t + a_discrete) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return self.p_o_given_s[self.s_t].rvs()\n",
    "  \n",
    "  def sample_a(self, a):\n",
    "    a_d, p_a = self._p_a_discrete_given_a(a=a)\n",
    "    return np.random.choice(a_d, p=p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece325b-01db-4009-8c37-4c1be14a8bb9",
   "metadata": {},
   "source": [
    "## Random Agent Behavior\n",
    "\n",
    "To test the environment we simulate a random agent's interactions with it. Here, the random agent samples actions uniformly in the interval `[-2, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf95919-97c5-4022-8342-c2c0940984d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import continuous_action_and_observation_environment as caoe\n",
    "importlib.reload(caoe)\n",
    "\n",
    "env = caoeContinuousAOEnv(N=16, # number of states\n",
    "                    s_food=0, # location of the food source\n",
    "                    sigma_move=0.75, # Gaussian noise around continuous move\n",
    "                    o_sample_size=100, # variance of observation decreases with increasing sample size.\n",
    "                    a_lims=[-3,3]) # maximum number of steps in either direction\n",
    "\n",
    "n_steps = 100\n",
    "ss, oo, aa = [], [], []\n",
    "\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "oo.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.uniform(low=env.a_lims[0], high=env.a_lims[1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f47e9c-1610-4619-aade-e6721ce78e56",
   "metadata": {},
   "source": [
    "We inspect the sequence of states, actions and emissions during this interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3938da-54fc-4ac9-888b-7762bd032b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 12))\n",
    "ax[0].plot(ss, label='agent state $s_t$')\n",
    "ax[0].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='food source', linewidth=1)\n",
    "for i in range(len(aa)):\n",
    "  ax[0].plot([i, i], [ss[i], ss[i]+aa[i]], \n",
    "             color='orange', \n",
    "             linewidth=0.5,\n",
    "             marker= CARETUP if aa[i] > 0 else CARETDOWN,\n",
    "             label=None if i > 0 else 'action')\n",
    "  \n",
    "ax[0].set_xlabel('timestep t')\n",
    "ax[0].set_ylabel('state s')\n",
    "ax[0].legend(loc=1)\n",
    "\n",
    "ax[1].plot(np.array(oo))\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].set_ylabel('observation o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e160-2d21-4416-90d0-8a472fe4457e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
