{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZb1q82bcSY6"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE56XqOllc2u"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeO0jC8FcRqm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOgu7cO1WGmq"
   },
   "source": [
    "# Minimal Agent\n",
    "\n",
    "This environment/ agent was adapted from \n",
    "\n",
    "**Paper** https://arxiv.org/abs/1503.04187\n",
    "\n",
    "**GitHub**\n",
    "https://github.com/vschaik/Active-Inference/blob/78e37aca669fef611e7c82534142139715382c92/IFE.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjozl09n0daH"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36tsnH2eWEoP"
   },
   "outputs": [],
   "source": [
    "# environment\n",
    "class MinimalEnv(object):\n",
    "  \"\"\" Wrap-around 1D state space with single food source.\n",
    "  \n",
    "  The probability of sensing food at locations near the food source decays \n",
    "  exponentially with increasing distance.\n",
    "  \n",
    "  state (int): 1 of N discrete locations in 1D space.\n",
    "  observation (bool): food detected yes/ no.\n",
    "  actions(int): {-1, 1} intention to move left or right.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               N = 16, # how many discrete locations can the agent reside in\n",
    "               s_0 = 0, # where does the agent start each episode?\n",
    "               s_food = 8, # where is the food?\n",
    "               p_move = 0.75, # execute intent with p, else don't move.\n",
    "               p_o_max = 0.9, # maximum probability of sensing food\n",
    "               o_decay = 0.2 # decay rate of observing distant food source\n",
    "               ):\n",
    "    \n",
    "    self.o_decay = o_decay\n",
    "    self.p_move = p_move\n",
    "    self.p_o_max = p_o_max\n",
    "    self.s_0 = s_0\n",
    "    self.s_food = s_food\n",
    "    self.s_N = N\n",
    "    self.o_N = 2 # {False, True} indicating whether food has been found\n",
    "    self.a_N = 2 # {0, 1} to move left/ right in wrap-around 1D state-space\n",
    "    \"\"\"\n",
    "    environment dynamics are governed by two probability distributions\n",
    "    1. state transition probability p(s'|s, a)\n",
    "    2. emission/ observation probability p(o|s)\n",
    "    although we only need to be able to sample from these distributions to \n",
    "    implement the environment, we pre-compute the full conditional probability\n",
    "    ables here so agents can access the true dynamics if required.\n",
    "    \"\"\"\n",
    "    self.p_o_given_s = self.emission_probability() # Matrix A\n",
    "    self.p_s1_given_s_a = self.transition_dynamics() # Matrix B\n",
    "    self.s_t = None # state at current timestep\n",
    "\n",
    "\n",
    "  def transition_dynamics(self):\n",
    "    \"\"\" computes transition probability p(s'| s, a) \n",
    "    \n",
    "    Returns:\n",
    "    p[s, a, s1] of size (s_N, a_N, s_N)\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.zeros((self.s_N, self.a_N, self.s_N))\n",
    "    p[:,0,:] = self.p_move * np.roll(np.identity(self.s_N), -1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    p[:,1,:] = self.p_move * np.roll(np.identity(self.s_N), 1, axis=1) \\\n",
    "              + (1-self.p_move) * np.identity(self.s_N)\n",
    "    return p\n",
    "\n",
    "  def emission_probability(self):\n",
    "    \"\"\" computes conditional probability table p(o|s). \n",
    "    \n",
    "    Returns:\n",
    "    p[s, o] of size (s_N, o_N)\n",
    "    \"\"\"\n",
    "    s = np.arange(self.s_N)\n",
    "    # distance from food source\n",
    "    d = np.minimum(np.abs(s - self.s_food), \n",
    "                   np.abs(s + self.s_N - self.s_food))\n",
    "    p = np.zeros((self.s_N, self.o_N))\n",
    "    # exponentially decaying concentration ~ probability of detection\n",
    "    p[:,1] = self.p_o_max * np.exp(-self.o_decay * d)\n",
    "    p[:,0] = 1 - p[:,1]\n",
    "    return p\n",
    "\n",
    "  def reset(self):\n",
    "    self.s_t = self.s_0\n",
    "    return self.sample_o()\n",
    "\n",
    "  def step(self, a):\n",
    "    if (self.s_t is None):\n",
    "      print(\"Warning: reset environment before first action.\")\n",
    "      self.reset()\n",
    "\n",
    "    if (a not in [0, 1]):\n",
    "      print(\"Warning: only permitted actions are [0, 1].\")\n",
    "\n",
    "    # convert action index to action\n",
    "    a = [-1,1][a]\n",
    "\n",
    "    if np.random.random() < self.p_move:\n",
    "      self.s_t = (self.s_t + a) % self.s_N\n",
    "    return self.sample_o()\n",
    "\n",
    "  def sample_o(self):\n",
    "    return np.random.random() < self.p_o_given_s[self.s_t,1]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05w4-YT_zSjS"
   },
   "source": [
    "Sampling environment interactions of a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "6wzmtzlcWjC8",
    "outputId": "3a4ca89e-44ee-43c2-9673-a552c0df11c7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "n_steps = 100\n",
    "\n",
    "env = MinimalEnv()\n",
    "ss, os = [], []\n",
    "o = env.reset()\n",
    "ss.append(env.s_t)\n",
    "os.append(o)\n",
    "\n",
    "for i in range(n_steps):\n",
    "  a = np.random.choice([0,1]) # random agent\n",
    "  o = env.step(a)\n",
    "  ss.append(env.s_t)\n",
    "  os.append(o)\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(16, 12))\n",
    "ax[0].plot(env.p_o_given_s[:,1])\n",
    "ax[0].set_xlabel('state')\n",
    "ax[0].set_ylabel('$p(o=True|s)$')\n",
    "ax[1].plot(ss, label='agent state $s_t$')\n",
    "ax[1].plot(np.ones_like(ss) * env.s_food, \n",
    "           'r--', label='target state $s^*$', linewidth=1)\n",
    "ax[1].set_xlabel('timestep t')\n",
    "ax[1].legend()\n",
    "ax[2].plot(np.array(os))\n",
    "ax[2].set_xlabel('timestep t')\n",
    "ax[2].set_ylabel('observation o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE6C0xOc0VaE"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJHRzUKJdiuG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# agent\n",
    "\"\"\"\n",
    "- the agent has an internal brain state b\n",
    "- it receives a sensory state o in each timestep\n",
    "- it chooses an action a to minimise variational free energy\n",
    "- it updates its brain state to b' based on b, a, and s\n",
    "\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max())\n",
    "  return e / e.sum()\n",
    "\n",
    "def KL(a, b):\n",
    "  \"\"\" Discrete KL divergence.\"\"\"\n",
    "  return np.dot(a, (np.log(a) - np.log(b)))\n",
    "\n",
    "def get_b_star(s_star=0, s_N=16):\n",
    "  b = np.zeros(s_N)\n",
    "  b[s_star] = 10\n",
    "  return b\n",
    "\n",
    "class MinimalAgent(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               p_s1_given_s_a, # true environment transition probability\n",
    "               p_o_given_s, # true environment emission probability\n",
    "               b_star, # logits of desired state distribution (thought desire was expressed in terms of sensor states?)\n",
    "               a_N=2, # number of discrete actions\n",
    "               b_N=16, # number of internal states (tabular representation of p(b))\n",
    "               ):\n",
    "    \n",
    "    # environment dynamics\n",
    "    self.p_s1_given_s_a = p_s1_given_s_a\n",
    "    self.p_o_given_s = p_o_given_s\n",
    "    self.a_N = a_N\n",
    "    \n",
    "    # belief state\n",
    "    self.b_N = b_N # number of belief states\n",
    "    self.b_star = b_star # desired distribution over belief states\n",
    "    self.b_t = None # current belief state (undefined before reset)\n",
    "\n",
    "  def reset(self):\n",
    "    self.b_t = np.zeros(self.b_N) # uniform belieft at start\n",
    "\n",
    "  def act(self, o):\n",
    "    min_fe = None\n",
    "    argmin_fe = None\n",
    "\n",
    "    # evaluate policies by evaluating single next action\n",
    "    # - more generally, we evaluate trajectories of actions (pi: a_0, ..., a_tau)\n",
    "    # - we pick actions by sampling from softmax(G_pi)\n",
    "    for a in range(self.a_N):\n",
    "      # Note: we use action indices to represent actions (not {-1, 1})\n",
    "      fe = self.free_energy(self.b_star, o, a)\n",
    "      if (min_fe is None) or (fe < min_fe):\n",
    "        min_fe = fe\n",
    "        argmin_fe = a\n",
    "\n",
    "      \n",
    "    return argmin_fe\n",
    "\n",
    "  @staticmethod\n",
    "  def q(b):\n",
    "    \"\"\" Variational distribution of environment state s given belief state b.\n",
    "        p(s|b)\n",
    "\n",
    "      (model_encoding, variational_density)\n",
    "    \"\"\"\n",
    "    return softmax(b)\n",
    "\n",
    "  @classmethod\n",
    "  def dq(self, b):\n",
    "    \"\"\" Derivative of the variational distribution.\n",
    "     (model_encoding_derivative)\n",
    "    \"\"\"\n",
    "    q = self.q(b)\n",
    "    # Softmax derivative\n",
    "    return np.diag(q) - np.outer(q, q)\n",
    "\n",
    "  def generative_density(self, b, o, a):\n",
    "    \"\"\"\n",
    "    Next state prediction from generative model.\n",
    "    Here, the generative model is equal to the true environment dynamics.\n",
    "    (generative_density)\n",
    "\n",
    "    P(s', o | b, a) = Sum_over_s(P(s' | a, s) * P(o | s) * P(s | b))\n",
    "    s' only depends on a and s, o only depends on s, and s only depends on b.\n",
    "\n",
    "    Agent's prediction of next state probability given belief state and action\n",
    "    (calculated separately for both sensory states).\n",
    "    \"\"\"\n",
    "\n",
    "    # generative model of the next state p(s1, o | b, a)\n",
    "    # todo: adapt to return joint for both observations\n",
    "    p_o_s_given_b = self.q(self.b_t) * self.p_o_given_s[:,o] # joint prob p(o, s| b)\n",
    "    p_s1_o_given_b_a = np.dot(p_o_s_given_b, self.p_s1_given_s_a[:,a,:])\n",
    "    return p_s1_o_given_b_a\n",
    "\n",
    "  def free_energy(self, b_star, o, a):\n",
    "    # estimate of expected free energy, used for action selection\n",
    "    q = self.q(b_star) # where I want to be\n",
    "    p = self.generative_density(self.b_t, o, a=a) # where I get to taking action a\n",
    "    return KL(q, p)\n",
    "\n",
    "  def update_state(self, o, a, n_steps, lr=1.0):\n",
    "    # internal belief state at time t+1 can be initialised\n",
    "    # a) uniformly (expressing minimal knowledge about the future)\n",
    "    # b) biased towards the current state (assuming small changes)\n",
    "    # c) by updating current belief according to current world model\n",
    "    #    this assumes that we know the inverse q^-1(b|s) which, in general, we don't\n",
    "    b_prime = np.copy(self.b_t) # (b), alternatively np.zeros(self.b_N) (a)\n",
    "\n",
    "    # posterior joint of next state and last observation given  last action \n",
    "    # and last belief state. This is constant across update iterations\n",
    "    p = self.generative_density(self.b_t, o, a)\n",
    "    #plt.plot(p, label=\"$p(s', o | b, a)$\")\n",
    "\n",
    "    for i in range(n_steps):\n",
    "      q = self.q(b_prime)\n",
    "      # KL(q, p)\n",
    "      #F = np.dot(q, (np.log(q) - np.log(p))\n",
    "      \n",
    "      # free energy gradient wrt belief state\n",
    "      dq = self.dq(b_prime)\n",
    "      Y = 1 + (np.log(q) - np.log(p))\n",
    "      db = np.dot(dq, Y)\n",
    "\n",
    "      b_prime -= lr * db\n",
    "\n",
    "    self.b_t = b_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xNCvtsWtHpNi",
    "outputId": "3bf6149e-9303-447c-d023-50b707af2ff6"
   },
   "outputs": [],
   "source": [
    "# environment\n",
    "p_o_max = 4.0**(-1.0/16)\n",
    "o_decay = np.log(4)/16\n",
    "s_food = 8\n",
    "p_move=0.75\n",
    "s_0 = 0\n",
    "env = MinimalEnv(p_o_max=p_o_max,\n",
    "                 o_decay=o_decay,\n",
    "                 s_food=s_food,\n",
    "                 p_move=p_move,\n",
    "                 s_0=s_0)\n",
    "\n",
    "# agent\n",
    "target_position = 11\n",
    "agent = MinimalAgent(p_s1_given_s_a=env.p_s1_given_s_a, \n",
    "                     p_o_given_s=env.p_o_given_s,\n",
    "                     a_N=2,\n",
    "                     b_star = get_b_star(target_position))\n",
    "agent.reset()\n",
    "\n",
    "# simulation\n",
    "n_epochs = 100 # environment steps\n",
    "n_steps = 100 # gradient descent steps per epoch\n",
    "lr = 1.0\n",
    "\n",
    "ss, oo, aa, bb = [], [], [], []\n",
    "o = int(env.reset())\n",
    "for env_step in range(n_epochs):\n",
    "\n",
    "  if env_step % 10 == 0:\n",
    "    print(f'Env step {env_step}')\n",
    "\n",
    "  # pick action that minimises free energy\n",
    "  a = agent.act(o)\n",
    "\n",
    "  ss.append(env.s_t)\n",
    "  oo.append(o)\n",
    "  aa.append(a)\n",
    "  bb.append(softmax(agent.b_t))\n",
    "\n",
    "  # update belief to minimise free energy\n",
    "  agent.update_state(o, a, n_steps, lr=lr)\n",
    "\n",
    "  # take action in environment\n",
    "  o = int(env.step(a))\n",
    "\n",
    "# display trace\n",
    "fig, ax = plt.subplots(3, 1, figsize=(16, 6*3))\n",
    "\n",
    "plt.sca(ax[0])\n",
    "plt.plot(ss, label='$s_t$')\n",
    "plt.plot(np.ones_like(ss)*target_position, label='target')\n",
    "plt.legend()\n",
    "plt.xlim(0,n_epochs)\n",
    "\n",
    "plt.sca(ax[1])\n",
    "bb = np.array(bb)\n",
    "plt.imshow(bb.T,\n",
    "              interpolation=\"nearest\", \n",
    "              aspect = \"auto\", \n",
    "              vmin = 0, vmax = bb.max(), \n",
    "              cmap = \"viridis\", origin='lower')\n",
    "plt.plot([0,n_steps], [target_position, target_position], 'r--', label='target')\n",
    "plt.plot([0,n_steps], [s_food, s_food], 'w--', label='food')\n",
    "plt.plot(ss, label='$s_t$')\n",
    "plt.xlim(0,n_epochs)\n",
    "plt.legend()\n",
    "\n",
    "plt.sca(ax[2])\n",
    "plt.plot(aa, label='action')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQjCh7ORjV-j"
   },
   "source": [
    "# Open Questions\n",
    "\n",
    "- The Minimal Agent expresses desire as a distribution over brain states (internal model of world states); I was expecting this to be defined as a distribution over sensor states instead (want to always find food or want to find food 30% of the time).\n",
    "\n",
    "- the initial guess b_prime before gradient descent exploits the simplicity of the model (sigmoid of table), which does not hold in general; use b' = b or b'=uniform to start with instead.\n",
    "\n",
    "\n",
    "# Open Tasks\n",
    "\n",
    "- **Agree on notation**\n",
    "\n",
    "- The minimal agent assumes the true environment dynamics to be known; in RL we usually learn an implicit or explicit model from data. Learn internal model of the world through interaction (Chapter 7.4)\n",
    "\n",
    "- Make belief non-tabular (e.g., Gaussian or a neural network)\n",
    "\n",
    "- Define preference in terms of sensory states instead of belief states\n",
    "\n",
    "- Attack a problem where planning (beyond single-step dynamics) is required or highly beneficial.\n",
    "\n",
    "- Revisit collaborative Active Inference paper\n",
    "\n",
    "- Revisit pointing without a pointer paper\n",
    "\n",
    "- Implement pointing without a pointer as collaborative active inference\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Zge3z_wW9Ex"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txH0PVtZNCDT"
   },
   "source": [
    "# Pointing without a pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4JfClwIZ-Zm"
   },
   "outputs": [],
   "source": [
    "class User(object):\n",
    "\n",
    "  def __init__(self, n_targets=2):\n",
    "    \n",
    "    self.n_targets = n_targets\n",
    "    self.goal = None # index of preferred target, assigned during reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.intent = np.random.choice(self.n_targets)\n",
    "\n",
    "  def step(self, observation):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:active_inference]",
   "language": "python",
   "name": "conda-env-active_inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
