{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a25494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3730f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_slc_n_epochs = 100\n",
    "action_slc_lr = 0.1\n",
    "belief_updt_n_epochs = 100\n",
    "belief_updt_lr = 0.1\n",
    "n_target = 2\n",
    "mvt_amplitude = 3\n",
    "max_coord = torch.tensor([300., 600.])\n",
    "n_step_per_rollout = 1\n",
    "n_spl_epist_val = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter(torch.zeros(n_target))\n",
    "\n",
    "opt = torch.optim.Adam([a, ], lr=belief_updt_lr)\n",
    "\n",
    "for step in range(action_slc_n_epochs):\n",
    "\n",
    "    old_a = a.clone()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    action = torch.sigmoid(a)*360\n",
    "    total_efe = 0\n",
    "    for _ in range(n_rollout):\n",
    "        efe_rollout = 0\n",
    "\n",
    "        x_rollout = x.clone()\n",
    "        b_rollout = b.clone()\n",
    "\n",
    "        action_plan = torch.zeros((n_step_per_rollout, n_target))\n",
    "        action_plan[0] = action\n",
    "        if n_step_per_rollout > 1:\n",
    "            action_plan[1:] = torch.rand((n_step_per_rollout-1, n_target)) * 360\n",
    "\n",
    "        for step in range(n_step_per_rollout):\n",
    "\n",
    "            a = action_plan[step]\n",
    "            prev_x = x.clone()\n",
    "\n",
    "            pos = x\n",
    "            for i in range(n_target):\n",
    "                angle = a[i]\n",
    "                x_prime = 1.0\n",
    "                if 90 < angle <= 270:\n",
    "                    x_prime *= -1\n",
    "\n",
    "                y_prime = torch.tan(torch.deg2rad(angle)) * x_prime\n",
    "\n",
    "                norm = mvt_amplitude / torch.sqrt(y_prime**2 + x_prime**2)\n",
    "                movement = torch.tensor([x_prime, y_prime]) * norm\n",
    "                pos[i] += movement\n",
    "\n",
    "            for coord in range(2):\n",
    "                for target in range(self.n_target):\n",
    "                    if pos[target, coord] > max_coord[coord]:\n",
    "                        pos[target, coord] = max_coord[coord]\n",
    "\n",
    "            epistemic_value = 0\n",
    "            b_prime_spl = torch.zeros((n_spl_epist_val, n_target))\n",
    "            # Not sure about from which b to sample from?\n",
    "            goals = torch.multinomial(torch.softmax(b, dim=0), n_spl_epist_val, replacement=True)\n",
    "            for i in range(n_spl_epist_val):\n",
    "                goal = goals[i]\n",
    "                y = user_sim_act(goal=goal, positions=x, prev_positions=prev_x)\n",
    "                \n",
    "                logp_y = user_logp_action(positions=x, action=y)\n",
    "\n",
    "                logq = torch.log_softmax(b - b.max(), dim=0)\n",
    "                logp_yq = logq + logp_y\n",
    "\n",
    "                # --- belief update --- #\n",
    "                \n",
    "                b_prime = torch.nn.Parameter(b.clone())\n",
    "                opt = torch.optim.Adam([b_prime, ], lr=0.1)\n",
    "\n",
    "                for step in range(belief_updt_lr):\n",
    "\n",
    "                    old_b_prime = b_prime.clone()\n",
    "\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                    b_prime_scaled = b_prime.clone()\n",
    "                    b_prime_scaled -= b_prime_scaled.max() \n",
    "                    q_prime = torch.softmax(b_prime_scaled, dim=0)\n",
    "                    kl_div = torch.sum(q_prime * (q_prime.log() - logp_yq))\n",
    "                    kl_div.backward(retain_graph=True)\n",
    "                    opt.step()\n",
    "\n",
    "                    if torch.isclose(old_b_prime, b_prime).all():\n",
    "                        break\n",
    "                \n",
    "                # ------------------- #\n",
    "                \n",
    "                b_prime_spl[i] = b_prime\n",
    "                epistemic_value += kl_div\n",
    "\n",
    "            epistemic_value /= n_spl_epist_val\n",
    "            b_rollout = b_prime_spl[torch.randint(n_spl_epist_val, (1, ))].squeeze()\n",
    "            \n",
    "            efe_step = - epistemic_value  # Need to add intrinsic value\n",
    "\n",
    "            efe_rollout += decay_factor**step * efe_step\n",
    "\n",
    "        total_efe += efe_rollout\n",
    "\n",
    "    total_efe /= n_rollout\n",
    "    total_efe.backward(retain_graph=True)\n",
    "    opt.step()\n",
    "\n",
    "    if torch.isclose(old_a, a).all():\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
