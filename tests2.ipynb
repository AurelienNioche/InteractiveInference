{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f72390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf95539",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_slc_n_epochs = 100\n",
    "action_slc_lr = 0.1\n",
    "belief_updt_n_epochs = 100\n",
    "belief_updt_lr = 0.1\n",
    "n_target = 2\n",
    "mvt_amplitude = 3\n",
    "max_coord = torch.tensor([300., 600.])\n",
    "n_rollout = 1\n",
    "n_step_per_rollout = 1\n",
    "n_spl_epist_val = 5\n",
    "user_sigma = 5\n",
    "decay_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab97668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 150.],\n",
       "        [225., 450.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0.25, 0.25], [0.75, 0.75]]) * max_coord\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e17f38b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(n_target)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb7c58ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x12b220a30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb266a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter(torch.zeros(n_target))\n",
    "\n",
    "opt = torch.optim.Adam([a, ], lr=belief_updt_lr)\n",
    "\n",
    "for step in range(action_slc_n_epochs):\n",
    "\n",
    "    old_a = a.clone()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    action = torch.sigmoid(a)*360\n",
    "    total_efe = 0\n",
    "    for _ in range(n_rollout):\n",
    "        efe_rollout = 0\n",
    "\n",
    "        x_rollout = x.clone()\n",
    "        b_rollout = b.clone()\n",
    "\n",
    "        action_plan = torch.zeros((n_step_per_rollout, n_target))\n",
    "        action_plan[0] = action\n",
    "        if n_step_per_rollout > 1:\n",
    "            action_plan[1:] = torch.rand((n_step_per_rollout-1, n_target)) * 360\n",
    "\n",
    "        for step in range(n_step_per_rollout):\n",
    "\n",
    "            a = action_plan[step]\n",
    "            \n",
    "            # ---- Update positions based on action ---------------------------------------------\n",
    "            \n",
    "            x_rollout_previous = x_rollout.clone()\n",
    "\n",
    "            for i in range(n_target):\n",
    "                angle = a[i]\n",
    "                x_prime = 1.0\n",
    "                if 90 < angle <= 270:\n",
    "                    x_prime *= -1\n",
    "\n",
    "                y_prime = torch.tan(torch.deg2rad(angle)) * x_prime\n",
    "\n",
    "                norm = mvt_amplitude / torch.sqrt(y_prime**2 + x_prime**2)\n",
    "                movement = torch.tensor([x_prime, y_prime]) * norm\n",
    "                x_rollout[i] += movement\n",
    "\n",
    "            for coord in range(2):\n",
    "                for target in range(n_target):\n",
    "                    if x_rollout[target, coord] > max_coord[coord]:\n",
    "                        x_rollout[target, coord] = max_coord[coord]\n",
    "\n",
    "            # ------------------------------------------------------------------------------            \n",
    "            # Evaluate epistemic value -----------------------------------------------------\n",
    "            # ------------------------------------------------------------------------------\n",
    "            \n",
    "            epistemic_value = 0\n",
    "            b_prime_spl = torch.zeros((n_spl_epist_val, n_target))\n",
    "            q = torch.softmax(b_rollout - b_rollout.max(), dim=0)\n",
    "            logq = q.log()\n",
    "            \n",
    "            # Sample from assistant beliefs ---------------------------------------\n",
    "            goals = torch.multinomial(q, n_spl_epist_val, replacement=True)\n",
    "            \n",
    "            for i in range(n_spl_epist_val):\n",
    "                \n",
    "                goal = goals[i]\n",
    "                \n",
    "                # Simulate action based on goal ----------------------------------------\n",
    "                \n",
    "                delta = x_rollout[goal] - x_rollout_previous[goal]\n",
    "                noise = torch.randn(2) * user_sigma\n",
    "                y = delta + noise\n",
    "                \n",
    "                # Compute log probability of user action given a specific goal in mind -------\n",
    "                logp_y = torch.zeros(n_target)\n",
    "                for target in range(n_target):\n",
    "                    for coord in range(2):\n",
    "                        d = x_rollout[target, coord] - x_rollout_previous[target, coord]\n",
    "                        logp_coord = torch.distributions.Normal(d, user_sigma).log_prob(y[coord])\n",
    "                        logp_y[target] += logp_coord\n",
    "\n",
    "                logp_yq = logq + logp_y\n",
    "\n",
    "                # --- Revise beliefs --------------------------------------------------\n",
    "                \n",
    "                b_prime = torch.nn.Parameter(b_rollout.clone())\n",
    "                opt = torch.optim.Adam([b_prime, ], lr=belief_updt_lr)\n",
    "\n",
    "                for step in range(belief_updt_n_epochs):\n",
    "\n",
    "                    old_b_prime = b_prime.clone()\n",
    "\n",
    "                    opt.zero_grad()\n",
    "                    with torch.no_grad():\n",
    "                        b_prime_scaled = b_prime - b_prime.max()\n",
    "                    q_prime = torch.softmax(b_prime_scaled, dim=0)\n",
    "                    kl_div = torch.sum(q_prime * (q_prime.log() - logp_yq))\n",
    "                    kl_div.backward(retain_graph=True)\n",
    "                    opt.step()\n",
    "\n",
    "                    if torch.isclose(old_b_prime, b_prime).all():\n",
    "                        break\n",
    "                \n",
    "                # -------------------\n",
    "                \n",
    "                b_prime_spl[i] = b_prime\n",
    "                epistemic_value += kl_div\n",
    "\n",
    "            epistemic_value /= n_spl_epist_val\n",
    "            b_rollout = b_prime_spl[torch.randint(n_spl_epist_val, (1, ))].squeeze()\n",
    "            \n",
    "            efe_step = - epistemic_value  # Need to add intrinsic value\n",
    "\n",
    "            efe_rollout += decay_factor**step * efe_step\n",
    "\n",
    "        total_efe += efe_rollout\n",
    "\n",
    "    total_efe /= n_rollout\n",
    "    total_efe.backward(retain_graph=True)\n",
    "    opt.step()\n",
    "\n",
    "    if torch.isclose(old_a, a).all():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341c18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
